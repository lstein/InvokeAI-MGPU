{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>InvokeAI is an implementation of Stable Diffusion, the open source text-to-image and image-to-image generator. It provides a streamlined process with various new features and options to aid the image generation process. It runs on Windows, Mac and Linux machines, and runs on GPU cards with as little as 4 GB of RAM.</p>"},{"location":"#quick-links","title":"Quick Links","text":"Installation Features Getting Started FAQ Contributing Code and Downloads Bug Reports   Join the Discord Server!"},{"location":"#invokeai-features","title":"InvokeAI Features","text":""},{"location":"#installation","title":"Installation","text":"<ul> <li>Automated Installer</li> <li>Manual Installation</li> <li>Docker Installation</li> </ul>"},{"location":"#the-invokeai-web-interface","title":"The InvokeAI Web Interface","text":"<ul> <li>WebUI overview</li> <li>WebUI hotkey reference guide</li> <li>WebUI Unified Canvas for Img2Img, inpainting and outpainting</li> </ul>"},{"location":"#image-management","title":"Image Management","text":"<ul> <li>Image2Image</li> <li>Adding custom styles and subjects</li> <li>Upscaling and Face Reconstruction</li> <li>Other Features</li> </ul>"},{"location":"#model-management","title":"Model Management","text":"<ul> <li>Installing</li> <li>Model Merging</li> <li>ControlNet Models</li> <li>Style/Subject Concepts and Embeddings</li> <li>Watermarking and the Not Safe for Work (NSFW) Checker</li> </ul>"},{"location":"#prompt-engineering","title":"Prompt Engineering","text":"<ul> <li>Prompt Syntax</li> </ul>"},{"location":"#invokeai-configuration","title":"InvokeAI Configuration","text":"<ul> <li>Guide to InvokeAI Runtime Settings</li> <li>Database Maintenance and other Command Line Utilities</li> </ul>"},{"location":"#troubleshooting","title":"Troubleshooting","text":"<p>Please check out our  FAQ to get solutions for common installation problems and other issues.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Anyone who wishes to contribute to this project, whether documentation, features, bug fixes, code cleanup, testing, or code reviews, is very much encouraged to do so. </p> <p>Please take a look at our Contribution documentation to learn more about contributing to InvokeAI.  </p>"},{"location":"#contributors","title":"Contributors","text":"<p>This software is a combined effort of various people from across the world. Check out the list of all these amazing people. We thank them for their time, hard work and effort.</p>"},{"location":"#support","title":"Support","text":"<p>For support, please use this repository's GitHub Issues tracking service. Feel free to send me an email if you use and like the script.</p> <p>Original portions of the software are Copyright \u00a9 2022-23 by The InvokeAI Team.</p>"},{"location":"CHANGELOG/","title":"Changelog","text":""},{"location":"CHANGELOG/#v235-22-may-2023","title":"v2.3.5 (22 May 2023)","text":"<p>This release (along with the post1 and post2 follow-on releases) expands support for additional LoRA and LyCORIS models, upgrades diffusers versions, and fixes a few bugs.</p>"},{"location":"CHANGELOG/#lora-and-lycoris-support-improvement","title":"LoRA and LyCORIS Support Improvement","text":"<pre><code>A number of LoRA/LyCORIS fine-tune files (those which alter the text encoder as well as the unet model) were not having the desired effect in InvokeAI. This bug has now been fixed. Full documentation of LoRA support is available at InvokeAI LoRA Support.\nPreviously, InvokeAI did not distinguish between LoRA/LyCORIS models based on Stable Diffusion v1.5 vs those based on v2.0 and 2.1, leading to a crash when an incompatible model was loaded. This has now been fixed. In addition, the web pulldown menus for LoRA and Textual Inversion selection have been enhanced to show only those files that are compatible with the currently-selected Stable Diffusion model.\nSupport for the newer LoKR LyCORIS files has been added.\n</code></pre>"},{"location":"CHANGELOG/#library-updates-and-speedreproducibility-advancements","title":"Library Updates and Speed/Reproducibility Advancements","text":"<p>The major enhancement in this version is that NVIDIA users no longer need to decide between speed and reproducibility. Previously, if you activated the Xformers library, you would see improvements in speed and memory usage, but multiple images generated with the same seed and other parameters would be slightly different from each other. This is no longer the case. Relative to 2.3.5 you will see improved performance when running without Xformers, and even better performance when Xformers is activated. In both cases, images generated with the same settings will be identical.</p> <p>Here are the new library versions: Library     Version Torch   2.0.0 Diffusers   0.16.1 Xformers    0.0.19 Compel  1.1.5 Other Improvements</p>"},{"location":"CHANGELOG/#performance-improvements","title":"Performance Improvements","text":"<pre><code>When a model is loaded for the first time, InvokeAI calculates its checksum for incorporation into the PNG metadata. This process could take up to a minute on network-mounted disks and WSL mounts. This release noticeably speeds up the process.\n</code></pre>"},{"location":"CHANGELOG/#bug-fixes","title":"Bug Fixes","text":"<pre><code>The \"import models from directory\" and \"import from URL\" functionality in the console-based model installer has now been fixed.\nWhen running the WebUI, we have reduced the number of times that InvokeAI reaches out to HuggingFace to fetch the list of embeddable Textual Inversion models. We have also caught and fixed a problem with the updater not correctly detecting when another instance of the updater is running\n</code></pre>"},{"location":"CHANGELOG/#v234-7-april-2023","title":"v2.3.4 (7 April 2023)","text":"<p>What's New in 2.3.4</p> <p>This features release adds support for LoRA (Low-Rank Adaptation) and LyCORIS (Lora beYond Conventional) models, as well as some minor bug fixes.</p>"},{"location":"CHANGELOG/#lora-and-lycoris-support","title":"LoRA and LyCORIS Support","text":"<p>LoRA files contain fine-tuning weights that enable particular styles, subjects or concepts to be applied to generated images. LyCORIS files are an extended variant of LoRA. InvokeAI supports the most common LoRA/LyCORIS format, which ends in the suffix .safetensors. You will find numerous LoRA and LyCORIS models for download at Civitai, and a small but growing number at Hugging Face. Full documentation of LoRA support is available at InvokeAI LoRA Support.( Pre-release note: this page will only be available after release)</p> <p>To use LoRA/LyCORIS models in InvokeAI:</p> <pre><code>Download the .safetensors files of your choice and place in /path/to/invokeai/loras. This directory was not present in earlier version of InvokeAI but will be created for you the first time you run the command-line or web client. You can also create the directory manually.\n\nAdd withLora(lora-file,weight) to your prompts. The weight is optional and will default to 1.0. A few examples, assuming that a LoRA file named loras/sushi.safetensors is present:\n</code></pre> <p>family sitting at dinner table eating sushi withLora(sushi,0.9) family sitting at dinner table eating sushi withLora(sushi, 0.75) family sitting at dinner table eating sushi withLora(sushi)</p> <p>Multiple withLora() prompt fragments are allowed. The weight can be arbitrarily large, but the useful range is roughly 0.5 to 1.0. Higher weights make the LoRA's influence stronger. Negative weights are also allowed, which can lead to some interesting effects.</p> <pre><code>Generate as you usually would! If you find that the image is too \"crisp\" try reducing the overall CFG value or reducing individual LoRA weights. As is the case with all fine-tunes, you'll get the best results when running the LoRA on top of the model similar to, or identical with, the one that was used during the LoRA's training. Don't try to load a SD 1.x-trained LoRA into a SD 2.x model, and vice versa. This will trigger a non-fatal error message and generation will not proceed.\n\nYou can change the location of the loras directory by passing the --lora_directory option to `invokeai.\n</code></pre>"},{"location":"CHANGELOG/#new-webui-lora-and-textual-inversion-buttons","title":"New WebUI LoRA and Textual Inversion Buttons","text":"<p>This version adds two new web interface buttons for inserting LoRA and Textual Inversion triggers into the prompt as shown in the screenshot below.</p> <p>Clicking on one or the other of the buttons will bring up a menu of available LoRA/LyCORIS or Textual Inversion trigger terms. Select a menu item to insert the properly-formatted withLora() or  prompt fragment into the positive prompt. The number in parentheses indicates the number of trigger terms currently in the prompt. You may click the button again and deselect the LoRA or trigger to remove it from the prompt, or simply edit the prompt directly. <p>Currently terms are inserted into the positive prompt textbox only. However, some textual inversion embeddings are designed to be used with negative prompts. To move a textual inversion trigger into the negative prompt, simply cut and paste it.</p> <p>By default the Textual Inversion menu only shows locally installed models found at startup time in /path/to/invokeai/embeddings. However, InvokeAI has the ability to dynamically download and install additional Textual Inversion embeddings from the HuggingFace Concepts Library. You may choose to display the most popular of these (with five or more likes) in the Textual Inversion menu by going to Settings and turning on \"Show Textual Inversions from HF Concepts Library.\" When this option is activated, the locally-installed TI embeddings will be shown first, followed by uninstalled terms from Hugging Face. See The Hugging Face Concepts Library and Importing Textual Inversion files for more information.</p>"},{"location":"CHANGELOG/#minor-features-and-fixes","title":"Minor features and fixes","text":"<p>This release changes model switching behavior so that the command-line and Web UIs save the last model used and restore it the next time they are launched. It also improves the behavior of the installer so that the pip utility is kept up to date.</p>"},{"location":"CHANGELOG/#known-bugs-in-234","title":"Known Bugs in 2.3.4","text":"<p>These are known bugs in the release.</p> <pre><code>The Ancestral DPMSolverMultistepScheduler (k_dpmpp_2a) sampler is not yet implemented for diffusers models and will disappear from the WebUI Sampler menu when a diffusers model is selected.\nWindows Defender will sometimes raise Trojan or backdoor alerts for the codeformer.pth face restoration model, as well as the CIDAS/clipseg and runwayml/stable-diffusion-v1.5 models. These are false positives and can be safely ignored. InvokeAI performs a malware scan on all models as they are loaded. For additional security, you should use safetensors models whenever they are available.\n</code></pre>"},{"location":"CHANGELOG/#v233-28-march-2023","title":"v2.3.3 (28 March 2023)","text":"<p>This is a bugfix and minor feature release.</p>"},{"location":"CHANGELOG/#bugfixes","title":"Bugfixes","text":"<p>Since version 2.3.2 the following bugs have been fixed: Bugs</p> <pre><code>When using legacy checkpoints with an external VAE, the VAE file is now scanned for malware prior to loading. Previously only the main model weights file was scanned.\nTextual inversion will select an appropriate batchsize based on whether xformers is active, and will default to xformers enabled if the library is detected.\nThe batch script log file names have been fixed to be compatible with Windows.\nOccasional corruption of the .next_prefix file (which stores the next output file name in sequence) on Windows systems is now detected and corrected.\nSupport loading of legacy config files that have no personalization (textual inversion) section.\nAn infinite loop when opening the developer's console from within the invoke.sh script has been corrected.\nDocumentation fixes, including a recipe for detecting and fixing problems with the AMD GPU ROCm driver.\n</code></pre> <p>Enhancements</p> <pre><code>It is now possible to load and run several community-contributed SD-2.0 based models, including the often-requested \"Illuminati\" model.\nThe \"NegativePrompts\" embedding file, and others like it, can now be loaded by placing it in the InvokeAI embeddings directory.\nIf no --model is specified at launch time, InvokeAI will remember the last model used and restore it the next time it is launched.\nOn Linux systems, the invoke.sh launcher now uses a prettier console-based interface. To take advantage of it, install the dialog package using your package manager (e.g. sudo apt install dialog).\nWhen loading legacy models (safetensors/ckpt) you can specify a custom config file and/or a VAE by placing like-named files in the same directory as the model following this example:\n</code></pre> <p>my-favorite-model.ckpt my-favorite-model.yaml my-favorite-model.vae.pt      # or my-favorite-model.vae.safetensors</p>"},{"location":"CHANGELOG/#known-bugs-in-233","title":"Known Bugs in 2.3.3","text":"<p>These are known bugs in the release.</p> <pre><code>The Ancestral DPMSolverMultistepScheduler (k_dpmpp_2a) sampler is not yet implemented for diffusers models and will disappear from the WebUI Sampler menu when a diffusers model is selected.\nWindows Defender will sometimes raise Trojan or backdoor alerts for the codeformer.pth face restoration model, as well as the CIDAS/clipseg and runwayml/stable-diffusion-v1.5 models. These are false positives and can be safely ignored. InvokeAI performs a malware scan on all models as they are loaded. For additional security, you should use safetensors models whenever they are available.\n</code></pre>"},{"location":"CHANGELOG/#v232-11-march-2023","title":"v2.3.2 (11 March 2023)","text":"<p>This is a bugfix and minor feature release.</p>"},{"location":"CHANGELOG/#bugfixes_1","title":"Bugfixes","text":"<p>Since version 2.3.1 the following bugs have been fixed:</p> <pre><code>Black images appearing for potential NSFW images when generating with legacy checkpoint models and both --no-nsfw_checker and --ckpt_convert turned on.\nBlack images appearing when generating from models fine-tuned on Stable-Diffusion-2-1-base. When importing V2-derived models, you may be asked to select whether the model was derived from a \"base\" model (512 pixels) or the 768-pixel SD-2.1 model.\nThe \"Use All\" button was not restoring the Hi-Res Fix setting on the WebUI\nWhen using the model installer console app, models failed to import correctly when importing from directories with spaces in their names. A similar issue with the output directory was also fixed.\nCrashes that occurred during model merging.\nRestore previous naming of Stable Diffusion base and 768 models.\nUpgraded to latest versions of diffusers, transformers, safetensors and accelerate libraries upstream. We hope that this will fix the assertion NDArray &gt; 2**32 issue that MacOS users have had when generating images larger than 768x768 pixels. Please report back.\n</code></pre> <p>As part of the upgrade to diffusers, the location of the diffusers-based models has changed from models/diffusers to models/hub. When you launch InvokeAI for the first time, it will prompt you to OK a one-time move. This should be quick and harmless, but if you have modified your models/diffusers directory in some way, for example using symlinks, you may wish to cancel the migration and make appropriate adjustments. New \"Invokeai-batch\" script</p>"},{"location":"CHANGELOG/#invoke-ai-batch","title":"Invoke AI Batch","text":"<p>2.3.2 introduces a new command-line only script called invokeai-batch that can be used to generate hundreds of images from prompts and settings that vary systematically. This can be used to try the same prompt across multiple combinations of models, steps, CFG settings and so forth. It also allows you to template prompts and generate a combinatorial list like:</p> <p>a shack in the mountains, photograph a shack in the mountains, watercolor a shack in the mountains, oil painting a chalet in the mountains, photograph a chalet in the mountains, watercolor a chalet in the mountains, oil painting a shack in the desert, photograph ...</p> <p>If you have a system with multiple GPUs, or a single GPU with lots of VRAM, you can parallelize generation across the combinatorial set, reducing wait times and using your system's resources efficiently (make sure you have good GPU cooling).</p> <p>To try invokeai-batch out. Launch the \"developer's console\" using the invoke launcher script, or activate the invokeai virtual environment manually. From the console, give the command invokeai-batch --help in order to learn how the script works and create your first template file for dynamic prompt generation.</p>"},{"location":"CHANGELOG/#known-bugs-in-232","title":"Known Bugs in 2.3.2","text":"<p>These are known bugs in the release.</p> <pre><code>The Ancestral DPMSolverMultistepScheduler (k_dpmpp_2a) sampler is not yet implemented for diffusers models and will disappear from the WebUI Sampler menu when a diffusers model is selected.\nWindows Defender will sometimes raise a Trojan alert for the codeformer.pth face restoration model. As far as we have been able to determine, this is a false positive and can be safely whitelisted.\n</code></pre>"},{"location":"CHANGELOG/#v231-22-february-2023","title":"v2.3.1 (22 February 2023)","text":"<p>This is primarily a bugfix release, but it does provide several new features that will improve the user experience.</p>"},{"location":"CHANGELOG/#enhanced-support-for-model-management","title":"Enhanced support for model management","text":"<p>InvokeAI now makes it convenient to add, remove and modify models. You can individually import models that are stored on your local system, scan an entire folder and its subfolders for models and import them automatically, and even directly import models from the internet by providing their download URLs. You also have the option of designating a local folder to scan for new models each time InvokeAI is restarted.</p> <p>There are three ways of accessing the model management features:</p> <pre><code>From the WebUI, click on the cube to the right of the model selection menu. This will bring up a form that allows you to import models individually from your local disk or scan a directory for models to import.\n\nUsing the Model Installer App\n</code></pre> <p>Choose option (5) download and install models from the invoke launcher script to start a new console-based application for model management. You can use this to select from a curated set of starter models, or import checkpoint, safetensors, and diffusers models from a local disk or the internet. The example below shows importing two checkpoint URLs from popular SD sites and a HuggingFace diffusers model using its Repository ID. It also shows how to designate a folder to be scanned at startup time for new models to import.</p> <p>Command-line users can start this app using the command invokeai-model-install.</p> <pre><code>Using the Command Line Client (CLI)\n</code></pre> <p>The !install_model and !convert_model commands have been enhanced to allow entering of URLs and local directories to scan and import. The first command installs .ckpt and .safetensors files as-is. The second one converts them into the faster diffusers format before installation.</p> <p>Internally InvokeAI is able to probe the contents of a .ckpt or .safetensors file to distinguish among v1.x, v2.x and inpainting models. This means that you do not need to include \"inpaint\" in your model names to use an inpainting model. Note that Stable Diffusion v2.x models will be autoconverted into a diffusers model the first time you use it.</p> <p>Please see INSTALLING MODELS for more information on model management.</p>"},{"location":"CHANGELOG/#an-improved-installer-experience","title":"An Improved Installer Experience","text":"<p>The installer now launches a console-based UI for setting and changing commonly-used startup options:</p> <p>After selecting the desired options, the installer installs several support models needed by InvokeAI's face reconstruction and upscaling features and then launches the interface for selecting and installing models shown earlier. At any time, you can edit the startup options by launching invoke.sh/invoke.bat and entering option (6) change InvokeAI startup options</p> <p>Command-line users can launch the new configure app using invokeai-configure.</p> <p>This release also comes with a renewed updater. To do an update without going through a whole reinstallation, launch invoke.sh or invoke.bat and choose option (9) update InvokeAI . This will bring you to a screen that prompts you to update to the latest released version, to the most current development version, or any released or unreleased version you choose by selecting the tag or branch of the desired version.</p> <p>Command-line users can run this interface by typing invokeai-configure</p>"},{"location":"CHANGELOG/#image-symmetry-options","title":"Image Symmetry Options","text":"<p>There are now features to generate horizontal and vertical symmetry during generation. The way these work is to wait until a selected step in the generation process and then to turn on a mirror image effect. In addition to generating some cool images, you can also use this to make side-by-side comparisons of how an image will look with more or fewer steps. Access this option from the WebUI by selecting Symmetry from the image generation settings, or within the CLI by using the options --h_symmetry_time_pct and --v_symmetry_time_pct (these can be abbreviated to --h_sym and --v_sym like all other options).</p>"},{"location":"CHANGELOG/#a-new-unified-canvas-look","title":"A New Unified Canvas Look","text":"<p>This release introduces a beta version of the WebUI Unified Canvas. To try it out, open up the settings dialogue in the WebUI (gear icon) and select Use Canvas Beta Layout:</p> <p>Refresh the screen and go to to Unified Canvas (left side of screen, third icon from the top). The new layout is designed to provide more space to work in and to keep the image controls close to the image itself:</p> <p>Model conversion and merging within the WebUI</p> <p>The WebUI now has an intuitive interface for model merging, as well as for permanent conversion of models from legacy .ckpt/.safetensors formats into diffusers format. These options are also available directly from the invoke.sh/invoke.bat scripts. An easier way to contribute translations to the WebUI</p> <p>We have migrated our translation efforts to Weblate, a FOSS translation product. Maintaining the growing project's translations is now far simpler for the maintainers and community. Please review our brief translation guide for more information on how to contribute. Numerous internal bugfixes and performance issues</p>"},{"location":"CHANGELOG/#bug-fixes_1","title":"Bug Fixes","text":"<p>This releases quashes multiple bugs that were reported in 2.3.0. Major internal changes include upgrading to diffusers 0.13.0, and using the compel library for prompt parsing. See Detailed Change Log for a detailed list of bugs caught and squished. Summary of InvokeAI command line scripts (all accessible via the launcher menu) Command     Description invokeai    Command line interface invokeai --web  Web interface invokeai-model-install  Model installer with console forms-based front end invokeai-ti --gui   Textual inversion, with a console forms-based front end invokeai-merge --gui    Model merging, with a console forms-based front end invokeai-configure  Startup configuration; can also be used to reinstall support models invokeai-update     InvokeAI software updater</p>"},{"location":"CHANGELOG/#known-bugs-in-231","title":"Known Bugs in 2.3.1","text":"<p>These are known bugs in the release.     MacOS users generating 768x768 pixel images or greater using diffusers models may experience a hard crash with assertion NDArray &gt; 2**32 This appears to be an issu...</p>"},{"location":"CHANGELOG/#v230-15-january-2023","title":"v2.3.0 (15 January 2023)","text":"<p>**Transition to diffusers</p> <p>Version 2.3 provides support for both the traditional <code>.ckpt</code> weight checkpoint files as well as the HuggingFace <code>diffusers</code> format. This introduces several changes you should know about.</p> <ol> <li>The models.yaml format has been updated. There are now two    different type of configuration stanza. The traditional ckpt    one will look like this, with a <code>format</code> of <code>ckpt</code> and a    <code>weights</code> field that points to the absolute or ROOTDIR-relative    location of the ckpt file.</li> </ol> <pre><code>inpainting-1.5:\n   description: RunwayML SD 1.5 model optimized for inpainting (4.27 GB)\n   repo_id: runwayml/stable-diffusion-inpainting\n   format: ckpt\n   width: 512\n   height: 512\n   weights: models/ldm/stable-diffusion-v1/sd-v1-5-inpainting.ckpt\n   config: configs/stable-diffusion/v1-inpainting-inference.yaml\n   vae: models/ldm/stable-diffusion-v1/vae-ft-mse-840000-ema-pruned.ckpt\n</code></pre> <p>A configuration stanza for a diffusers model hosted at HuggingFace will look like this,   with a <code>format</code> of <code>diffusers</code> and a <code>repo_id</code> that points to the   repository ID of the model on HuggingFace:</p> <pre><code>stable-diffusion-2.1:\ndescription: Stable Diffusion version 2.1 diffusers model (5.21 GB)\nrepo_id: stabilityai/stable-diffusion-2-1\nformat: diffusers\n</code></pre> <p>A configuration stanza for a diffuers model stored locally should   look like this, with a <code>format</code> of <code>diffusers</code>, but a <code>path</code> field   that points at the directory that contains <code>model_index.json</code>:</p> <pre><code>waifu-diffusion:\ndescription: Latest waifu diffusion 1.4\nformat: diffusers\npath: models/diffusers/hakurei-haifu-diffusion-1.4\n</code></pre> <ol> <li>In order of precedence, InvokeAI will now use HF_HOME, then    XDG_CACHE_HOME, then finally default to <code>ROOTDIR/models</code> to    store HuggingFace diffusers models.</li> </ol> <p>Consequently, the format of the models directory has changed to    mimic the HuggingFace cache directory. When HF_HOME and XDG_HOME    are not set, diffusers models are now automatically downloaded    and retrieved from the directory <code>ROOTDIR/models/diffusers</code>,    while other models are stored in the directory    <code>ROOTDIR/models/hub</code>. This organization is the same as that used    by HuggingFace for its cache management.</p> <p>This allows you to share diffusers and ckpt model files easily with    other machine learning applications that use the HuggingFace    libraries. To do this, set the environment variable HF_HOME    before starting up InvokeAI to tell it what directory to    cache models in. To tell InvokeAI to use the standard HuggingFace    cache directory, you would set HF_HOME like this (Linux/Mac):</p> <p><code>export HF_HOME=~/.cache/huggingface</code></p> <p>Both HuggingFace and InvokeAI will fall back to the XDG_CACHE_HOME    environment variable if HF_HOME is not set; this path    takes precedence over <code>ROOTDIR/models</code> to allow for the same sharing    with other machine learning applications that use HuggingFace    libraries.</p> <ol> <li> <p>If you upgrade to InvokeAI 2.3.* from an earlier version, there    will be a one-time migration from the old models directory format    to the new one. You will see a message about this the first time    you start <code>invoke.py</code>.</p> </li> <li> <p>Both the front end back ends of the model manager have been    rewritten to accommodate diffusers. You can import models using    their local file path, using their URLs, or their HuggingFace    repo_ids. On the command line, all these syntaxes work:</p> </li> </ol> <pre><code>!import_model stabilityai/stable-diffusion-2-1-base\n!import_model /opt/sd-models/sd-1.4.ckpt\n!import_model https://huggingface.co/Fictiverse/Stable_Diffusion_PaperCut_Model/blob/main/PaperCut_v1.ckpt\n</code></pre> <p>**KNOWN BUGS (15 January 2023)</p> <ol> <li> <p>On CUDA systems, the 768 pixel stable-diffusion-2.0 and    stable-diffusion-2.1 models can only be run as <code>diffusers</code> models    when the <code>xformer</code> library is installed and configured. Without    <code>xformers</code>, InvokeAI returns black images.</p> </li> <li> <p>Inpainting and outpainting have regressed in quality.</p> </li> </ol> <p>Both these issues are being actively worked on.</p>"},{"location":"CHANGELOG/#v224-11-december-2022","title":"v2.2.4 (11 December 2022)","text":"<p>the <code>invokeai</code> directory</p> <p>Previously there were two directories to worry about, the directory that contained the InvokeAI source code and the launcher scripts, and the <code>invokeai</code> directory that contained the models files, embeddings, configuration and outputs. With the 2.2.4 release, this dual system is done away with, and everything, including the <code>invoke.bat</code> and <code>invoke.sh</code> launcher scripts, now live in a directory named <code>invokeai</code>. By default this directory is located in your home directory (e.g. <code>\\Users\\yourname</code> on Windows), but you can select where it goes at install time.</p> <p>After installation, you can delete the install directory (the one that the zip file creates when it unpacks). Do not delete or move the <code>invokeai</code> directory!</p> <p>Initialization file <code>invokeai/invokeai.init</code></p> <p>You can place frequently-used startup options in this file, such as the default number of steps or your preferred sampler. To keep everything in one place, this file has now been moved into the <code>invokeai</code> directory and is named <code>invokeai.init</code>.</p> <p>To update from Version 2.2.3</p> <p>The easiest route is to download and unpack one of the 2.2.4 installer files. When it asks you for the location of the <code>invokeai</code> runtime directory, respond with the path to the directory that contains your 2.2.3 <code>invokeai</code>. That is, if <code>invokeai</code> lives at <code>C:\\Users\\fred\\invokeai</code>, then answer with <code>C:\\Users\\fred</code> and answer \"Y\" when asked if you want to reuse the directory.</p> <p>The <code>update.sh</code> (<code>update.bat</code>) script that came with the 2.2.3 source installer does not know about the new directory layout and won't be fully functional.</p> <p>To update to 2.2.5 (and beyond) there's now an update path</p> <p>As they become available, you can update to more recent versions of InvokeAI using an <code>update.sh</code> (<code>update.bat</code>) script located in the <code>invokeai</code> directory. Running it without any arguments will install the most recent version of InvokeAI. Alternatively, you can get set releases by running the <code>update.sh</code> script with an argument in the command shell. This syntax accepts the path to the desired release's zip file, which you can find by clicking on the green \"Code\" button on this repository's home page.</p> <p>Other 2.2.4 Improvements</p> <ul> <li>Fix InvokeAI GUI initialization by @addianto in #1687</li> <li>fix link in documentation by @lstein in #1728</li> <li>Fix broken link by @ShawnZhong in #1736</li> <li>Remove reference to binary installer by @lstein in #1731</li> <li>documentation fixes for 2.2.3 by @lstein in #1740</li> <li>Modify installer links to point closer to the source installer by @ebr in   #1745</li> <li>add documentation warning about 1650/60 cards by @lstein in #1753</li> <li>Fix Linux source URL in installation docs by @andybearman in #1756</li> <li>Make install instructions discoverable in readme by @damian0815 in #1752</li> <li>typo fix by @ofirkris in #1755</li> <li>Non-interactive model download (support HUGGINGFACE_TOKEN) by @ebr in #1578</li> <li>fix(srcinstall): shell installer - cp scripts instead of linking by @tildebyte   in #1765</li> <li>stability and usage improvements to binary &amp; source installers by @lstein in   #1760</li> <li>fix off-by-one bug in cross-attention-control by @damian0815 in #1774</li> <li>Eventually update APP_VERSION to 2.2.3 by @spezialspezial in #1768</li> <li>invoke script cds to its location before running by @lstein in #1805</li> <li>Make PaperCut and VoxelArt models load again by @lstein in #1730</li> <li>Fix --embedding_directory / --embedding_path not working by @blessedcoolant in   #1817</li> <li>Clean up readme by @hipsterusername in #1820</li> <li>Optimized Docker build with support for external working directory by @ebr in   #1544</li> <li>disable pushing the cloud container by @mauwii in #1831</li> <li>Fix docker push github action and expand with additional metadata by @ebr in   #1837</li> <li>Fix Broken Link To Notebook by @VedantMadane in #1821</li> <li>Account for flat models by @spezialspezial in #1766</li> <li>Update invoke.bat.in isolate environment variables by @lynnewu in #1833</li> <li>Arch Linux Specific PatchMatch Instructions &amp; fixing conda install on linux by   @SammCheese in #1848</li> <li>Make force free GPU memory work in img2img by @addianto in #1844</li> <li>New installer by @lstein</li> </ul>"},{"location":"CHANGELOG/#v223-2-december-2022","title":"v2.2.3 (2 December 2022)","text":"<p>Note</p> <p>This point release removes references to the binary installer from the installation guide. The binary installer is not stable at the current time. First time users are encouraged to use the \"source\" installer as described in Installing InvokeAI with the Source Installer</p> <p>With InvokeAI 2.2, this project now provides enthusiasts and professionals a robust workflow solution for creating AI-generated and human facilitated compositions. Additional enhancements have been made as well, improving safety, ease of use, and installation.</p> <p>Optimized for efficiency, InvokeAI needs only ~3.5GB of VRAM to generate a 512x768 image (and less for smaller images), and is compatible with Windows/Linux/Mac (M1 &amp; M2).</p> <p>You can see the release video here, which introduces the main WebUI enhancement for version 2.2 - The Unified Canvas. This new workflow is the biggest enhancement added to the WebUI to date, and unlocks a stunning amount of potential for users to create and iterate on their creations. The following sections describe what's new for InvokeAI.</p>"},{"location":"CHANGELOG/#v222-30-november-2022","title":"v2.2.2 (30 November 2022)","text":"<p>Note</p> <p>The binary installer is not ready for prime time. First time users are recommended to install via the \"source\" installer accessible through the links at the bottom of this page.****</p> <p>With InvokeAI 2.2, this project now provides enthusiasts and professionals a robust workflow solution for creating AI-generated and human facilitated compositions. Additional enhancements have been made as well, improving safety, ease of use, and installation.</p> <p>Optimized for efficiency, InvokeAI needs only ~3.5GB of VRAM to generate a 512x768 image (and less for smaller images), and is compatible with Windows/Linux/Mac (M1 &amp; M2).</p> <p>You can see the release video here, which introduces the main WebUI enhancement for version 2.2 - The Unified Canvas. This new workflow is the biggest enhancement added to the WebUI to date, and unlocks a stunning amount of potential for users to create and iterate on their creations. The following sections describe what's new for InvokeAI.</p>"},{"location":"CHANGELOG/#v220-2-december-2022","title":"v2.2.0 (2 December 2022)","text":"<p>With InvokeAI 2.2, this project now provides enthusiasts and professionals a robust workflow solution for creating AI-generated and human facilitated compositions. Additional enhancements have been made as well, improving safety, ease of use, and installation.</p> <p>Optimized for efficiency, InvokeAI needs only ~3.5GB of VRAM to generate a 512x768 image (and less for smaller images), and is compatible with Windows/Linux/Mac (M1 &amp; M2).</p> <p>You can see the release video here, which introduces the main WebUI enhancement for version 2.2 - The Unified Canvas. This new workflow is the biggest enhancement added to the WebUI to date, and unlocks a stunning amount of potential for users to create and iterate on their creations. The following sections describe what's new for InvokeAI.</p>"},{"location":"CHANGELOG/#v213-13-november-2022","title":"v2.1.3 (13 November 2022)","text":"<ul> <li>A choice of installer scripts that automate installation and configuration.   See   Installation.</li> <li>A streamlined manual installation process that works for both Conda and   PIP-only installs. See   Manual Installation.</li> <li>The ability to save frequently-used startup options (model to load, steps,   sampler, etc) in a <code>.invokeai</code> file. See   Client</li> <li>Support for AMD GPU cards (non-CUDA) on Linux machines.</li> <li>Multiple bugs and edge cases squashed.</li> </ul>"},{"location":"CHANGELOG/#v210-2-november-2022","title":"v2.1.0 (2 November 2022)","text":"<ul> <li>update mac instructions to use invokeai for env name by @willwillems in #1030</li> <li>Update .gitignore by @blessedcoolant in #1040</li> <li>reintroduce fix for m1 from #579 missing after merge by @skurovec in #1056</li> <li>Update Stable_Diffusion_AI_Notebook.ipynb (Take 2) by @ChloeL19 in #1060</li> <li>Print out the device type which is used by @manzke in #1073</li> <li>Hires Addition by @hipsterusername in #1063</li> <li>fix for \"1 leaked semaphore objects to clean up at shutdown\" on M1 by   @skurovec in #1081</li> <li>Forward dream.py to invoke.py using the same interpreter, add deprecation   warning by @db3000 in #1077</li> <li>fix noisy images at high step counts by @lstein in #1086</li> <li>Generalize facetool strength argument by @db3000 in #1078</li> <li>Enable fast switching among models at the invoke&gt; command line by @lstein in   #1066</li> <li>Fix Typo, committed changing ldm environment to invokeai by @jdries3 in #1095</li> <li>Update generate.py by @unreleased in #1109</li> <li>Update 'ldm' env to 'invokeai' in troubleshooting steps by @19wolf in #1125</li> <li>Fixed documentation typos and resolved merge conflicts by @rupeshs in #1123</li> <li>Fix broken doc links, fix malaprop in the project subtitle by @majick in #1131</li> <li>Only output facetool parameters if enhancing faces by @db3000 in #1119</li> <li>Update gitignore to ignore codeformer weights at new location by   @spezialspezial in #1136</li> <li>fix links to point to invoke-ai.github.io #1117 by @mauwii in #1143</li> <li>Rework-mkdocs by @mauwii in #1144</li> <li>add option to CLI and pngwriter that allows user to set PNG compression level   by @lstein in #1127</li> <li>Fix img2img DDIM index out of bound by @wfng92 in #1137</li> <li>Fix gh actions by @mauwii in #1128</li> <li>update mac instructions to use invokeai for env name by @willwillems in #1030</li> <li>Update .gitignore by @blessedcoolant in #1040</li> <li>reintroduce fix for m1 from #579 missing after merge by @skurovec in #1056</li> <li>Update Stable_Diffusion_AI_Notebook.ipynb (Take 2) by @ChloeL19 in #1060</li> <li>Print out the device type which is used by @manzke in #1073</li> <li>Hires Addition by @hipsterusername in #1063</li> <li>fix for \"1 leaked semaphore objects to clean up at shutdown\" on M1 by   @skurovec in #1081</li> <li>Forward dream.py to invoke.py using the same interpreter, add deprecation   warning by @db3000 in #1077</li> <li>fix noisy images at high step counts by @lstein in #1086</li> <li>Generalize facetool strength argument by @db3000 in #1078</li> <li>Enable fast switching among models at the invoke&gt; command line by @lstein in   #1066</li> <li>Fix Typo, committed changing ldm environment to invokeai by @jdries3 in #1095</li> <li>Fixed documentation typos and resolved merge conflicts by @rupeshs in #1123</li> <li>Only output facetool parameters if enhancing faces by @db3000 in #1119</li> <li>add option to CLI and pngwriter that allows user to set PNG compression level   by @lstein in #1127</li> <li>Fix img2img DDIM index out of bound by @wfng92 in #1137</li> <li>Add text prompt to inpaint mask support by @lstein in #1133</li> <li>Respect http[s] protocol when making socket.io middleware by @damian0815 in   #976</li> <li>WebUI: Adds Codeformer support by @psychedelicious in #1151</li> <li>Skips normalizing prompts for web UI metadata by @psychedelicious in #1165</li> <li>Add Asymmetric Tiling by @carson-katri in #1132</li> <li>Web UI: Increases max CFG Scale to 200 by @psychedelicious in #1172</li> <li>Corrects color channels in face restoration; Fixes #1167 by @psychedelicious   in #1175</li> <li>Flips channels using array slicing instead of using OpenCV by @psychedelicious   in #1178</li> <li>Fix typo in docs: s/Formally/Formerly by @noodlebox in #1176</li> <li>fix clipseg loading problems by @lstein in #1177</li> <li>Correct color channels in upscale using array slicing by @wfng92 in #1181</li> <li>Web UI: Filters existing images when adding new images; Fixes #1085 by   @psychedelicious in #1171</li> <li>fix a number of bugs in textual inversion by @lstein in #1190</li> <li>Improve !fetch, add !replay command by @ArDiouscuros in #882</li> <li>Fix generation of image with s&gt;1000 by @holstvoogd in #951</li> <li>Web UI: Gallery improvements by @psychedelicious in #1198</li> <li>Update CLI.md by @krummrey in #1211</li> <li>outcropping improvements by @lstein in #1207</li> <li>add support for loading VAE autoencoders by @lstein in #1216</li> <li>remove duplicate fix_func for MPS by @wfng92 in #1210</li> <li>Metadata storage and retrieval fixes by @lstein in #1204</li> <li>nix: add shell.nix file by @Cloudef in #1170</li> <li>Web UI: Changes vite dist asset paths to relative by @psychedelicious in #1185</li> <li>Web UI: Removes isDisabled from PromptInput by @psychedelicious in #1187</li> <li>Allow user to generate images with initial noise as on M1 / mps system by   @ArDiouscuros in #981</li> <li>feat: adding filename format template by @plucked in #968</li> <li>Web UI: Fixes broken bundle by @psychedelicious in #1242</li> <li>Support runwayML custom inpainting model by @lstein in #1243</li> <li>Update IMG2IMG.md by @talitore in #1262</li> <li>New dockerfile - including a build- and a run- script as well as a GH-Action   by @mauwii in #1233</li> <li>cut over from karras to model noise schedule for higher steps by @lstein in   #1222</li> <li>Prompt tweaks by @lstein in #1268</li> <li>Outpainting implementation by @Kyle0654 in #1251</li> <li>fixing aspect ratio on hires by @tjennings in #1249</li> <li>Fix-build-container-action by @mauwii in #1274</li> <li>handle all unicode characters by @damian0815 in #1276</li> <li>adds models.user.yml to .gitignore by @JakeHL in #1281</li> <li>remove debug branch, set fail-fast to false by @mauwii in #1284</li> <li>Protect-secrets-on-pr by @mauwii in #1285</li> <li>Web UI: Adds initial inpainting implementation by @psychedelicious in #1225</li> <li>fix environment-mac.yml - tested on x64 and arm64 by @mauwii in #1289</li> <li>Use proper authentication to download model by @mauwii in #1287</li> <li>Prevent indexing error for mode RGB by @spezialspezial in #1294</li> <li>Integrate sd-v1-5 model into test matrix (easily expandable), remove   unecesarry caches by @mauwii in #1293</li> <li>add --no-interactive to configure_invokeai step by @mauwii in #1302</li> <li>1-click installer and updater. Uses micromamba to install git and conda into a   contained environment (if necessary) before running the normal installation   script by @cmdr2 in #1253</li> <li>configure_invokeai.py script downloads the weight files by @lstein in #1290</li> </ul>"},{"location":"CHANGELOG/#v201-13-october-2022","title":"v2.0.1 (13 October 2022)","text":"<ul> <li>fix noisy images at high step count when using k* samplers</li> <li>dream.py script now calls invoke.py module directly rather than via a new   python process (which could break the environment)</li> </ul>"},{"location":"CHANGELOG/#v200-9-october-2022","title":"v2.0.0 (9 October 2022)","text":"<ul> <li><code>dream.py</code> script renamed <code>invoke.py</code>. A <code>dream.py</code> script wrapper remains for   backward compatibility.</li> <li>Completely new WebGUI - launch with <code>python3 scripts/invoke.py --web</code></li> <li>img2img runs on all k* samplers</li> <li>Support for   negative prompts</li> <li>Support for CodeFormer face reconstruction</li> <li>Support for Textual Inversion on Macintoshes</li> <li>Support in both WebGUI and CLI for   post-processing of previously-generated images   using facial reconstruction, ESRGAN upscaling, outcropping (similar to DALL-E   infinite canvas), and \"embiggen\" upscaling. See the <code>!fix</code> command.</li> <li>New <code>--hires</code> option on <code>invoke&gt;</code> line allows   larger images to be created without duplicating elements,   at the cost of some performance.</li> <li>New <code>--perlin</code> and <code>--threshold</code> options allow you to add and control   variation during image generation (see   Thresholding and Perlin Noise Initialization)</li> <li>Extensive metadata now written into PNG files, allowing reliable regeneration   of images and tweaking of previous settings.</li> <li>Command-line completion in <code>invoke.py</code> now works on Windows, Linux and Mac   platforms.</li> <li>Improved command-line completion behavior New commands   added:</li> <li>List command-line history with <code>!history</code></li> <li>Search command-line history with <code>!search</code></li> <li>Clear history with <code>!clear</code></li> <li>Deprecated <code>--full_precision</code> / <code>-F</code>. Simply omit it and <code>invoke.py</code> will auto   configure. To switch away from auto use the new flag like   <code>--precision=float32</code>.</li> </ul>"},{"location":"CHANGELOG/#v114-11-september-2022","title":"v1.14 (11 September 2022)","text":"<ul> <li>Memory optimizations for small-RAM cards. 512x512 now possible on 4 GB GPUs.</li> <li>Full support for Apple hardware with M1 or M2 chips.</li> <li>Add \"seamless mode\" for circular tiling of image. Generates beautiful effects.   (prixt).</li> <li>Inpainting support.</li> <li>Improved web server GUI.</li> <li>Lots of code and documentation cleanups.</li> </ul>"},{"location":"CHANGELOG/#v113-3-september-2022","title":"v1.13 (3 September 2022)","text":"<ul> <li>Support image variations (see VARIATIONS   (Kevin Gibbons and many contributors and   reviewers)</li> <li>Supports a Google Colab notebook for a standalone server running on Google   hardware Arturo Mendivil</li> <li>WebUI supports GFPGAN/ESRGAN facial reconstruction and upscaling   Kevin Gibbons</li> <li>WebUI supports incremental display of in-progress images during generation   Kevin Gibbons</li> <li>A new configuration file scheme that allows new models (including upcoming   stable-diffusion-v1.5) to be added without altering the code.   (David Wager)</li> <li>Can specify --grid on invoke.py command line as the default.</li> <li>Miscellaneous internal bug and stability fixes.</li> <li>Works on M1 Apple hardware.</li> <li>Multiple bug fixes.</li> </ul>"},{"location":"CHANGELOG/#v112-28-august-2022","title":"v1.12 (28 August 2022)","text":"<ul> <li>Improved file handling, including ability to read prompts from standard input.   (kudos to Yunsaki</li> <li>The web server is now integrated with the invoke.py script. Invoke by adding   --web to the invoke.py command arguments.</li> <li>Face restoration and upscaling via GFPGAN and Real-ESGAN are now automatically   enabled if the GFPGAN directory is located as a sibling to Stable Diffusion.   VRAM requirements are modestly reduced. Thanks to both   Blessedcoolant and   Oceanswave for their work on this.</li> <li>You can now swap samplers on the invoke&gt; command line.   Blessedcoolant</li> </ul>"},{"location":"CHANGELOG/#v111-26-august-2022","title":"v1.11 (26 August 2022)","text":"<ul> <li>NEW FEATURE: Support upscaling and face enhancement using the GFPGAN module.   (kudos to Oceanswave</li> <li>You now can specify a seed of -1 to use the previous image's seed, -2 to use   the seed for the image generated before that, etc. Seed memory only extends   back to the previous command, but will work on all images generated with the   -n# switch.</li> <li>Variant generation support temporarily disabled pending more general solution.</li> <li>Created a feature branch named yunsaki-morphing-invoke which adds   experimental support for iteratively modifying the prompt and its parameters.   Please   seePull Request #86 for   a synopsis of how this works. Note that when this feature is eventually added   to the main branch, it will may be modified significantly.</li> </ul>"},{"location":"CHANGELOG/#v110-25-august-2022","title":"v1.10 (25 August 2022)","text":"<ul> <li>A barebones but fully functional interactive web server for online generation   of txt2img and img2img.</li> </ul>"},{"location":"CHANGELOG/#v109-24-august-2022","title":"v1.09 (24 August 2022)","text":"<ul> <li>A new -v option allows you to generate multiple variants of an initial image   in img2img mode. (kudos to Oceanswave.    See this discussion in the PR for examples and details on use)</li> <li>Added ability to personalize text to image generation (kudos to   Oceanswave and   nicolai256)</li> <li>Enabled all of the samplers from k_diffusion</li> </ul>"},{"location":"CHANGELOG/#v108-24-august-2022","title":"v1.08 (24 August 2022)","text":"<ul> <li>Escape single quotes on the invoke&gt; command before trying to parse. This   avoids parse errors.</li> <li>Removed instruction to get Python3.8 as first step in Windows install.   Anaconda3 does it for you.</li> <li>Added bounds checks for numeric arguments that could cause crashes.</li> <li>Cleaned up the copyright and license agreement files.</li> </ul>"},{"location":"CHANGELOG/#v107-23-august-2022","title":"v1.07 (23 August 2022)","text":"<ul> <li>Image filenames will now never fill gaps in the sequence, but will be assigned   the next higher name in the chosen directory. This ensures that the alphabetic   and chronological sort orders are the same.</li> </ul>"},{"location":"CHANGELOG/#v106-23-august-2022","title":"v1.06 (23 August 2022)","text":"<ul> <li>Added weighted prompt support contributed by   xraxra</li> <li>Example of using weighted prompts to tweak a demonic figure contributed by   bmaltais</li> </ul>"},{"location":"CHANGELOG/#v105-22-august-2022-after-the-drop","title":"v1.05 (22 August 2022 - after the drop)","text":"<ul> <li>Filenames now use the following formats: 000010.95183149.png -- Two files   produced by the same command (e.g. -n2), 000010.26742632.png -- distinguished   by a different seed.</li> </ul> <p>000011.455191342.01.png -- Two files produced by the same command using   000011.455191342.02.png -- a batch size&gt;1 (e.g. -b2). They have the same seed.</p> <p>000011.4160627868.grid#1-4.png -- a grid of four images (-g); the whole grid   can be regenerated with the indicated key</p> <ul> <li>It should no longer be possible for one image to overwrite another</li> <li>You can use the \"cd\" and \"pwd\" commands at the invoke&gt; prompt to set and   retrieve the path of the output directory.</li> </ul>"},{"location":"CHANGELOG/#v104-22-august-2022-after-the-drop","title":"v1.04 (22 August 2022 - after the drop)","text":"<ul> <li>Updated README to reflect installation of the released weights.</li> <li>Suppressed very noisy and inconsequential warning when loading the frozen CLIP   tokenizer.</li> </ul>"},{"location":"CHANGELOG/#v103-22-august-2022","title":"v1.03 (22 August 2022)","text":"<ul> <li>The original txt2img and img2img scripts from the CompViz repository have been   moved into a subfolder named \"orig_scripts\", to reduce confusion.</li> </ul>"},{"location":"CHANGELOG/#v102-21-august-2022","title":"v1.02 (21 August 2022)","text":"<ul> <li>A copy of the prompt and all of its switches and options is now stored in the   corresponding image in a tEXt metadata field named \"Dream\". You can read the   prompt using scripts/images2prompt.py, or an image editor that allows you to   explore the full metadata. Please run \"conda env update\" to load the k_lms   dependencies!!</li> </ul>"},{"location":"CHANGELOG/#v101-21-august-2022","title":"v1.01 (21 August 2022)","text":"<ul> <li>added k_lms sampling. Please run \"conda env update\" to load the k_lms   dependencies!!</li> <li>use half precision arithmetic by default, resulting in faster execution and   lower memory requirements Pass argument --full_precision to invoke.py to get   slower but more accurate image generation</li> </ul>"},{"location":"CHANGELOG/#links","title":"Links","text":"<ul> <li>Read Me</li> </ul>"},{"location":"CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or   advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at https://github.com/invoke-ai/InvokeAI/issues.  All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"CODE_OF_CONDUCT/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"CODE_OF_CONDUCT/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"CODE_OF_CONDUCT/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"CODE_OF_CONDUCT/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior,  harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"RELEASE/","title":"Release Process","text":"<p>The app is published in twice, in different build formats.</p> <ul> <li>A PyPI distribution. This includes both a source distribution and built distribution (a wheel). Users install with <code>pip install invokeai</code>. The updater uses this build.</li> <li>An installer on the InvokeAI Releases Page. This is a zip file with install scripts and a wheel. This is only used for new installs.</li> </ul>"},{"location":"RELEASE/#general-prep","title":"General Prep","text":"<p>Make a developer call-out for PRs to merge. Merge and test things out.</p> <p>While the release workflow does not include end-to-end tests, it does pause before publishing so you can download and test the final build.</p>"},{"location":"RELEASE/#release-workflow","title":"Release Workflow","text":"<p>The <code>release.yml</code> workflow runs a number of jobs to handle code checks, tests, build and publish on PyPI.</p> <p>It is triggered on tag push, when the tag matches <code>v*</code>. It doesn't matter if you've prepped a release branch like <code>release/v3.5.0</code> or are releasing from <code>main</code> - it works the same.</p> <p>Because commits are reference-counted, it is safe to create a release branch, tag it, let the workflow run, then delete the branch. So long as the tag exists, that commit will exist.</p>"},{"location":"RELEASE/#triggering-the-workflow","title":"Triggering the Workflow","text":"<p>Run <code>make tag-release</code> to tag the current commit and kick off the workflow.</p> <p>The release may also be dispatched manually.</p>"},{"location":"RELEASE/#workflow-jobs-and-process","title":"Workflow Jobs and Process","text":"<p>The workflow consists of a number of concurrently-run jobs, and two final publish jobs.</p> <p>The publish jobs require manual approval and are only run if the other jobs succeed.</p>"},{"location":"RELEASE/#check-version-job","title":"<code>check-version</code> Job","text":"<p>This job checks that the git ref matches the app version. It matches the ref against the <code>__version__</code> variable in <code>invokeai/version/invokeai_version.py</code>.</p> <p>When the workflow is triggered by tag push, the ref is the tag. If the workflow is run manually, the ref is the target selected from the Use workflow from dropdown.</p> <p>This job uses samuelcolvin/check-python-version.</p> <p>Any valid version specifier works, so long as the tag matches the version. The release workflow works exactly the same for <code>RC</code>, <code>post</code>, <code>dev</code>, etc.</p>"},{"location":"RELEASE/#check-and-test-jobs","title":"Check and Test Jobs","text":"<ul> <li><code>python-tests</code>: runs <code>pytest</code> on matrix of platforms</li> <li><code>python-checks</code>: runs <code>ruff</code> (format and lint)</li> <li><code>frontend-tests</code>: runs <code>vitest</code></li> <li><code>frontend-checks</code>: runs <code>prettier</code> (format), <code>eslint</code> (lint), <code>dpdm</code> (circular refs), <code>tsc</code> (static type check) and <code>knip</code> (unused imports)</li> </ul> <p>TODO We should add <code>mypy</code> or <code>pyright</code> to the <code>check-python</code> job.</p> <p>TODO We should add an end-to-end test job that generates an image.</p>"},{"location":"RELEASE/#build-installer-job","title":"<code>build-installer</code> Job","text":"<p>This sets up both python and frontend dependencies and builds the python package. Internally, this runs <code>installer/create_installer.sh</code> and uploads two artifacts:</p> <ul> <li><code>dist</code>: the python distribution, to be published on PyPI</li> <li><code>InvokeAI-installer-${VERSION}.zip</code>: the installer to be included in the GitHub release</li> </ul>"},{"location":"RELEASE/#sanity-check-smoke-test","title":"Sanity Check &amp; Smoke Test","text":"<p>At this point, the release workflow pauses as the remaining publish jobs require approval. Time to test the installer.</p> <p>Because the installer pulls from PyPI, and we haven't published to PyPI yet, you will need to install from the wheel:</p> <ul> <li>Download and unzip <code>dist.zip</code> and the installer from the Summary tab of the workflow</li> <li>Run the installer script using the <code>--wheel</code> CLI arg, pointing at the wheel:</li> </ul> <pre><code>./install.sh --wheel ../InvokeAI-4.0.0rc6-py3-none-any.whl\n</code></pre> <ul> <li>Install to a temporary directory so you get the new user experience</li> <li>Download a model and generate</li> </ul> <p>The same wheel file is bundled in the installer and in the <code>dist</code> artifact, which is uploaded to PyPI. You should end up with the exactly the same installation as if the installer got the wheel from PyPI.</p>"},{"location":"RELEASE/#something-isnt-right","title":"Something isn't right","text":"<p>If testing reveals any issues, no worries. Cancel the workflow, which will cancel the pending publish jobs (you didn't approve them prematurely, right?).</p> <p>Now you can start from the top:</p> <ul> <li>Fix the issues and PR the fixes per usual</li> <li>Get the PR approved and merged per usual</li> <li>Switch to <code>main</code> and pull in the fixes</li> <li>Run <code>make tag-release</code> to move the tag to <code>HEAD</code> (which has the fixes) and kick off the release workflow again</li> <li>Re-do the sanity check</li> </ul>"},{"location":"RELEASE/#pypi-publish-jobs","title":"PyPI Publish Jobs","text":"<p>The publish jobs will run if any of the previous jobs fail.</p> <p>They use GitHub environments, which are configured as trusted publishers on PyPI.</p> <p>Both jobs require a maintainer to approve them from the workflow's Summary tab.</p> <ul> <li>Click the Review deployments button</li> <li>Select the environment (either <code>testpypi</code> or <code>pypi</code>)</li> <li>Click Approve and deploy</li> </ul> <p>If the version already exists on PyPI, the publish jobs will fail. PyPI only allows a given version to be published once - you cannot change it. If version published on PyPI has a problem, you'll need to \"fail forward\" by bumping the app version and publishing a followup release.</p>"},{"location":"RELEASE/#failing-pypi-publish","title":"Failing PyPI Publish","text":"<p>Check the python infrastructure status page for incidents.</p> <p>If there are no incidents, contact @hipsterusername or @lstein, who have owner access to GH and PyPI, to see if access has expired or something like that.</p>"},{"location":"RELEASE/#publish-testpypi-job","title":"<code>publish-testpypi</code> Job","text":"<p>Publishes the distribution on the Test PyPI index, using the <code>testpypi</code> GitHub environment.</p> <p>This job is not required for the production PyPI publish, but included just in case you want to test the PyPI release.</p> <p>If approved and successful, you could try out the test release like this:</p> <pre><code># Create a new virtual environment\npython -m venv ~/.test-invokeai-dist --prompt test-invokeai-dist\n# Install the distribution from Test PyPI\npip install --index-url https://test.pypi.org/simple/ invokeai\n# Run and test the app\ninvokeai-web\n# Cleanup\ndeactivate\nrm -rf ~/.test-invokeai-dist\n</code></pre>"},{"location":"RELEASE/#publish-pypi-job","title":"<code>publish-pypi</code> Job","text":"<p>Publishes the distribution on the production PyPI index, using the <code>pypi</code> GitHub environment.</p>"},{"location":"RELEASE/#publish-the-github-release-with-installer","title":"Publish the GitHub Release with installer","text":"<p>Once the release is published to PyPI, it's time to publish the GitHub release.</p> <ol> <li>Draft a new release on GitHub, choosing the tag that triggered the release.</li> <li>Write the release notes, describing important changes. The Generate release notes button automatically inserts the changelog and new contributors, and you can copy/paste the intro from previous releases.</li> <li>Use <code>scripts/get_external_contributions.py</code> to get a list of external contributions to shout out in the release notes.</li> <li>Upload the zip file created in <code>build</code> job into the Assets section of the release notes.</li> <li>Check Set as a pre-release if it's a pre-release.</li> <li>Check Create a discussion for this release.</li> <li>Publish the release.</li> <li>Announce the release in Discord.</li> </ol> <p>TODO Workflows can create a GitHub release from a template and upload release assets. One popular action to handle this is ncipollo/release-action. A future enhancement to the release process could set this up.</p>"},{"location":"RELEASE/#manual-build","title":"Manual Build","text":"<p>The <code>build installer</code> workflow can be dispatched manually. This is useful to test the installer for a given branch or tag.</p> <p>No checks are run, it just builds.</p>"},{"location":"RELEASE/#manual-release","title":"Manual Release","text":"<p>The <code>release</code> workflow can be dispatched manually. You must dispatch the workflow from the right tag, else it will fail the version check.</p> <p>This functionality is available as a fallback in case something goes wonky. Typically, releases should be triggered via tag push as described above.</p>"},{"location":"contributing/ARCHITECTURE/","title":"Invoke.AI Architecture","text":"<pre><code>flowchart TB\n\n  subgraph apps[Applications]\n    webui[WebUI]\n    cli[CLI]\n\n  subgraph webapi[Web API]\n    api[HTTP API]\n    sio[Socket.IO]\n  end\n\n  end\n\n  subgraph invoke[Invoke]\n    direction LR\n    invoker\n    services\n    sessions\n    invocations\n  end\n\n  subgraph core[AI Core]\n    Generate\n  end\n\n  webui --&gt; webapi\n  webapi --&gt; invoke\n  cli --&gt; invoke\n\n  invoker --&gt; services &amp; sessions\n  invocations --&gt; services\n  sessions --&gt; invocations\n\n  services --&gt; core\n\n  %% Styles\n  classDef sg fill:#5028C8,font-weight:bold,stroke-width:2,color:#fff,stroke:#14141A\n  classDef default stroke-width:2px,stroke:#F6B314,color:#fff,fill:#14141A\n\n  class apps,webapi,invoke,core sg\n</code></pre>"},{"location":"contributing/ARCHITECTURE/#applications","title":"Applications","text":"<p>Applications are built on top of the invoke framework. They should construct <code>invoker</code> and then interact through it. They should avoid interacting directly with core code in order to support a variety of configurations.</p>"},{"location":"contributing/ARCHITECTURE/#web-ui","title":"Web UI","text":"<p>The Web UI is built on top of an HTTP API built with FastAPI and Socket.IO. The frontend code is found in <code>/frontend</code> and the backend code is found in <code>/ldm/invoke/app/api_app.py</code> and <code>/ldm/invoke/app/api/</code>. The code is further organized as such:</p> Component Description api_app.py Sets up the API app, annotates the OpenAPI spec with additional data, and runs the API dependencies Creates all invoker services and the invoker, and provides them to the API events An eventing system that could in the future be adapted to support horizontal scale-out sockets The Socket.IO interface - handles listening to and emitting session events (events are defined in the events service module) routers API definitions for different areas of API functionality"},{"location":"contributing/ARCHITECTURE/#cli","title":"CLI","text":"<p>The CLI is built automatically from invocation metadata, and also supports invocation piping and auto-linking. Code is available in <code>/ldm/invoke/app/cli_app.py</code>.</p>"},{"location":"contributing/ARCHITECTURE/#invoke","title":"Invoke","text":"<p>The Invoke framework provides the interface to the underlying AI systems and is built with flexibility and extensibility in mind. There are four major concepts: invoker, sessions, invocations, and services.</p>"},{"location":"contributing/ARCHITECTURE/#invoker","title":"Invoker","text":"<p>The invoker (<code>/ldm/invoke/app/services/invoker.py</code>) is the primary interface through which applications interact with the framework. Its primary purpose is to create, manage, and invoke sessions. It also maintains two sets of services: - invocation services, which are used by invocations to interact with core functionality. - invoker services, which are used by the invoker to manage sessions and manage the invocation queue.</p>"},{"location":"contributing/ARCHITECTURE/#sessions","title":"Sessions","text":"<p>Invocations and links between them form a graph, which is maintained in a session. Sessions can be queued for invocation, which will execute their graph (either the next ready invocation, or all invocations). Sessions also maintain execution history for the graph (including storage of any outputs). An invocation may be added to a session at any time, and there is capability to add and entire graph at once, as well as to automatically link new invocations to previous invocations. Invocations can not be deleted or modified once added.</p> <p>The session graph does not support looping. This is left as an application problem to prevent additional complexity in the graph.</p>"},{"location":"contributing/ARCHITECTURE/#invocations","title":"Invocations","text":"<p>Invocations represent individual units of execution, with inputs and outputs. All invocations are located in <code>/ldm/invoke/app/invocations</code>, and are all automatically discovered and made available in the applications. These are the primary way to expose new functionality in Invoke.AI, and the implementation guide explains how to add new invocations.</p>"},{"location":"contributing/ARCHITECTURE/#services","title":"Services","text":"<p>Services provide invocations access AI Core functionality and other necessary functionality (e.g. image storage). These are available in <code>/ldm/invoke/app/services</code>. As a general rule, new services should provide an interface as an abstract base class, and may provide a lightweight local implementation by default in their module. The goal for all services should be to enable the usage of different implementations (e.g. using cloud storage for image storage), but should not load any module dependencies unless that implementation has been used (i.e. don't import anything that won't be used, especially if it's expensive to import).</p>"},{"location":"contributing/ARCHITECTURE/#ai-core","title":"AI Core","text":"<p>The AI Core is represented by the rest of the code base (i.e. the code outside of <code>/ldm/invoke/app/</code>).</p>"},{"location":"contributing/CONTRIBUTING/","title":"Contributing","text":"<p>Invoke AI originated as a project built by the community, and that vision carries forward today as we aim to build the best pro-grade tools available. We work together to incorporate the latest in AI/ML research, making these tools available in over 20 languages to artists and creatives around the world as part of our fully permissive OSS project designed for individual users to self-host and use.</p>"},{"location":"contributing/CONTRIBUTING/#methods-of-contributing-to-invoke-ai","title":"Methods of Contributing to Invoke AI","text":"<p>Anyone who wishes to contribute to InvokeAI, whether features, bug fixes, code cleanup, testing, code reviews, documentation or translation is very much encouraged to do so.</p>"},{"location":"contributing/CONTRIBUTING/#development","title":"Development","text":"<p>If you\u2019d like to help with development, please see our development guide. </p> <p>New Contributors: If you\u2019re unfamiliar with contributing to open source projects, take a look at our new contributor guide.</p>"},{"location":"contributing/CONTRIBUTING/#nodes","title":"Nodes","text":"<p>If you\u2019d like to add a Node, please see our nodes contribution guide.</p>"},{"location":"contributing/CONTRIBUTING/#support-and-triaging","title":"Support and Triaging","text":"<p>Helping support other users in Discord and on Github are valuable forms of contribution that we greatly appreciate. </p> <p>We receive many issues and requests for help from users. We're limited in bandwidth relative to our the user base, so providing answers to questions or helping identify causes of issues is very helpful. By doing this, you enable us to spend time on the highest priority work. </p>"},{"location":"contributing/CONTRIBUTING/#documentation","title":"Documentation","text":"<p>If you\u2019d like to help with documentation, please see our documentation guide.</p>"},{"location":"contributing/CONTRIBUTING/#translation","title":"Translation","text":"<p>If you'd like to help with translation, please see our\u00a0translation guide.</p>"},{"location":"contributing/CONTRIBUTING/#tutorials","title":"Tutorials","text":"<p>Please reach out to @imic or @hipsterusername on Discord to help create tutorials for InvokeAI.</p> <p>We hope you enjoy using our software as much as we enjoy creating it, and we hope that some of those of you who are reading this will elect to become part of our contributor community.</p>"},{"location":"contributing/CONTRIBUTING/#contributors","title":"Contributors","text":"<p>This project is a combined effort of dedicated people from across the world.\u00a0Check out the list of all these amazing people. We thank them for their time, hard work and effort.</p>"},{"location":"contributing/CONTRIBUTING/#code-of-conduct","title":"Code of Conduct","text":"<p>The InvokeAI community is a welcoming place, and we want your help in maintaining that. Please review our Code of Conduct to learn more - it's essential to maintaining a respectful and inclusive environment.</p> <p>By making a contribution to this project, you certify that:</p> <ol> <li>The contribution was created in whole or in part by you and you have the right to submit it under the open-source license indicated in this project\u2019s GitHub repository; or</li> <li>The contribution is based upon previous work that, to the best of your knowledge, is covered under an appropriate open-source license and you have the right under that license to submit that work with modifications, whether created in whole or in part by you, under the same open-source license (unless you are permitted to submit under a different license); or</li> <li>The contribution was provided directly to you by some other person who certified (1) or (2) and you have not modified it; or</li> <li>You understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information you submit with it, including your sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open-source license(s) involved.</li> </ol> <p>This disclaimer is not a license and does not grant any rights or permissions. You must obtain necessary permissions and licenses, including from third parties, before contributing to this project.</p> <p>This disclaimer is provided \"as is\" without warranty of any kind, whether expressed or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose, or non-infringement. In no event shall the authors or copyright holders be liable for any claim, damages, or other liability, whether in an action of contract, tort, or otherwise, arising from, out of, or in connection with the contribution or the use or other dealings in the contribution.</p>"},{"location":"contributing/CONTRIBUTING/#support","title":"Support","text":"<p>For support, please use this repository's GitHub Issues, or join the Discord.</p> <p>Original portions of the software are Copyright \u00a9 2023 by respective contributors.</p> <p>Remember, your contributions help make this project great. We're excited to see what you'll bring to our community!</p>"},{"location":"contributing/DOWNLOAD_QUEUE/","title":"The InvokeAI Download Queue","text":"<p>The DownloadQueueService provides a multithreaded parallel download queue for arbitrary URLs, with queue prioritization, event handling, and restart capabilities.</p>"},{"location":"contributing/DOWNLOAD_QUEUE/#simple-example","title":"Simple Example","text":"<pre><code>from invokeai.app.services.download import DownloadQueueService, TqdmProgress\n\ndownload_queue = DownloadQueueService()\nfor url in ['https://github.com/invoke-ai/InvokeAI/blob/main/invokeai/assets/a-painting-of-a-fire.png?raw=true',\n            'https://github.com/invoke-ai/InvokeAI/blob/main/invokeai/assets/birdhouse.png?raw=true',\n            'https://github.com/invoke-ai/InvokeAI/blob/main/invokeai/assets/missing.png',\n            'https://civitai.com/api/download/models/152309?type=Model&amp;format=SafeTensor',\n            ]:\n\n    # urls start downloading as soon as download() is called\n    download_queue.download(source=url,\n                            dest='/tmp/downloads',\n                            on_progress=TqdmProgress().update\n                            )\n\ndownload_queue.join()  # wait for all downloads to finish\nfor job in download_queue.list_jobs():\n    print(job.model_dump_json(exclude_none=True, indent=4),\"\\n\")\n</code></pre> <p>Output:</p> <pre><code>{\n    \"source\": \"https://github.com/invoke-ai/InvokeAI/blob/main/invokeai/assets/a-painting-of-a-fire.png?raw=true\",\n    \"dest\": \"/tmp/downloads\",\n    \"id\": 0,\n    \"priority\": 10,\n    \"status\": \"completed\",\n    \"download_path\": \"/tmp/downloads/a-painting-of-a-fire.png\",\n    \"job_started\": \"2023-12-04T05:34:41.742174\",\n    \"job_ended\": \"2023-12-04T05:34:42.592035\",\n    \"bytes\": 666734,\n    \"total_bytes\": 666734\n} \n\n{\n    \"source\": \"https://github.com/invoke-ai/InvokeAI/blob/main/invokeai/assets/birdhouse.png?raw=true\",\n    \"dest\": \"/tmp/downloads\",\n    \"id\": 1,\n    \"priority\": 10,\n    \"status\": \"completed\",\n    \"download_path\": \"/tmp/downloads/birdhouse.png\",\n    \"job_started\": \"2023-12-04T05:34:41.741975\",\n    \"job_ended\": \"2023-12-04T05:34:42.652841\",\n    \"bytes\": 774949,\n    \"total_bytes\": 774949\n}\n\n{\n    \"source\": \"https://github.com/invoke-ai/InvokeAI/blob/main/invokeai/assets/missing.png\",\n    \"dest\": \"/tmp/downloads\",\n    \"id\": 2,\n    \"priority\": 10,\n    \"status\": \"error\",\n    \"job_started\": \"2023-12-04T05:34:41.742079\",\n    \"job_ended\": \"2023-12-04T05:34:42.147625\",\n    \"bytes\": 0,\n    \"total_bytes\": 0,\n    \"error_type\": \"HTTPError(Not Found)\",\n    \"error\": \"Traceback (most recent call last):\\n  File \\\"/home/lstein/Projects/InvokeAI/invokeai/app/services/download/download_default.py\\\", line 182, in _download_next_item\\n    self._do_download(job)\\n  File \\\"/home/lstein/Projects/InvokeAI/invokeai/app/services/download/download_default.py\\\", line 206, in _do_download\\n    raise HTTPError(resp.reason)\\nrequests.exceptions.HTTPError: Not Found\\n\"\n}\n\n{\n    \"source\": \"https://civitai.com/api/download/models/152309?type=Model&amp;format=SafeTensor\",\n    \"dest\": \"/tmp/downloads\",\n    \"id\": 3,\n    \"priority\": 10,\n    \"status\": \"completed\",\n    \"download_path\": \"/tmp/downloads/xl_more_art-full_v1.safetensors\",\n    \"job_started\": \"2023-12-04T05:34:42.147645\",\n    \"job_ended\": \"2023-12-04T05:34:43.735990\",\n    \"bytes\": 719020768,\n    \"total_bytes\": 719020768\n} \n</code></pre>"},{"location":"contributing/DOWNLOAD_QUEUE/#the-api","title":"The API","text":"<p>The default download queue is <code>DownloadQueueService</code>, an implementation of ABC <code>DownloadQueueServiceBase</code>. It juggles multiple background download requests and provides facilities for interrogating and cancelling the requests. Access to a current or past download task is mediated via <code>DownloadJob</code> objects which report the current status of a job request</p>"},{"location":"contributing/DOWNLOAD_QUEUE/#the-queue-object","title":"The Queue Object","text":"<p>A default download queue is located in <code>ApiDependencies.invoker.services.download_queue</code>. However, you can create additional instances if you need to isolate your queue from the main one.</p> <pre><code>queue = DownloadQueueService(event_bus=events)\n</code></pre> <p><code>DownloadQueueService()</code> takes three optional arguments:</p> Argument Type Default Description <code>max_parallel_dl</code> int 5 Maximum number of simultaneous downloads allowed <code>event_bus</code> EventServiceBase None System-wide FastAPI event bus for reporting download events <code>requests_session</code> requests.sessions.Session None An alternative requests Session object to use for the download <p><code>max_parallel_dl</code> specifies how many download jobs are allowed to run simultaneously. Each will run in a different thread of execution.</p> <p><code>event_bus</code> is an EventServiceBase, typically the one created at InvokeAI startup. If present, download events are periodically emitted on this bus to allow clients to follow download progress.</p> <p><code>requests_session</code> is a url library requests Session object. It is used for testing.</p>"},{"location":"contributing/DOWNLOAD_QUEUE/#the-job-object","title":"The Job object","text":"<p>The queue operates on a series of download job objects. These objects specify the source and destination of the download, and keep track of the progress of the download.</p> <p>Two job types are defined. <code>DownloadJob</code> and <code>MultiFileDownloadJob</code>. The former is a pydantic object with the following fields:</p> Field Type Default Description Fields passed in at job creation time <code>source</code> AnyHttpUrl Where to download from <code>dest</code> Path Where to download to <code>access_token</code> str [optional] string containing authentication token for access <code>on_start</code> Callable [optional] callback when the download starts <code>on_progress</code> Callable [optional] callback called at intervals during download progress <code>on_complete</code> Callable [optional] callback called after successful download completion <code>on_error</code> Callable [optional] callback called after an error occurs <code>id</code> int auto assigned Job ID, an integer &gt;= 0 <code>priority</code> int 10 Job priority. Lower priorities run before higher priorities Fields updated over the course of the download task <code>status</code> DownloadJobStatus Status code <code>download_path</code> Path Path to the location of the downloaded file <code>job_started</code> float Timestamp for when the job started running <code>job_ended</code> float Timestamp for when the job completed or errored out <code>job_sequence</code> int A counter that is incremented each time a model is dequeued <code>bytes</code> int 0 Bytes downloaded so far <code>total_bytes</code> int 0 Total size of the file at the remote site <code>error_type</code> str String version of the exception that caused an error during download <code>error</code> str String version of the traceback associated with an error <code>cancelled</code> bool False Set to true if the job was cancelled by the caller <p>When you create a job, you can assign it a <code>priority</code>. If multiple jobs are queued, the job with the lowest priority runs first.</p> <p>Every job has a <code>source</code> and a <code>dest</code>. <code>source</code> is a pydantic.networks AnyHttpUrl object. The <code>dest</code> is a path on the local filesystem that specifies the destination for the downloaded object. Its semantics are described below.</p> <p>When the job is submitted, it is assigned a numeric <code>id</code>. The id can then be used to fetch the job object from the queue.</p> <p>The <code>status</code> field is updated by the queue to indicate where the job is in its lifecycle. Values are defined in the string enum <code>DownloadJobStatus</code>, a symbol available from <code>invokeai.app.services.download_manager</code>. Possible values are:</p> Value String Value ** Description ** <code>WAITING</code> waiting Job is on the queue but not yet running <code>RUNNING</code> running The download is started <code>COMPLETED</code> completed Job has finished its work without an error <code>ERROR</code> error Job encountered an error and will not run again <p><code>job_started</code> and <code>job_ended</code> indicate when the job was started (using a python timestamp) and when it completed.</p> <p>In case of an error, the job's status will be set to <code>DownloadJobStatus.ERROR</code>, the text of the Exception that caused the error will be placed in the <code>error_type</code> field and the traceback that led to the error will be in <code>error</code>.</p> <p>A cancelled job will have status <code>DownloadJobStatus.ERROR</code> and an <code>error_type</code> field of \"DownloadJobCancelledException\". In addition, the job's <code>cancelled</code> property will be set to True.</p> <p>The <code>MultiFileDownloadJob</code> is used for diffusers model downloads, which contain multiple files and directories under a common root:</p> Field Type Default Description Fields passed in at job creation time <code>download_parts</code> Set[DownloadJob] Component download jobs <code>dest</code> Path Where to download to <code>on_start</code> Callable [optional] callback when the download starts <code>on_progress</code> Callable [optional] callback called at intervals during download progress <code>on_complete</code> Callable [optional] callback called after successful download completion <code>on_error</code> Callable [optional] callback called after an error occurs <code>id</code> int auto assigned Job ID, an integer &gt;= 0 Fields updated over the course of the download task <code>status</code> DownloadJobStatus Status code <code>download_path</code> Path Path to the root of the downloaded files <code>bytes</code> int 0 Bytes downloaded so far <code>total_bytes</code> int 0 Total size of the file at the remote site <code>error_type</code> str String version of the exception that caused an error during download <code>error</code> str String version of the traceback associated with an error <code>cancelled</code> bool False Set to true if the job was cancelled by the caller <p>Note that the MultiFileDownloadJob does not support the <code>priority</code>, <code>job_started</code>, <code>job_ended</code> or <code>content_type</code> attributes. You can get these from the individual download jobs in <code>download_parts</code>.</p>"},{"location":"contributing/DOWNLOAD_QUEUE/#callbacks","title":"Callbacks","text":"<p>Download jobs can be associated with a series of callbacks, each with the signature <code>Callable[[\"DownloadJob\"], None]</code>. The callbacks are assigned using optional arguments <code>on_start</code>, <code>on_progress</code>, <code>on_complete</code> and <code>on_error</code>. When the corresponding event occurs, the callback wil be invoked and passed the job. The callback will be run in a <code>try:</code> context in the same thread as the download job. Any exceptions that occur during execution of the callback will be caught and converted into a log error message, thereby allowing the download to continue.</p>"},{"location":"contributing/DOWNLOAD_QUEUE/#tqdmprogress","title":"<code>TqdmProgress</code>","text":"<p>The <code>invokeai.app.services.download.download_default</code> module defines a class named <code>TqdmProgress</code> which can be used as an <code>on_progress</code> handler to display a completion bar in the console. Use as follows:</p> <pre><code>from invokeai.app.services.download import TqdmProgress\n\ndownload_queue.download(source='http://some.server.somewhere/some_file',\n                        dest='/tmp/downloads',\n                        on_progress=TqdmProgress().update\n                        )\n</code></pre>"},{"location":"contributing/DOWNLOAD_QUEUE/#events","title":"Events","text":"<p>If the queue was initialized with the InvokeAI event bus (the case when using <code>ApiDependencies.invoker.services.download_queue</code>), then download events will also be issued on the bus. The events are:</p> <ul> <li> <p><code>download_started</code> -- This is issued when a job is taken off the queue and a request is made to the remote server for the URL headers, but before any data has been downloaded. The event payload will contain the keys <code>source</code> and <code>download_path</code>. The latter contains the path that the URL will be downloaded to.</p> </li> <li> <p><code>download_progress -- This is issued periodically as the download runs. The payload contains the keys</code>source<code>,</code>download_path<code>,</code>current_bytes<code>and</code>total_bytes`. The latter two fields can be used to display the percent complete.</p> </li> <li> <p><code>download_complete</code> -- This is issued when the download completes successfully. The payload contains the keys <code>source</code>, <code>download_path</code> and <code>total_bytes</code>.</p> </li> <li> <p><code>download_error</code> -- This is issued when the download stops because of an error condition. The payload contains the fields <code>error_type</code> and <code>error</code>. The former is the text representation of the exception, and the latter is a traceback showing where the error occurred.</p> </li> </ul>"},{"location":"contributing/DOWNLOAD_QUEUE/#job-control","title":"Job control","text":"<p>To create a job call the queue's <code>download()</code> method. You can list all jobs using <code>list_jobs()</code>, fetch a single job by its with <code>id_to_job()</code>, cancel a running job with <code>cancel_job()</code>, cancel all running jobs with <code>cancel_all_jobs()</code>, and wait for all jobs to finish with <code>join()</code>.</p>"},{"location":"contributing/DOWNLOAD_QUEUE/#job-queuedownloadsource-dest-priority-access_token-on_start-on_progress-on_complete-on_cancelled-on_error","title":"job = queue.download(source, dest, priority, access_token, on_start, on_progress, on_complete, on_cancelled, on_error)","text":"<p>Create a new download job and put it on the queue, returning the DownloadJob object.</p>"},{"location":"contributing/DOWNLOAD_QUEUE/#multifile_job-queuemultifile_downloadparts-dest-access_token-on_start-on_progress-on_complete-on_cancelled-on_error","title":"multifile_job = queue.multifile_download(parts, dest, access_token, on_start, on_progress, on_complete, on_cancelled, on_error)","text":"<p>This is similar to download(), but instead of taking a single source, it accepts a <code>parts</code> argument consisting of a list of <code>RemoteModelFile</code> objects. Each part corresponds to a URL/Path pair, where the URL is the location of the remote file, and the Path is the destination.</p> <p><code>RemoteModelFile</code> can be imported from <code>invokeai.backend.model_manager.metadata</code>, and consists of a url/path pair. Note that the path must be relative.</p> <p>The method returns a <code>MultiFileDownloadJob</code>.</p> <pre><code>from invokeai.backend.model_manager.metadata import RemoteModelFile\nremote_file_1 = RemoteModelFile(url='http://www.foo.bar/my/pytorch_model.safetensors'',\n                                path='my_model/textencoder/pytorch_model.safetensors'\n                          )\nremote_file_2 = RemoteModelFile(url='http://www.bar.baz/vae.ckpt',\n                                path='my_model/vae/diffusers_model.safetensors'\n                          )\njob = queue.multifile_download(parts=[remote_file_1, remote_file_2],\n                               dest='/tmp/downloads',\n                               on_progress=TqdmProgress().update)\nqueue.wait_for_job(job)\nprint(f\"The files were downloaded to {job.download_path}\")\n</code></pre>"},{"location":"contributing/DOWNLOAD_QUEUE/#jobs-queuelist_jobs","title":"jobs = queue.list_jobs()","text":"<p>Return a list of all active and inactive <code>DownloadJob</code>s.</p>"},{"location":"contributing/DOWNLOAD_QUEUE/#job-queueid_to_jobid","title":"job = queue.id_to_job(id)","text":"<p>Return the job corresponding to given ID.</p> <p>Return a list of all active and inactive <code>DownloadJob</code>s.</p>"},{"location":"contributing/DOWNLOAD_QUEUE/#queueprune_jobs","title":"queue.prune_jobs()","text":"<p>Remove inactive (complete or errored) jobs from the listing returned by <code>list_jobs()</code>.</p>"},{"location":"contributing/DOWNLOAD_QUEUE/#queuejoin","title":"queue.join()","text":"<p>Block until all pending jobs have run to completion or errored out.</p>"},{"location":"contributing/INVOCATIONS/","title":"Nodes","text":"<p>Features in InvokeAI are added in the form of modular nodes systems called Invocations.</p> <p>An Invocation is simply a single operation that takes in some inputs and gives out some outputs. We can then chain multiple Invocations together to create more complex functionality.</p>"},{"location":"contributing/INVOCATIONS/#invocations-directory","title":"Invocations Directory","text":"<p>InvokeAI Nodes can be found in the <code>invokeai/app/invocations</code> directory. These can be used as examples to create your own nodes.</p> <p>New nodes should be added to a subfolder in <code>nodes</code> direction found at the root level of the InvokeAI installation location. Nodes added to this folder will be able to be used upon application startup.</p> <p>Example <code>nodes</code> subfolder structure:</p> <pre><code>\u251c\u2500\u2500 __init__.py # Invoke-managed custom node loader\n\u2502\n\u251c\u2500\u2500 cool_node\n\u2502   \u251c\u2500\u2500 __init__.py # see example below\n\u2502   \u2514\u2500\u2500 cool_node.py\n\u2502\n\u2514\u2500\u2500 my_node_pack\n    \u251c\u2500\u2500 __init__.py # see example below\n    \u251c\u2500\u2500 tasty_node.py\n    \u251c\u2500\u2500 bodacious_node.py\n    \u251c\u2500\u2500 utils.py\n    \u2514\u2500\u2500 extra_nodes\n        \u2514\u2500\u2500 fancy_node.py\n</code></pre> <p>Each node folder must have an <code>__init__.py</code> file that imports its nodes. Only nodes imported in the <code>__init__.py</code> file are loaded. See the README in the nodes folder for more examples:</p> <pre><code>from .cool_node import CoolInvocation\n</code></pre>"},{"location":"contributing/INVOCATIONS/#creating-a-new-invocation","title":"Creating A New Invocation","text":"<p>In order to understand the process of creating a new Invocation, let us actually create one.</p> <p>In our example, let us create an Invocation that will take in an image, resize it and output the resized image.</p> <p>The first set of things we need to do when creating a new Invocation are -</p> <ul> <li>Create a new class that derives from a predefined parent class called   <code>BaseInvocation</code>.</li> <li>Every Invocation must have a <code>docstring</code> that describes what this Invocation   does.</li> <li>While not strictly required, we suggest every invocation class name ends in   \"Invocation\", eg \"CropImageInvocation\".</li> <li>Every Invocation must use the <code>@invocation</code> decorator to provide its unique   invocation type. You may also provide its title, tags and category using the   decorator.</li> <li>Invocations are strictly typed. We make use of the native   typing library and the   installed pydantic library for   validation.</li> </ul> <p>So let us do that.</p> <pre><code>from invokeai.app.invocations.baseinvocation import BaseInvocation, invocation\n\n@invocation('resize')\nclass ResizeInvocation(BaseInvocation):\n    '''Resizes an image'''\n</code></pre> <p>That's great.</p> <p>Now we have setup the base of our new Invocation. Let us think about what inputs our Invocation takes.</p> <ul> <li>We need an <code>image</code> that we are going to resize.</li> <li>We will need new <code>width</code> and <code>height</code> values to which we need to resize the   image to.</li> </ul>"},{"location":"contributing/INVOCATIONS/#inputs","title":"Inputs","text":"<p>Every Invocation input must be defined using the <code>InputField</code> function. This is a wrapper around the pydantic <code>Field</code> function, which handles a few extra things and provides type hints. Like everything else, this should be strictly typed and defined.</p> <p>So let us create these inputs for our Invocation. First up, the <code>image</code> input we need. Generally, we can use standard variable types in Python but InvokeAI already has a custom <code>ImageField</code> type that handles all the stuff that is needed for image inputs.</p> <p>But what is this <code>ImageField</code> ..? It is a special class type specifically written to handle how images are dealt with in InvokeAI. We will cover how to create your own custom field types later in this guide. For now, let's go ahead and use it.</p> <pre><code>from invokeai.app.invocations.baseinvocation import BaseInvocation, InputField, invocation\nfrom invokeai.app.invocations.primitives import ImageField\n\n@invocation('resize')\nclass ResizeInvocation(BaseInvocation):\n\n    # Inputs\n    image: ImageField = InputField(description=\"The input image\")\n</code></pre> <p>Let us break down our input code.</p> <pre><code>image: ImageField = InputField(description=\"The input image\")\n</code></pre> Part Value Description Name <code>image</code> The variable that will hold our image Type Hint <code>ImageField</code> The types for our field. Indicates that the image must be an <code>ImageField</code> type. Field <code>InputField(description=\"The input image\")</code> The image variable is an <code>InputField</code> which needs a description. <p>Great. Now let us create our other inputs for <code>width</code> and <code>height</code></p> <pre><code>from invokeai.app.invocations.baseinvocation import BaseInvocation, InputField, invocation\nfrom invokeai.app.invocations.primitives import ImageField\n\n@invocation('resize')\nclass ResizeInvocation(BaseInvocation):\n    '''Resizes an image'''\n\n    image: ImageField = InputField(description=\"The input image\")\n    width: int = InputField(default=512, ge=64, le=2048, description=\"Width of the new image\")\n    height: int = InputField(default=512, ge=64, le=2048, description=\"Height of the new image\")\n</code></pre> <p>As you might have noticed, we added two new arguments to the <code>InputField</code> definition for <code>width</code> and <code>height</code>, called <code>gt</code> and <code>le</code>. They stand for greater than or equal to and less than or equal to.</p> <p>These impose contraints on those fields, and will raise an exception if the values do not meet the constraints. Field constraints are provided by pydantic, so anything you see in the pydantic docs will work.</p> <p>Note: Any time it is possible to define constraints for our field, we should do it so the frontend has more information on how to parse this field.</p> <p>Perfect. We now have our inputs. Let us do something with these.</p>"},{"location":"contributing/INVOCATIONS/#invoke-function","title":"Invoke Function","text":"<p>The <code>invoke</code> function is where all the magic happens. This function provides you the <code>context</code> parameter that is of the type <code>InvocationContext</code> which will give you access to the current context of the generation and all the other services that are provided by it by InvokeAI.</p> <p>Let us create this function first.</p> <pre><code>from invokeai.app.invocations.baseinvocation import BaseInvocation, InputField, invocation, InvocationContext\nfrom invokeai.app.invocations.primitives import ImageField\n\n@invocation('resize')\nclass ResizeInvocation(BaseInvocation):\n    '''Resizes an image'''\n\n    image: ImageField = InputField(description=\"The input image\")\n    width: int = InputField(default=512, ge=64, le=2048, description=\"Width of the new image\")\n    height: int = InputField(default=512, ge=64, le=2048, description=\"Height of the new image\")\n\n    def invoke(self, context: InvocationContext):\n        pass\n</code></pre>"},{"location":"contributing/INVOCATIONS/#outputs","title":"Outputs","text":"<p>The output of our Invocation will be whatever is returned by this <code>invoke</code> function. Like with our inputs, we need to strongly type and define our outputs too.</p> <p>What is our output going to be? Another image. Normally you'd have to create a type for this but InvokeAI already offers you an <code>ImageOutput</code> type that handles all the necessary info related to image outputs. So let us use that.</p> <p>We will cover how to create your own output types later in this guide.</p> <pre><code>from invokeai.app.invocations.baseinvocation import BaseInvocation, InputField, invocation, InvocationContext\nfrom invokeai.app.invocations.primitives import ImageField\nfrom invokeai.app.invocations.image import ImageOutput\n\n@invocation('resize')\nclass ResizeInvocation(BaseInvocation):\n    '''Resizes an image'''\n\n    image: ImageField = InputField(description=\"The input image\")\n    width: int = InputField(default=512, ge=64, le=2048, description=\"Width of the new image\")\n    height: int = InputField(default=512, ge=64, le=2048, description=\"Height of the new image\")\n\n    def invoke(self, context: InvocationContext) -&gt; ImageOutput:\n        pass\n</code></pre> <p>Perfect. Now that we have our Invocation setup, let us do what we want to do.</p> <ul> <li>We will first load the image using one of the services provided by InvokeAI to   load the image.</li> <li>We will resize the image using <code>PIL</code> to our input data.</li> <li>We will output this image in the format we set above.</li> </ul> <p>So let's do that.</p> <pre><code>from invokeai.app.invocations.baseinvocation import BaseInvocation, InputField, invocation, InvocationContext\nfrom invokeai.app.invocations.primitives import ImageField\nfrom invokeai.app.invocations.image import ImageOutput, ResourceOrigin, ImageCategory\n\n@invocation(\"resize\")\nclass ResizeInvocation(BaseInvocation):\n    \"\"\"Resizes an image\"\"\"\n\n    image: ImageField = InputField(description=\"The input image\")\n    width: int = InputField(default=512, ge=64, le=2048, description=\"Width of the new image\")\n    height: int = InputField(default=512, ge=64, le=2048, description=\"Height of the new image\")\n\n    def invoke(self, context: InvocationContext) -&gt; ImageOutput:\n        # Load the input image as a PIL image\n        image = context.images.get_pil(self.image.image_name)\n\n        # Resize the image\n        resized_image = image.resize((self.width, self.height))\n\n        # Save the image\n        image_dto = context.images.save(image=resized_image)\n\n        # Return an ImageOutput\n        return ImageOutput.build(image_dto)\n</code></pre> <p>Note: Do not be overwhelmed by the <code>ImageOutput</code> process. InvokeAI has a certain way that the images need to be dispatched in order to be stored and read correctly. In 99% of the cases when dealing with an image output, you can simply copy-paste the template above.</p>"},{"location":"contributing/INVOCATIONS/#customization","title":"Customization","text":"<p>We can use the <code>@invocation</code> decorator to provide some additional info to the UI, like a custom title, tags and category.</p> <p>We also encourage providing a version. This must be a semver version string (\"\\(MAJOR.\\)MINOR.$PATCH\"). The UI will let users know if their workflow is using a mismatched version of the node.</p> <pre><code>@invocation(\"resize\", title=\"My Resizer\", tags=[\"resize\", \"image\"], category=\"My Invocations\", version=\"1.0.0\")\nclass ResizeInvocation(BaseInvocation):\n    \"\"\"Resizes an image\"\"\"\n\n    image: ImageField = InputField(description=\"The input image\")\n    ...\n</code></pre> <p>That's it. You made your own Resize Invocation.</p>"},{"location":"contributing/INVOCATIONS/#result","title":"Result","text":"<p>Once you make your Invocation correctly, the rest of the process is fully automated for you.</p> <p>When you launch InvokeAI, you can go to <code>http://localhost:9090/docs</code> and see your new Invocation show up there with all the relevant info.</p> <p></p> <p>When you launch the frontend UI, you can go to the Node Editor tab and find your new Invocation ready to be used.</p> <p></p>"},{"location":"contributing/INVOCATIONS/#contributing-nodes","title":"Contributing Nodes","text":"<p>Once you've created a Node, the next step is to share it with the community! The best way to do this is to submit a Pull Request to add the Node to the Community Nodes list. If you're not sure how to do that, take a look a at our contributing nodes overview.</p>"},{"location":"contributing/INVOCATIONS/#advanced","title":"Advanced","text":""},{"location":"contributing/INVOCATIONS/#custom-output-types","title":"Custom Output Types","text":"<p>Like with custom inputs, sometimes you might find yourself needing custom outputs that InvokeAI does not provide. We can easily set one up.</p> <p>Now that you are familiar with Invocations and Inputs, let us use that knowledge to create an output that has an <code>image</code> field, a <code>color</code> field and a <code>string</code> field.</p> <ul> <li>An invocation output is a class that derives from the parent class of   <code>BaseInvocationOutput</code>.</li> <li>All invocation outputs must use the <code>@invocation_output</code> decorator to provide   their unique output type.</li> <li>Output fields must use the provided <code>OutputField</code> function. This is very   similar to the <code>InputField</code> function described earlier - it's a wrapper around   <code>pydantic</code>'s <code>Field()</code>.</li> <li>It is not mandatory but we recommend using names ending with <code>Output</code> for   output types.</li> <li>It is not mandatory but we highly recommend adding a <code>docstring</code> to describe   what your output type is for.</li> </ul> <p>Now that we know the basic rules for creating a new output type, let us go ahead and make it.</p> <pre><code>from .baseinvocation import BaseInvocationOutput, OutputField, invocation_output\nfrom .primitives import ImageField, ColorField\n\n@invocation_output('image_color_string_output')\nclass ImageColorStringOutput(BaseInvocationOutput):\n    '''Base class for nodes that output a single image'''\n\n    image: ImageField = OutputField(description=\"The image\")\n    color: ColorField = OutputField(description=\"The color\")\n    text: str = OutputField(description=\"The string\")\n</code></pre> <p>That's all there is to it.</p>"},{"location":"contributing/INVOCATIONS/#custom-input-fields","title":"Custom Input Fields","text":"<p>Now that you know how to create your own Invocations, let us dive into slightly more advanced topics.</p> <p>While creating your own Invocations, you might run into a scenario where the existing fields in InvokeAI do not meet your requirements. In such cases, you can create your own fields.</p> <p>Let us create one as an example. Let us say we want to create a color input field that represents a color code. But before we start on that here are some general good practices to keep in mind.</p>"},{"location":"contributing/INVOCATIONS/#best-practices","title":"Best Practices","text":"<ul> <li>There is no naming convention for input fields but we highly recommend that   you name it something appropriate like <code>ColorField</code>.</li> <li>It is not mandatory but it is heavily recommended to add a relevant   <code>docstring</code> to describe your field.</li> <li>Keep your field in the same file as the Invocation that it is made for or in   another file where it is relevant.</li> </ul> <p>All input types a class that derive from the <code>BaseModel</code> type from <code>pydantic</code>. So let's create one.</p> <pre><code>from pydantic import BaseModel\n\nclass ColorField(BaseModel):\n    '''A field that holds the rgba values of a color'''\n    pass\n</code></pre> <p>Perfect. Now let us create the properties for our field. This is similar to how you created input fields for your Invocation. All the same rules apply. Let us create four fields representing the red\u00ae, blue(b), green(g) and alpha(a) channel of the color.</p> <p>Technically, the properties are also called fields - but in this case, it refers to a <code>pydantic</code> field.</p> <pre><code>class ColorField(BaseModel):\n    '''A field that holds the rgba values of a color'''\n    r: int = Field(ge=0, le=255, description=\"The red channel\")\n    g: int = Field(ge=0, le=255, description=\"The green channel\")\n    b: int = Field(ge=0, le=255, description=\"The blue channel\")\n    a: int = Field(ge=0, le=255, description=\"The alpha channel\")\n</code></pre> <p>That's it. We now have a new input field type that we can use in our Invocations like this.</p> <pre><code>color: ColorField = InputField(default=ColorField(r=0, g=0, b=0, a=0), description='Background color of an image')\n</code></pre>"},{"location":"contributing/INVOCATIONS/#using-the-custom-field","title":"Using the custom field","text":"<p>When you start the UI, your custom field will be automatically recognized.</p> <p>Custom fields only support connection inputs in the Workflow Editor.</p>"},{"location":"contributing/LOCAL_DEVELOPMENT/","title":"Local Development","text":"<p>If you are looking to contribute you will need to have a local development environment. See the Developer Install for full details.</p> <p>Broadly this involves cloning the repository, installing the pre-reqs, and InvokeAI (in editable form). Assuming this is working, choose your area of focus.</p>"},{"location":"contributing/LOCAL_DEVELOPMENT/#documentation","title":"Documentation","text":"<p>We use mkdocs for our documentation with the material theme. Documentation is written in markdown files under the <code>./docs</code> folder and then built into a static website for hosting with GitHub Pages at invoke-ai.github.io/InvokeAI.</p> <p>To contribute to the documentation you'll need to install the dependencies. Note the use of <code>\"</code>.</p> <pre><code>pip install \".[docs]\"\n</code></pre> <p>Now, to run the documentation locally with hot-reloading for changes made.</p> <pre><code>mkdocs serve\n</code></pre> <p>You'll then be prompted to connect to <code>http://127.0.0.1:8080</code> in order to access.</p>"},{"location":"contributing/LOCAL_DEVELOPMENT/#backend","title":"Backend","text":"<p>The backend is contained within the <code>./invokeai/backend</code> and <code>./invokeai/app</code> directories. To get started please install the development dependencies.</p> <p>From the root of the repository run the following command. Note the use of <code>\"</code>.</p> <pre><code>pip install \".[dev,test]\"\n</code></pre> <p>These are optional groups of packages which are defined within the <code>pyproject.toml</code> and will be required for testing the changes you make to the code.</p>"},{"location":"contributing/LOCAL_DEVELOPMENT/#tests","title":"Tests","text":"<p>See the tests documentation for information about running and writing tests.</p>"},{"location":"contributing/LOCAL_DEVELOPMENT/#reloading-changes","title":"Reloading Changes","text":"<p>Experimenting with changes to the Python source code is a drag if you have to re-start the server \u2014 and re-load those multi-gigabyte models \u2014 after every change.</p> <p>For a faster development workflow, add the <code>--dev_reload</code> flag when starting the server. The server will watch for changes to all the Python files in the <code>invokeai</code> directory and apply those changes to the running server on the fly.</p> <p>This will allow you to avoid restarting the server (and reloading models) in most cases, but there are some caveats; see the jurigged documentation for details.</p>"},{"location":"contributing/LOCAL_DEVELOPMENT/#front-end","title":"Front End","text":""},{"location":"contributing/LOCAL_DEVELOPMENT/#invoke-ui","title":"Invoke UI","text":"<p>https://invoke-ai.github.io/InvokeAI/contributing/frontend/OVERVIEW/</p>"},{"location":"contributing/LOCAL_DEVELOPMENT/#developing-invokeai-in-vscode","title":"Developing InvokeAI in VSCode","text":"<p>VSCode offers some nice tools:</p> <ul> <li>python debugger</li> <li>automatic <code>venv</code> activation</li> <li>remote dev (e.g. run InvokeAI on a beefy linux desktop while you type in   comfort on your macbook)</li> </ul>"},{"location":"contributing/LOCAL_DEVELOPMENT/#setup","title":"Setup","text":"<p>You'll need the Python and Pylance extensions installed first.</p> <p>It's also really handy to install the <code>Jupyter</code> extensions:</p> <ul> <li>Jupyter</li> <li>Jupyter Cell Tags</li> <li>Jupyter Notebook Renderers</li> <li>Jupyter Slide Show</li> </ul>"},{"location":"contributing/LOCAL_DEVELOPMENT/#invokeai-workspace","title":"InvokeAI workspace","text":"<p>Creating a VSCode workspace for working on InvokeAI is highly recommended. It can hold InvokeAI-specific settings and configs.</p> <p>To make a workspace:</p> <ul> <li>Open the InvokeAI repo dir in VSCode</li> <li><code>File</code> &gt; <code>Save Workspace As</code> &gt; save it outside the repo</li> </ul>"},{"location":"contributing/LOCAL_DEVELOPMENT/#default-python-interpreter-ie-automatic-virtual-environment-activation","title":"Default python interpreter (i.e. automatic virtual environment activation)","text":"<ul> <li>Use command palette to run command   <code>Preferences: Open Workspace Settings (JSON)</code></li> <li>Add <code>python.defaultInterpreterPath</code> to <code>settings</code>, pointing to your <code>venv</code>'s   python</li> </ul> <p>Should look something like this:</p> <pre><code>{\n  // I like to have all InvokeAI-related folders in my workspace\n  \"folders\": [\n    {\n      // repo root\n      \"path\": \"InvokeAI\"\n    },\n    {\n      // InvokeAI root dir, where `invokeai.yaml` lives\n      \"path\": \"/path/to/invokeai_root\"\n    }\n  ],\n  \"settings\": {\n    // Where your InvokeAI `venv`'s python executable lives\n    \"python.defaultInterpreterPath\": \"/path/to/invokeai_root/.venv/bin/python\"\n  }\n}\n</code></pre> <p>Now when you open the VSCode integrated terminal, or do anything that needs to run python, it will automatically be in your InvokeAI virtual environment.</p> <p>Bonus: When you create a Jupyter notebook, when you run it, you'll be prompted for the python interpreter to run in. This will default to your <code>venv</code> python, and so you'll have access to the same python environment as the InvokeAI app.</p> <p>This is super handy.</p>"},{"location":"contributing/LOCAL_DEVELOPMENT/#enabling-type-checking-with-pylance","title":"Enabling Type-Checking with Pylance","text":"<p>We use python's typing system in InvokeAI. PR reviews will include checking that types are present and correct. We don't enforce types with <code>mypy</code> at this time, but that is on the horizon.</p> <p>Using a code analysis tool to automatically type check your code (and types) is very important when writing with types. These tools provide immediate feedback in your editor when types are incorrect, and following their suggestions lead to fewer runtime bugs.</p> <p>Pylance, installed at the beginning of this guide, is the de-facto python LSP (language server protocol). It provides type checking in the editor (among many other features). Once installed, you do need to enable type checking manually:</p> <ul> <li>Open a python file</li> <li>Look along the status bar in VSCode for <code>{ } Python</code></li> <li>Click the <code>{ }</code></li> <li>Turn type checking on - basic is fine</li> </ul> <p>You'll now see red squiggly lines where type issues are detected. Hover your cursor over the indicated symbols to see what's wrong.</p> <p>In 99% of cases when the type checker says there is a problem, there really is a problem, and you should take some time to understand and resolve what it is pointing out.</p>"},{"location":"contributing/LOCAL_DEVELOPMENT/#debugging-configs-with-launchjson","title":"Debugging configs with <code>launch.json</code>","text":"<p>Debugging configs are managed in a <code>launch.json</code> file. Like most VSCode configs, these can be scoped to a workspace or folder.</p> <p>Follow the official guide to set up your <code>launch.json</code> and try it out.</p> <p>Now we can create the InvokeAI debugging configs:</p> <pre><code>{\n  // Use IntelliSense to learn about possible attributes.\n  // Hover to view descriptions of existing attributes.\n  // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387\n  \"version\": \"0.2.0\",\n  \"configurations\": [\n    {\n      // Run the InvokeAI backend &amp; serve the pre-built UI\n      \"name\": \"InvokeAI Web\",\n      \"type\": \"python\",\n      \"request\": \"launch\",\n      \"program\": \"scripts/invokeai-web.py\",\n      \"args\": [\n        // Your InvokeAI root dir (where `invokeai.yaml` lives)\n        \"--root\",\n        \"/path/to/invokeai_root\",\n        // Access the app from anywhere on your local network\n        \"--host\",\n        \"0.0.0.0\"\n      ],\n      \"justMyCode\": true\n    },\n    {\n      // Run the nodes-based CLI\n      \"name\": \"InvokeAI CLI\",\n      \"type\": \"python\",\n      \"request\": \"launch\",\n      \"program\": \"scripts/invokeai-cli.py\",\n      \"justMyCode\": true\n    },\n    {\n      // Run tests\n      \"name\": \"InvokeAI Test\",\n      \"type\": \"python\",\n      \"request\": \"launch\",\n      \"module\": \"pytest\",\n      \"args\": [\"--capture=no\"],\n      \"justMyCode\": true\n    },\n    {\n      // Run a single test\n      \"name\": \"InvokeAI Single Test\",\n      \"type\": \"python\",\n      \"request\": \"launch\",\n      \"module\": \"pytest\",\n      \"args\": [\n        // Change this to point to the specific test you are working on\n        \"tests/nodes/test_invoker.py\"\n      ],\n      \"justMyCode\": true\n    },\n    {\n      // This is the default, useful to just run a single file\n      \"name\": \"Python: File\",\n      \"type\": \"python\",\n      \"request\": \"launch\",\n      \"program\": \"${file}\",\n      \"justMyCode\": true\n    }\n  ]\n}\n</code></pre> <p>You'll see these configs in the debugging configs drop down. Running them will start InvokeAI with attached debugger, in the correct environment, and work just like the normal app.</p> <p>Enjoy debugging InvokeAI with ease (not that we have any bugs of course).</p>"},{"location":"contributing/LOCAL_DEVELOPMENT/#remote-dev","title":"Remote dev","text":"<p>This is very easy to set up and provides the same very smooth experience as local development. Environments and debugging, as set up above, just work, though you'd need to recreate the workspace and debugging configs on the remote.</p> <p>Consult the official guide to get it set up.</p> <p>Suggest using VSCode's included settings sync so that your remote dev host has all the same app settings and extensions automagically.</p>"},{"location":"contributing/LOCAL_DEVELOPMENT/#one-remote-dev-gotcha","title":"One remote dev gotcha","text":"<p>I've found the automatic port forwarding to be very flakey. You can disable it in <code>Preferences: Open Remote Settings (ssh: hostname)</code>. Search for <code>remote.autoForwardPorts</code> and untick the box.</p> <p>To forward ports very reliably, use SSH on the remote dev client (e.g. your macbook). Here's how to forward both backend API port (<code>9090</code>) and the frontend live dev server port (<code>5173</code>):</p> <pre><code>ssh \\\n    -L 9090:localhost:9090 \\\n    -L 5173:localhost:5173 \\\n    user@remote-dev-host\n</code></pre> <p>The forwarding stops when you close the terminal window, so suggest to do this outside the VSCode integrated terminal in case you need to restart VSCode for an extension update or something</p> <p>Now, on your remote dev client, you can open <code>localhost:9090</code> and access the UI, now served from the remote dev host, just the same as if it was running on the client.</p>"},{"location":"contributing/MODEL_MANAGER/","title":"Introduction to the Model Manager V2","text":"<p>The Model Manager is responsible for organizing the various machine learning models used by InvokeAI. It consists of a series of interdependent services that together handle the full lifecycle of a model. These are the:</p> <ul> <li> <p>ModelRecordServiceBase Responsible for managing model metadata and   configuration information. Among other things, the record service   tracks the type of the model, its provenance, and where it can be   found on disk.</p> </li> <li> <p>ModelInstallServiceBase A service for installing models to   disk. It uses <code>DownloadQueueServiceBase</code> to download models and   their metadata, and <code>ModelRecordServiceBase</code> to store that   information. It is also responsible for managing the InvokeAI   <code>models</code> directory and its contents.</p> </li> <li> <p>DownloadQueueServiceBase   A multithreaded downloader responsible   for downloading models from a remote source to disk. The download   queue has special methods for downloading repo_id folders from   Hugging Face, as well as discriminating among model versions in   Civitai, but can be used for arbitrary content.</p> </li> <li> <p>ModelLoadServiceBase   Responsible for loading a model from disk   into RAM and VRAM and getting it ready for inference.</p> </li> </ul>"},{"location":"contributing/MODEL_MANAGER/#location-of-the-code","title":"Location of the Code","text":"<p>The four main services can be found in <code>invokeai/app/services</code> in the following directories:</p> <ul> <li><code>invokeai/app/services/model_records/</code></li> <li><code>invokeai/app/services/model_install/</code></li> <li><code>invokeai/app/services/downloads/</code></li> <li><code>invokeai/app/services/model_load/</code></li> </ul> <p>Code related to the FastAPI web API can be found in <code>invokeai/app/api/routers/model_manager_v2.py</code>.</p>"},{"location":"contributing/MODEL_MANAGER/#whats-in-a-model-the-modelrecordservice","title":"What's in a Model? The ModelRecordService","text":"<p>The <code>ModelRecordService</code> manages the model's metadata. It supports a hierarchy of pydantic metadata \"config\" objects, which become increasingly specialized to support particular model types.</p>"},{"location":"contributing/MODEL_MANAGER/#modelconfigbase","title":"ModelConfigBase","text":"<p>All model metadata classes inherit from this pydantic class. it provides the following fields:</p> Field Name Type Description <code>key</code> str Unique identifier for the model <code>name</code> str Name of the model (not unique) <code>model_type</code> ModelType The type of the model <code>model_format</code> ModelFormat The format of the model (e.g. \"diffusers\"); also used as a Union discriminator <code>base_model</code> BaseModelType The base model that the model is compatible with <code>path</code> str Location of model on disk <code>hash</code> str Hash of the model <code>description</code> str Human-readable description of the model (optional) <code>source</code> str Model's source URL or repo id (optional) <p>The <code>key</code> is a unique 32-character random ID which was generated at install time. The <code>hash</code> field stores a hash of the model's contents at install time obtained by sampling several parts of the model's files using the <code>imohash</code> library. Over the course of the model's lifetime it may be transformed in various ways, such as changing its precision or converting it from a .safetensors to a diffusers model.</p> <p><code>ModelType</code>, <code>ModelFormat</code> and <code>BaseModelType</code> are string enums that are defined in <code>invokeai.backend.model_manager.config</code>. They are also imported by, and can be reexported from, <code>invokeai.app.services.model_manager.model_records</code>:</p> <pre><code>from invokeai.app.services.model_records import ModelType, ModelFormat, BaseModelType\n</code></pre> <p>The <code>path</code> field can be absolute or relative. If relative, it is taken to be relative to the <code>models_dir</code> setting in the user's <code>invokeai.yaml</code> file.</p>"},{"location":"contributing/MODEL_MANAGER/#checkpointconfig","title":"CheckpointConfig","text":"<p>This adds support for checkpoint configurations, and adds the following field:</p> Field Name Type Description <code>config</code> str Path to the checkpoint's config file <p><code>config</code> is the path to the checkpoint's config file. If relative, it is taken to be relative to the InvokeAI root directory (e.g. <code>configs/stable-diffusion/v1-inference.yaml</code>)</p>"},{"location":"contributing/MODEL_MANAGER/#mainconfig","title":"MainConfig","text":"<p>This adds support for \"main\" Stable Diffusion models, and adds these fields:</p> Field Name Type Description <code>vae</code> str Path to a VAE to use instead of the burnt-in one <code>variant</code> ModelVariantType Model variant type, such as \"inpainting\" <p><code>vae</code> can be an absolute or relative path. If relative, its base is taken to be the <code>models_dir</code> directory.</p> <p><code>variant</code> is an enumerated string class with values <code>normal</code>, <code>inpaint</code> and <code>depth</code>. If needed, it can be imported if needed from either <code>invokeai.app.services.model_records</code> or <code>invokeai.backend.model_manager.config</code>.</p>"},{"location":"contributing/MODEL_MANAGER/#onnxsd2config","title":"ONNXSD2Config","text":"Field Name Type Description <code>prediction_type</code> SchedulerPredictionType Scheduler prediction type to use, e.g. \"epsilon\" <code>upcast_attention</code> bool Model requires its attention module to be upcast <p>The <code>SchedulerPredictionType</code> enum can be imported from either <code>invokeai.app.services.model_records</code> or <code>invokeai.backend.model_manager.config</code>.</p>"},{"location":"contributing/MODEL_MANAGER/#other-config-classes","title":"Other config classes","text":"<p>There are a series of such classes each discriminated by their <code>ModelFormat</code>, including <code>LoRAConfig</code>, <code>IPAdapterConfig</code>, and so forth. These are rarely needed outside the model manager's internal code, but available in <code>invokeai.backend.model_manager.config</code> if needed. There is also a Union of all ModelConfig classes, called <code>AnyModelConfig</code> that can be imported from the same file.</p>"},{"location":"contributing/MODEL_MANAGER/#limitations-of-the-data-model","title":"Limitations of the Data Model","text":"<p>The config hierarchy has a major limitation in its handling of the base model type. Each model can only be compatible with one base model, which breaks down in the event of models that are compatible with two or more base models. For example, SD-1 VAEs also work with SD-2 models. A partial workaround is to use <code>BaseModelType.Any</code>, which indicates that the model is compatible with any of the base models. This works OK for some models, such as the IP Adapter image encoders, but is an all-or-nothing proposition.</p>"},{"location":"contributing/MODEL_MANAGER/#reading-and-writing-model-configuration-records","title":"Reading and Writing Model Configuration Records","text":"<p>The <code>ModelRecordService</code> provides the ability to retrieve model configuration records from SQL or YAML databases, update them, and write them back.</p> <p>A application-wide <code>ModelRecordService</code> is created during API initialization and can be retrieved within an invocation from the <code>InvocationContext</code> object:</p> <pre><code>store = context.services.model_manager.store\n</code></pre> <p>or from elsewhere in the code by accessing <code>ApiDependencies.invoker.services.model_manager.store</code>.</p>"},{"location":"contributing/MODEL_MANAGER/#creating-a-modelrecordservice","title":"Creating a <code>ModelRecordService</code>","text":"<p>To create a new <code>ModelRecordService</code> database or open an existing one, you can directly create either a <code>ModelRecordServiceSQL</code> or a <code>ModelRecordServiceFile</code> object:</p> <pre><code>from invokeai.app.services.model_records import ModelRecordServiceSQL, ModelRecordServiceFile\n\nstore = ModelRecordServiceSQL.from_connection(connection, lock)\nstore = ModelRecordServiceSQL.from_db_file('/path/to/sqlite_database.db')\nstore = ModelRecordServiceFile.from_db_file('/path/to/database.yaml')\n</code></pre> <p>The <code>from_connection()</code> form is only available from the <code>ModelRecordServiceSQL</code> class, and is used to manage records in a previously-opened SQLITE3 database using a <code>sqlite3.connection</code> object and a <code>threading.lock</code> object. It is intended for the specific use case of storing the record information in the main InvokeAI database, usually <code>databases/invokeai.db</code>.</p> <p>The <code>from_db_file()</code> methods can be used to open new connections to the named database files. If the file doesn't exist, it will be created and initialized.</p> <p>As a convenience, <code>ModelRecordServiceBase</code> offers two methods, <code>from_db_file</code> and <code>open</code>, which will return either a SQL or File implementation depending on the context. The former looks at the file extension to determine whether to open the file as a SQL database (\".db\") or as a file database (\".yaml\"). If the file exists, but is either the wrong type or does not contain the expected schema metainformation, then an appropriate <code>AssertionError</code> will be raised:</p> <pre><code>store = ModelRecordServiceBase.from_db_file('/path/to/a/file.{yaml,db}')\n</code></pre> <p>The <code>ModelRecordServiceBase.open()</code> method is specifically designed for use in the InvokeAI web server. Its signature is:</p> <pre><code>def open(\n       cls, \n    config: InvokeAIAppConfig, \n    conn: Optional[sqlite3.Connection] = None, \n    lock: Optional[threading.Lock] = None\n    ) -&gt; Union[ModelRecordServiceSQL, ModelRecordServiceFile]:\n</code></pre> <p>The way it works is as follows:</p> <ol> <li>Retrieve the value of the <code>model_config_db</code> option from the user's  <code>invokeai.yaml</code> config file.</li> <li>If <code>model_config_db</code> is <code>auto</code> (the default), then:</li> <li>Use the values of <code>conn</code> and <code>lock</code> to return a <code>ModelRecordServiceSQL</code> object   opened on the passed connection and lock.</li> <li>Open up a new connection to <code>databases/invokeai.db</code> if <code>conn</code>      and/or <code>lock</code> are missing (see note below).</li> <li>If <code>model_config_db</code> is a Path, then use <code>from_db_file</code>    to return the appropriate type of ModelRecordService.</li> <li>If <code>model_config_db</code> is None, then retrieve the legacy    <code>conf_path</code> option from <code>invokeai.yaml</code> and use the Path    indicated there. This will default to <code>configs/models.yaml</code>.</li> </ol> <p>So a typical startup pattern would be:</p> <pre><code>import sqlite3\nfrom invokeai.app.services.thread import lock\nfrom invokeai.app.services.model_records import ModelRecordServiceBase\nfrom invokeai.app.services.config import InvokeAIAppConfig\n\nconfig = InvokeAIAppConfig.get_config()\ndb_conn = sqlite3.connect(config.db_path.as_posix(), check_same_thread=False)\nstore = ModelRecordServiceBase.open(config, db_conn, lock)\n</code></pre>"},{"location":"contributing/MODEL_MANAGER/#fetching-a-models-configuration-from-modelrecordservicebase","title":"Fetching a Model's Configuration from <code>ModelRecordServiceBase</code>","text":"<p>Configurations can be retrieved in several ways.</p>"},{"location":"contributing/MODEL_MANAGER/#get_modelkey-anymodelconfig","title":"get_model(key) -&gt; AnyModelConfig","text":"<p>The basic functionality is to call the record store object's <code>get_model()</code> method with the desired model's unique key. It returns the appropriate subclass of ModelConfigBase:</p> <pre><code>model_conf = store.get_model('f13dd932c0c35c22dcb8d6cda4203764')\nprint(model_conf.path)\n\n&gt;&gt; '/tmp/models/ckpts/v1-5-pruned-emaonly.safetensors'\n</code></pre> <p>If the key is unrecognized, this call raises an <code>UnknownModelException</code>.</p>"},{"location":"contributing/MODEL_MANAGER/#existskey-anymodelconfig","title":"exists(key) -&gt; AnyModelConfig","text":"<p>Returns True if a model with the given key exists in the databsae.</p>"},{"location":"contributing/MODEL_MANAGER/#search_by_pathpath-anymodelconfig","title":"search_by_path(path) -&gt; AnyModelConfig","text":"<p>Returns the configuration of the model whose path is <code>path</code>. The path is matched using a simple string comparison and won't correctly match models referred to by different paths (e.g. using symbolic links).</p>"},{"location":"contributing/MODEL_MANAGER/#search_by_namename-base-type-listanymodelconfig","title":"search_by_name(name, base, type) -&gt; List[AnyModelConfig]","text":"<p>This method searches for models that match some combination of <code>name</code>, <code>BaseType</code> and <code>ModelType</code>. Calling without any arguments will return all the models in the database.</p>"},{"location":"contributing/MODEL_MANAGER/#all_models-listanymodelconfig","title":"all_models() -&gt; List[AnyModelConfig]","text":"<p>Return all the model configs in the database. Exactly equivalent to calling <code>search_by_name()</code> with no arguments.</p>"},{"location":"contributing/MODEL_MANAGER/#search_by_tagtags-listanymodelconfig","title":"search_by_tag(tags) -&gt; List[AnyModelConfig]","text":"<p><code>tags</code> is a list of strings. This method returns a list of model configs that contain all of the given tags. Examples:</p> <pre><code># find all models that are marked as both SFW and as generating\n# background scenery\nconfigs = store.search_by_tag(['sfw', 'scenery'])\n</code></pre> <p>Note that only tags are not searchable in this way. Other fields can be searched using a filter:</p> <pre><code>commercializable_models = [x for x in store.all_models() \\\n                           if x.license.contains('allowCommercialUse=Sell')]\n</code></pre>"},{"location":"contributing/MODEL_MANAGER/#version-str","title":"version() -&gt; str","text":"<p>Returns the version of the database, currently at <code>3.2</code></p>"},{"location":"contributing/MODEL_MANAGER/#model_info_by_namename-base_model-model_type-modelconfigbase","title":"model_info_by_name(name, base_model, model_type) -&gt; ModelConfigBase","text":"<p>This method exists to ease the transition from the previous version of the model manager, in which <code>get_model()</code> took the three arguments shown above. This looks for a unique model identified by name, base model and model type and returns it.</p> <p>The method will generate a <code>DuplicateModelException</code> if there are more than one models that share the same type, base and name. While unlikely, it is certainly possible to have a situation in which the user had added two models with the same name, base and type, one located at path <code>/foo/my_model</code> and the other at <code>/bar/my_model</code>. It is strongly recommended to search for models using <code>search_by_name()</code>, which can return multiple results, and then to select the desired model and pass its key to <code>get_model()</code>.</p>"},{"location":"contributing/MODEL_MANAGER/#writing-model-configs-to-the-database","title":"Writing model configs to the database","text":"<p>Several methods allow you to create and update stored model config records.</p>"},{"location":"contributing/MODEL_MANAGER/#add_modelkey-config-anymodelconfig","title":"add_model(key, config) -&gt; AnyModelConfig","text":"<p>Given a key and a configuration, this will add the model's configuration record to the database. <code>config</code> can either be a subclass of <code>ModelConfigBase</code> (i.e. any class listed in <code>AnyModelConfig</code>), or a <code>dict</code> of key/value pairs. In the latter case, the correct configuration class will be picked by Pydantic's discriminated union mechanism.</p> <p>If successful, the method will return the appropriate subclass of <code>ModelConfigBase</code>. It will raise a <code>DuplicateModelException</code> if a model with the same key is already in the database, or an <code>InvalidModelConfigException</code> if a dict was passed and Pydantic experienced a parse or validation error.</p>"},{"location":"contributing/MODEL_MANAGER/#update_modelkey-config-anymodelconfig","title":"update_model(key, config) -&gt; AnyModelConfig","text":"<p>Given a key and a configuration, this will update the model configuration record in the database. <code>config</code> can be either a instance of <code>ModelConfigBase</code>, or a sparse <code>dict</code> containing the fields to be updated. This will return an <code>AnyModelConfig</code> on success, or raise <code>InvalidModelConfigException</code> or <code>UnknownModelException</code> exceptions on failure.</p>"},{"location":"contributing/MODEL_MANAGER/#model-installation","title":"Model installation","text":"<p>The <code>ModelInstallService</code> class implements the <code>ModelInstallServiceBase</code> abstract base class, and provides a one-stop shop for all your model install needs. It provides the following functionality:</p> <ul> <li> <p>Registering a model config record for a model already located on the   local filesystem, without moving it or changing its path.</p> </li> <li> <p>Installing a model alreadiy located on the local filesystem, by   moving it into the InvokeAI root directory under the   <code>models</code> folder (or wherever config parameter <code>models_dir</code>   specifies).</p> </li> <li> <p>Probing of models to determine their type, base type and other key   information.</p> </li> <li> <p>Interface with the InvokeAI event bus to provide status updates on   the download, installation and registration process.</p> </li> <li> <p>Downloading a model from an arbitrary URL and installing it in   <code>models_dir</code>.</p> </li> <li> <p>Special handling for HuggingFace repo_ids to recursively download   the contents of the repository, paying attention to alternative   variants such as fp16.</p> </li> <li> <p>Saving tags and other metadata about the model into the invokeai database   when fetching from a repo that provides that type of information,   (currently only HuggingFace).</p> </li> </ul>"},{"location":"contributing/MODEL_MANAGER/#initializing-the-installer","title":"Initializing the installer","text":"<p>A default installer is created at InvokeAI api startup time and stored in <code>ApiDependencies.invoker.services.model_install</code> and can also be retrieved from an invocation's <code>context</code> argument with <code>context.services.model_install</code>.</p> <p>In the event you wish to create a new installer, you may use the following initialization pattern:</p> <pre><code>from invokeai.app.services.config import get_config\nfrom invokeai.app.services.model_records import ModelRecordServiceSQL\nfrom invokeai.app.services.model_install import ModelInstallService\nfrom invokeai.app.services.download import DownloadQueueService\nfrom invokeai.app.services.shared.sqlite.sqlite_database import SqliteDatabase\nfrom invokeai.backend.util.logging import InvokeAILogger\n\nconfig = get_config()\n\nlogger = InvokeAILogger.get_logger(config=config)\ndb = SqliteDatabase(config.db_path, logger)\nrecord_store = ModelRecordServiceSQL(db, logger)\nqueue = DownloadQueueService()\nqueue.start()\n\ninstaller = ModelInstallService(app_config=config,\n                                record_store=record_store,\n                                download_queue=queue\n                                )\ninstaller.start()\n</code></pre> <p>The full form of <code>ModelInstallService()</code> takes the following required parameters:</p> Argument Type Description <code>app_config</code> InvokeAIAppConfig InvokeAI app configuration object <code>record_store</code> ModelRecordServiceBase Config record storage database <code>download_queue</code> DownloadQueueServiceBase Download queue object <code>session</code> Optional[requests.Session] Swap in a different Session object (usually for debugging) <p>Once initialized, the installer will provide the following methods:</p>"},{"location":"contributing/MODEL_MANAGER/#install_job-installerheuristic_importsource-config-access_token","title":"install_job = installer.heuristic_import(source, [config], [access_token])","text":"<p>This is a simplified interface to the installer which takes a source string, an optional model configuration dictionary and an optional access token.</p> <p>The <code>source</code> is a string that can be any of these forms</p> <ol> <li>A path on the local filesystem (<code>C:\\\\users\\\\fred\\\\model.safetensors</code>)</li> <li>A Url pointing to a single downloadable model file (<code>https://civitai.com/models/58390/detail-tweaker-lora-lora</code>)</li> <li>A HuggingFace repo_id with any of the following formats:</li> <li><code>model/name</code> -- entire model</li> <li><code>model/name:fp32</code> -- entire model, using the fp32 variant</li> <li><code>model/name:fp16:vae</code> -- vae submodel, using the fp16 variant</li> <li><code>model/name::vae</code> -- vae submodel, using default precision</li> <li><code>model/name:fp16:path/to/model.safetensors</code> -- an individual model file, fp16 variant</li> <li><code>model/name::path/to/model.safetensors</code> -- an individual model file, default variant</li> </ol> <p>Note that by specifying a relative path to the top of the HuggingFace repo, you can download and install arbitrary models files.</p> <p>The variant, if not provided, will be automatically filled in with <code>fp32</code> if the user has requested full precision, and <code>fp16</code> otherwise. If a variant that does not exist is requested, then the method will install whatever HuggingFace returns as its default revision.</p> <p><code>config</code> is an optional dict of values that will override the autoprobed values for model type, base, scheduler prediction type, and so forth. See Model configuration and probing for details.</p> <p><code>access_token</code> is an optional access token for accessing resources that need authentication.</p> <p>The method will return a <code>ModelInstallJob</code>. This object is discussed at length in the following section.</p>"},{"location":"contributing/MODEL_MANAGER/#install_job-installerimport_model","title":"install_job = installer.import_model()","text":"<p>The <code>import_model()</code> method is the core of the installer. The following illustrates basic usage:</p> <pre><code>from invokeai.app.services.model_install import (\n LocalModelSource,\n HFModelSource,\n URLModelSource,\n)\n\nsource1 = LocalModelSource(path='/opt/models/sushi.safetensors')   # a local safetensors file\nsource2 = LocalModelSource(path='/opt/models/sushi_diffusers')     # a local diffusers folder\n\nsource3 = HFModelSource(repo_id='runwayml/stable-diffusion-v1-5')  # a repo_id\nsource4 = HFModelSource(repo_id='runwayml/stable-diffusion-v1-5', subfolder='vae')  # a subfolder within a repo_id\nsource5 = HFModelSource(repo_id='runwayml/stable-diffusion-v1-5', variant='fp16')   # a named variant of a HF model\nsource6 = HFModelSource(repo_id='runwayml/stable-diffusion-v1-5', subfolder='OrangeMix/OrangeMix1.ckpt')   # path to an individual model file\n\nsource7 = URLModelSource(url='https://civitai.com/api/download/models/63006')       # model located at a URL\nsource8 = URLModelSource(url='https://civitai.com/api/download/models/63006', access_token='letmein') # with an access token\n\nfor source in [source1, source2, source3, source4, source5, source6, source7]:\n   install_job = installer.install_model(source)\n\nsource2job = installer.wait_for_installs(timeout=120)\nfor source in sources:\n    job = source2job[source]\n if job.complete:\n  model_config = job.config_out\n  model_key = model_config.key\n  print(f\"{source} installed as {model_key}\")\n elif job.errored:\n     print(f\"{source}: {job.error_type}.\\nStack trace:\\n{job.error}\")\n</code></pre> <p>As shown here, the <code>import_model()</code> method accepts a variety of sources, including local safetensors files, local diffusers folders, HuggingFace repo_ids with and without a subfolder designation, Civitai model URLs and arbitrary URLs that point to checkpoint files (but not to folders).</p> <p>Each call to <code>import_model()</code> return a <code>ModelInstallJob</code> job, an object which tracks the progress of the install.</p> <p>If a remote model is requested, the model's files are downloaded in parallel across a multiple set of threads using the download queue. During the download process, the <code>ModelInstallJob</code> is updated to provide status and progress information. After the files (if any) are downloaded, the remainder of the installation runs in a single serialized background thread. These are the model probing, file copying, and config record database update steps.</p> <p>Multiple install jobs can be queued up. You may block until all install jobs are completed (or errored) by calling the <code>wait_for_installs()</code> method as shown in the code example. <code>wait_for_installs()</code> will return a <code>dict</code> that maps the requested source to its job. This object can be interrogated to determine its status. If the job errored out, then the error type and details can be recovered from <code>job.error_type</code> and <code>job.error</code>.</p> <p>The full list of arguments to <code>import_model()</code> is as follows:</p> Argument Type Default Description <code>source</code> ModelSource None The source of the model, Path, URL or repo_id <code>config</code> Dict[str, Any] None Override all or a portion of model's probed attributes <p>The next few sections describe the various types of ModelSource that can be passed to <code>import_model()</code>.</p> <p><code>config</code> can be used to override all or a portion of the configuration attributes returned by the model prober. See the section below for details.</p>"},{"location":"contributing/MODEL_MANAGER/#localmodelsource","title":"LocalModelSource","text":"<p>This is used for a model that is located on a locally-accessible Posix filesystem, such as a local disk or networked fileshare.</p> Argument Type Default Description <code>path</code> str Path None <code>inplace</code> bool False If set, the model file(s) will be left in their location; otherwise they will be copied into the InvokeAI root's <code>models</code> directory"},{"location":"contributing/MODEL_MANAGER/#urlmodelsource","title":"URLModelSource","text":"<p>This is used for a single-file model that is accessible via a URL. The fields are:</p> Argument Type Default Description <code>url</code> AnyHttpUrl None The URL for the model file. <code>access_token</code> str None An access token needed to gain access to this file. <p>The <code>AnyHttpUrl</code> class can be imported from <code>pydantic.networks</code>.</p> <p>Ordinarily, no metadata is retrieved from these sources. However, there is special-case code in the installer that looks for HuggingFace and fetches the corresponding model metadata from the corresponding repo.</p>"},{"location":"contributing/MODEL_MANAGER/#hfmodelsource","title":"HFModelSource","text":"<p>HuggingFace has the most complicated <code>ModelSource</code> structure:</p> Argument Type Default Description <code>repo_id</code> str None The ID of the desired model. <code>variant</code> ModelRepoVariant ModelRepoVariant('fp16') The desired variant. <code>subfolder</code> Path None Look for the model in a subfolder of the repo. <code>access_token</code> str None An access token needed to gain access to a subscriber's-only model. <p>The <code>repo_id</code> is the repository ID, such as <code>stabilityai/sdxl-turbo</code>.</p> <p>The <code>variant</code> is one of the various diffusers formats that HuggingFace supports and is used to pick out from the hodgepodge of files that in a typical HuggingFace repository the particular components needed for a complete diffusers model. <code>ModelRepoVariant</code> is an enum that can be imported from <code>invokeai.backend.model_manager</code> and has the following values:</p> Name String Value ModelRepoVariant.DEFAULT \"default\" ModelRepoVariant.FP16 \"fp16\" ModelRepoVariant.FP32 \"fp32\" ModelRepoVariant.ONNX \"onnx\" ModelRepoVariant.OPENVINO \"openvino\" ModelRepoVariant.FLAX \"flax\" <p>You can also pass the string forms to <code>variant</code> directly. Note that InvokeAI may not be able to load and run all variants. At the current time, specifying <code>ModelRepoVariant.DEFAULT</code> will retrieve model files that are unqualified, e.g. <code>pytorch_model.safetensors</code> rather than <code>pytorch_model.fp16.safetensors</code>. These are usually the 32-bit safetensors forms of the model.</p> <p>If <code>subfolder</code> is specified, then the requested model resides in a subfolder of the main model repository. This is typically used to fetch and install VAEs.</p> <p>Some models require you to be registered with HuggingFace and logged in. To download these files, you must provide an <code>access_token</code>. Internally, if no access token is provided, then <code>HfFolder.get_token()</code> will be called to fill it in with the cached one.</p>"},{"location":"contributing/MODEL_MANAGER/#monitoring-the-install-job-process","title":"Monitoring the install job process","text":"<p>When you create an install job with <code>import_model()</code>, it launches the download and installation process in the background and returns a <code>ModelInstallJob</code> object for monitoring the process.</p> <p>The <code>ModelInstallJob</code> class has the following structure:</p> Attribute Type Description <code>id</code> <code>int</code> Integer ID for this job <code>status</code> <code>InstallStatus</code> An enum of [<code>waiting</code>, <code>downloading</code>, <code>running</code>, <code>completed</code>, <code>error</code> and <code>cancelled</code>] <code>config_in</code> <code>dict</code> Overriding configuration values provided by the caller <code>config_out</code> <code>AnyModelConfig</code> After successful completion, contains the configuration record written to the database <code>inplace</code> <code>boolean</code> True if the caller asked to install the model in place using its local path <code>source</code> <code>ModelSource</code> The local path, remote URL or repo_id of the model to be installed <code>local_path</code> <code>Path</code> If a remote model, holds the path of the model after it is downloaded; if a local model, same as <code>source</code> <code>error_type</code> <code>str</code> Name of the exception that led to an error status <code>error</code> <code>str</code> Traceback of the error <p>If the <code>event_bus</code> argument was provided, events will also be broadcast to the InvokeAI event bus. The events will appear on the bus as an event of type <code>EventServiceBase.model_event</code>, a timestamp and the following event names:</p>"},{"location":"contributing/MODEL_MANAGER/#model_install_downloading","title":"<code>model_install_downloading</code>","text":"<p>For remote models only, <code>model_install_downloading</code> events will be issued at regular intervals as the download progresses. The event's payload contains the following keys:</p> Key Type Description <code>source</code> str String representation of the requested source <code>local_path</code> str String representation of the path to the downloading model (usually a temporary directory) <code>bytes</code> int How many bytes downloaded so far <code>total_bytes</code> int Total size of all the files that make up the model <code>parts</code> List[Dict] Information on the progress of the individual files that make up the model <p>The parts is a list of dictionaries that give information on each of the components pieces of the download. The dictionary's keys are <code>source</code>, <code>local_path</code>, <code>bytes</code> and <code>total_bytes</code>, and correspond to the like-named keys in the main event.</p> <p>Note that downloading events will not be issued for local models, and that downloading events occur before the running event.</p>"},{"location":"contributing/MODEL_MANAGER/#model_install_running","title":"<code>model_install_running</code>","text":"<p><code>model_install_running</code> is issued when all the required downloads have completed (if applicable) and the model probing, copying and registration process has now started.</p> <p>The payload will contain the key <code>source</code>.</p>"},{"location":"contributing/MODEL_MANAGER/#model_install_completed","title":"<code>model_install_completed</code>","text":"<p><code>model_install_completed</code> is issued once at the end of a successful installation. The payload will contain the keys <code>source</code>, <code>total_bytes</code> and <code>key</code>, where <code>key</code> is the ID under which the model has been registered.</p>"},{"location":"contributing/MODEL_MANAGER/#model_install_error","title":"<code>model_install_error</code>","text":"<p><code>model_install_error</code> is emitted if the installation process fails for some reason. The payload will contain the keys <code>source</code>, <code>error_type</code> and <code>error</code>. <code>error_type</code> is a short message indicating the nature of the error, and <code>error</code> is the long traceback to help debug the problem.</p>"},{"location":"contributing/MODEL_MANAGER/#model_install_cancelled","title":"<code>model_install_cancelled</code>","text":"<p><code>model_install_cancelled</code> is issued if the model installation is cancelled, or if one or more of its files' downloads are cancelled. The payload will contain <code>source</code>.</p>"},{"location":"contributing/MODEL_MANAGER/#following-the-model-status","title":"Following the model status","text":"<p>You may poll the <code>ModelInstallJob</code> object returned by <code>import_model()</code> to ascertain the state of the install. The job status can be read from the job's <code>status</code> attribute, an <code>InstallStatus</code> enum which has the enumerated values <code>WAITING</code>, <code>DOWNLOADING</code>, <code>RUNNING</code>, <code>COMPLETED</code>, <code>ERROR</code> and <code>CANCELLED</code>.</p> <p>For convenience, install jobs also provided the following boolean properties: <code>waiting</code>, <code>downloading</code>, <code>running</code>, <code>complete</code>, <code>errored</code> and <code>cancelled</code>, as well as <code>in_terminal_state</code>. The last will return True if the job is in the complete, errored or cancelled states.</p>"},{"location":"contributing/MODEL_MANAGER/#model-configuration-and-probing","title":"Model configuration and probing","text":"<p>The install service uses the <code>invokeai.backend.model_manager.probe</code> module during import to determine the model's type, base type, and other configuration parameters. Among other things, it assigns a default name and description for the model based on probed fields.</p> <p>When downloading remote models is implemented, additional configuration information, such as list of trigger terms, will be retrieved from the HuggingFace and Civitai model repositories.</p> <p>The probed values can be overriden by providing a dictionary in the optional <code>config</code> argument passed to <code>import_model()</code>. You may provide overriding values for any of the model's configuration attributes. Here is an example of setting the <code>SchedulerPredictionType</code> and <code>name</code> for an sd-2 model:</p> <pre><code>install_job = installer.import_model(\n               source=HFModelSource(repo_id='stabilityai/stable-diffusion-2-1',variant='fp32'),\n      config=dict(\n            prediction_type=SchedulerPredictionType('v_prediction')\n      name='stable diffusion 2 base model',\n            )\n       )\n</code></pre>"},{"location":"contributing/MODEL_MANAGER/#other-installer-methods","title":"Other installer methods","text":"<p>This section describes additional methods provided by the installer class.</p>"},{"location":"contributing/MODEL_MANAGER/#jobs-installerwait_for_installstimeout","title":"jobs = installer.wait_for_installs([timeout])","text":"<p>Block until all pending installs are completed or errored and then returns a list of completed jobs. The optional <code>timeout</code> argument will return from the call if jobs aren't completed in the specified time. An argument of 0 (the default) will block indefinitely.</p>"},{"location":"contributing/MODEL_MANAGER/#jobs-installerwait_for_jobjob-timeout","title":"jobs = installer.wait_for_job(job, [timeout])","text":"<p>Like <code>wait_for_installs()</code>, but block until a specific job has completed or errored, and then return the job.  The optional <code>timeout</code> argument will return from the call if the job doesn't complete in the specified time. An argument of 0 (the default) will block indefinitely.</p>"},{"location":"contributing/MODEL_MANAGER/#jobs-installerlist_jobs","title":"jobs = installer.list_jobs()","text":"<p>Return a list of all active and complete <code>ModelInstallJobs</code>.</p>"},{"location":"contributing/MODEL_MANAGER/#jobs-installerget_job_by_sourcesource","title":"jobs = installer.get_job_by_source(source)","text":"<p>Return a list of <code>ModelInstallJob</code> corresponding to the indicated model source.</p>"},{"location":"contributing/MODEL_MANAGER/#jobs-installerget_job_by_idid","title":"jobs = installer.get_job_by_id(id)","text":"<p>Return a list of <code>ModelInstallJob</code> corresponding to the indicated model id.</p>"},{"location":"contributing/MODEL_MANAGER/#jobs-installercancel_jobjob","title":"jobs = installer.cancel_job(job)","text":"<p>Cancel the indicated job.</p>"},{"location":"contributing/MODEL_MANAGER/#installerprune_jobs","title":"installer.prune_jobs","text":"<p>Remove jobs that are in a terminal state (i.e. complete, errored or cancelled) from the job list returned by <code>list_jobs()</code> and <code>get_job()</code>.</p>"},{"location":"contributing/MODEL_MANAGER/#installerapp_config-installerrecord_store-installerevent_bus","title":"installer.app_config, installer.record_store, installer.event_bus","text":"<p>Properties that provide access to the installer's <code>InvokeAIAppConfig</code>, <code>ModelRecordServiceBase</code> and <code>EventServiceBase</code> objects.</p>"},{"location":"contributing/MODEL_MANAGER/#key-installerregister_pathmodel_path-config-key-installerinstall_pathmodel_path-config","title":"key = installer.register_path(model_path, config), key = installer.install_path(model_path, config)","text":"<p>These methods bypass the download queue and directly register or install the model at the indicated path, returning the unique ID for the installed model.</p> <p>Both methods accept a Path object corresponding to a checkpoint or diffusers folder, and an optional dict of config attributes to use to override the values derived from model probing.</p> <p>The difference between <code>register_path()</code> and <code>install_path()</code> is that the former creates a model configuration record without changing the location of the model in the filesystem. The latter makes a copy of the model inside the InvokeAI models directory before registering it.</p>"},{"location":"contributing/MODEL_MANAGER/#installerunregisterkey","title":"installer.unregister(key)","text":"<p>This will remove the model config record for the model at key, and is equivalent to <code>installer.record_store.del_model(key)</code></p>"},{"location":"contributing/MODEL_MANAGER/#installerdeletekey","title":"installer.delete(key)","text":"<p>This is similar to <code>unregister()</code> but has the additional effect of conditionally deleting the underlying model file(s) if they reside within the InvokeAI models directory</p>"},{"location":"contributing/MODEL_MANAGER/#installerunconditionally_deletekey","title":"installer.unconditionally_delete(key)","text":"<p>This method is similar to <code>unregister()</code>, but also unconditionally deletes the corresponding model weights file(s), regardless of whether they are inside or outside the InvokeAI models hierarchy.</p>"},{"location":"contributing/MODEL_MANAGER/#path-installerdownload_and_cacheremote_source-access_token-timeout","title":"path = installer.download_and_cache(remote_source, [access_token], [timeout])","text":"<p>This utility routine will download the model file located at source, cache it, and return the path to the cached file. It does not attempt to determine the model type, probe its configuration values, or register it with the models database.</p> <p>You may provide an access token if the remote source requires authorization. The call will block indefinitely until the file is completely downloaded, cancelled or raises an error of some sort. If you provide a timeout (in seconds), the call will raise a <code>TimeoutError</code> exception if the download hasn't completed in the specified period.</p> <p>You may use this mechanism to request any type of file, not just a model. The file will be stored in a subdirectory of <code>INVOKEAI_ROOT/models/.cache</code>. If the requested file is found in the cache, its path will be returned without redownloading it.</p> <p>Be aware that the models cache is cleared of infrequently-used files and directories at regular intervals when the size of the cache exceeds the value specified in Invoke's <code>convert_cache</code> configuration variable.</p>"},{"location":"contributing/MODEL_MANAGER/#installerstartinvoker","title":"installer.start(invoker)","text":"<p>The <code>start</code> method is called by the API intialization routines when the API starts up. Its effect is to call <code>sync_to_config()</code> to synchronize the model record store database with what's currently on disk.</p>"},{"location":"contributing/MODEL_MANAGER/#get-on-line-the-download-queue","title":"Get on line: The Download Queue","text":"<p>InvokeAI can download arbitrary files using a multithreaded background download queue. Internally, the download queue is used for installing models located at remote locations. The queue is implemented by the <code>DownloadQueueService</code> defined in <code>invokeai.app.services.download_manager</code>. However, most of the implementation is spread out among several files in <code>invokeai/backend/model_manager/download/*</code></p> <p>A default download queue is located in <code>ApiDependencies.invoker.services.download_queue</code>. However, you can create additional instances if you need to isolate your queue from the main one.</p>"},{"location":"contributing/MODEL_MANAGER/#a-job-for-every-task","title":"A job for every task","text":"<p>The queue operates on a series of download job objects. These objects specify the source and destination of the download, and keep track of the progress of the download. Jobs come in a variety of shapes and colors as they are progressively specialized for particular download task.</p> <p>The basic job is the <code>DownloadJobBase</code>, a pydantic object with the following fields:</p> Field Type Default Description <code>id</code> int Job ID, an integer &gt;= 0 <code>priority</code> int 10 Job priority. Lower priorities run before higher priorities <code>source</code> str Where to download from (specialized types used in subclasses) <code>destination</code> Path Where to download to <code>status</code> DownloadJobStatus Idle Job's status (see below) <code>event_handlers</code> List[DownloadEventHandler] Event handlers (see below) <code>job_started</code> float Timestamp for when the job started running <code>job_ended</code> float Timestamp for when the job completed or errored out <code>job_sequence</code> int A counter that is incremented each time a model is dequeued <code>error</code> Exception A copy of the Exception that caused an error during download <p>When you create a job, you can assign it a <code>priority</code>. If multiple jobs are queued, the job with the lowest priority runs first. (Don't blame me! The Unix developers came up with this convention.)</p> <p>Every job has a <code>source</code> and a <code>destination</code>. <code>source</code> is a string in the base class, but subclassses redefine it more specifically.</p> <p>The <code>destination</code> must be the Path to a file or directory on the local filesystem. If the Path points to a new or existing file, then the source will be stored under that filename. If the Path ponts to an existing directory, then the downloaded file will be stored inside the directory, usually using the name assigned to it at the remote site in the <code>content-disposition</code> http field.</p> <p>When the job is submitted, it is assigned a numeric <code>id</code>. The id can then be used to control the job, such as starting, stopping and cancelling its download.</p> <p>The <code>status</code> field is updated by the queue to indicate where the job is in its lifecycle. Values are defined in the string enum <code>DownloadJobStatus</code>, a symbol available from <code>invokeai.app.services.download_manager</code>. Possible values are:</p> Value String Value Description <code>IDLE</code> idle Job created, but not submitted to the queue <code>ENQUEUED</code> enqueued Job is patiently waiting on the queue <code>RUNNING</code> running Job is running! <code>PAUSED</code> paused Job was paused and can be restarted <code>COMPLETED</code> completed Job has finished its work without an error <code>ERROR</code> error Job encountered an error and will not run again <code>CANCELLED</code> cancelled Job was cancelled and will not run (again) <p><code>job_started</code>, <code>job_ended</code> and <code>job_sequence</code> indicate when the job was started (using a python timestamp), when it completed, and the order in which it was taken off the queue. These are mostly used for debugging and performance testing.</p> <p>In case of an error, the Exception that caused the error will be placed in the <code>error</code> field, and the job's status will be set to <code>DownloadJobStatus.ERROR</code>.</p> <p>After an error occurs, any partially downloaded files will be deleted from disk, unless <code>preserve_partial_downloads</code> was set to True at job creation time (or set to True any time before the error occurred). Note that since all InvokeAI model install operations involve downloading files to a temporary directory that has a limited lifetime, this flag is not used by the model installer.</p> <p>There are a series of subclasses of <code>DownloadJobBase</code> that provide support for specific types of downloads. These are:</p>"},{"location":"contributing/MODEL_MANAGER/#downloadjobpath","title":"DownloadJobPath","text":"<p>This subclass redefines <code>source</code> to be a filesystem Path. It is used to move a file or directory from the <code>source</code> to the <code>destination</code> paths in the background using a uniform event-based infrastructure.</p>"},{"location":"contributing/MODEL_MANAGER/#downloadjobremotesource","title":"DownloadJobRemoteSource","text":"<p>This subclass adds the following fields to the job:</p> Field Type Default Description <code>bytes</code> int 0 bytes downloaded so far <code>total_bytes</code> int 0 total size to download <code>access_token</code> Any None an authorization token to present to the remote source <p>The job will start out with 0/0 in its bytes/total_bytes fields. Once it starts running, <code>total_bytes</code> will be populated from information provided in the HTTP download header (if available), and the number of bytes downloaded so far will be progressively incremented.</p>"},{"location":"contributing/MODEL_MANAGER/#downloadjoburl","title":"DownloadJobURL","text":"<p>This is a subclass of <code>DownloadJobBase</code>. It redefines <code>source</code> to be a Pydantic <code>AnyHttpUrl</code> object, which enforces URL validation checking on the field.</p> <p>Note that the installer service defines an additional subclass of <code>DownloadJobRemoteSource</code> that accepts HuggingFace repo_ids in addition to URLs. This is discussed later in this document.</p>"},{"location":"contributing/MODEL_MANAGER/#event-handlers","title":"Event handlers","text":"<p>While a job is being downloaded, the queue will emit events at periodic intervals. A typical series of events during a successful download session will look like this:</p> <ul> <li>enqueued</li> <li>running</li> <li>running</li> <li>running</li> <li>completed</li> </ul> <p>There will be a single enqueued event, followed by one or more running events, and finally one <code>completed</code>, <code>error</code> or <code>cancelled</code> events.</p> <p>It is possible for a caller to pause download temporarily, in which case the events may look something like this:</p> <ul> <li>enqueued</li> <li>running</li> <li>running</li> <li>paused</li> <li>running</li> <li>completed</li> </ul> <p>The download queue logs when downloads start and end (unless <code>quiet</code> is set to True at initialization time) but doesn't log any progress events. You will probably want to be alerted to events during the download job and provide more user feedback. In order to intercept and respond to events you may install a series of one or more event handlers in the job. Whenever the job's status changes, the chain of event handlers is traversed and executed in the same thread that the download job is running in.</p> <p>Event handlers have the signature <code>Callable[[\"DownloadJobBase\"], None]</code>, i.e.</p> <pre><code>def handler(job: DownloadJobBase):\n   pass\n</code></pre> <p>A typical handler will examine <code>job.status</code> and decide if there's something to be done. This can include cancelling or erroring the job, but more typically is used to report on the job status to the user interface or to perform certain actions on successful completion of the job.</p> <p>Event handlers can be attached to a job at creation time. In addition, you can create a series of default handlers that are attached to the queue object itself. These handlers will be executed for each job after the job's own handlers (if any) have run.</p> <p>During a download, running events are issued every time roughly 1% of the file is transferred. This is to provide just enough granularity to update a tqdm progress bar smoothly.</p> <p>Handlers can be added to a job after the fact using the job's <code>add_event_handler</code> method:</p> <pre><code>job.add_event_handler(my_handler)\n</code></pre> <p>All handlers can be cleared using the job's <code>clear_event_handlers()</code> method. Note that it might be a good idea to pause the job before altering its handlers.</p>"},{"location":"contributing/MODEL_MANAGER/#creating-a-download-queue-object","title":"Creating a download queue object","text":"<p>The <code>DownloadQueueService</code> constructor takes the following arguments:</p> Argument Type Default Description <code>event_handlers</code> List[DownloadEventHandler] [] Event handlers <code>max_parallel_dl</code> int 5 Maximum number of simultaneous downloads allowed <code>requests_session</code> requests.sessions.Session None An alternative requests Session object to use for the download <code>quiet</code> bool False Do work quietly without issuing log messages <p>A typical initialization sequence will look like:</p> <pre><code>from invokeai.app.services.download_manager import DownloadQueueService\n\ndef log_download_event(job: DownloadJobBase):\n logger.info(f'job={job.id}: status={job.status}')\n\nqueue = DownloadQueueService(\n       event_handlers=[log_download_event]\n        )\n</code></pre> <p>Event handlers can be provided to the queue at initialization time as shown in the example. These will be automatically appended to the handler list for any job that is submitted to this queue.</p> <p><code>max_parallel_dl</code> sets the number of simultaneous active downloads that are allowed. The default of five has not been benchmarked in any way, but seems to give acceptable performance.</p> <p><code>requests_session</code> can be used to provide a <code>requests</code> module Session object that will be used to stream remote URLs to disk. This facility was added for use in the module's unit tests to simulate a remote web server, but may be useful in other contexts.</p> <p><code>quiet</code> will prevent the queue from issuing any log messages at the INFO or higher levels.</p>"},{"location":"contributing/MODEL_MANAGER/#submitting-a-download-job","title":"Submitting a download job","text":"<p>You can submit a download job to the queue either by creating the job manually and passing it to the queue's <code>submit_download_job()</code> method, or using the <code>create_download_job()</code> method, which will do the same thing on your behalf.</p> <p>To use the former method, follow this example:</p> <pre><code>job = DownloadJobRemoteSource(\n         source='http://www.civitai.com/models/13456',\n   destination='/tmp/models/',\n   event_handlers=[my_handler1, my_handler2], # if desired\n   )\nqueue.submit_download_job(job, start=True)\n</code></pre> <p><code>submit_download_job()</code> takes just two arguments: the job to submit, and a flag indicating whether to immediately start the job (defaulting to True). If you choose not to start the job immediately, you can start it later by calling the queue's <code>start_job()</code> or <code>start_all_jobs()</code> methods, which are described later.</p> <p>To have the queue create the job for you, follow this example instead:</p> <pre><code>job = queue.create_download_job(\n         source='http://www.civitai.com/models/13456',\n   destdir='/tmp/models/',\n   filename='my_model.safetensors',\n   event_handlers=[my_handler1, my_handler2], # if desired\n   start=True,\n )\n</code></pre> <p>The <code>filename</code> argument forces the downloader to use the specified name for the file rather than the name provided by the remote source, and is equivalent to manually specifying a destination of `/tmp/models/my_model.safetensors' in the submitted job.</p> <p>Here is the full list of arguments that can be provided to <code>create_download_job()</code>:</p> Argument Type Default Description <code>source</code> Union[str, Path, AnyHttpUrl] Download remote or local source <code>destdir</code> Path Destination directory for downloaded file <code>filename</code> Path None Filename for downloaded file <code>start</code> bool True Enqueue the job immediately <code>priority</code> int 10 Starting priority for this job <code>access_token</code> str None Authorization token for this resource <code>event_handlers</code> List[DownloadEventHandler] [] Event handlers for this job <p>Internally, <code>create_download_job()</code> has a little bit of internal logic that looks at the type of the source and selects the right subclass of <code>DownloadJobBase</code> to create and enqueue.</p> <p>TODO: move this logic into its own method for overriding in subclasses.</p>"},{"location":"contributing/MODEL_MANAGER/#job-control","title":"Job control","text":"<p>Prior to completion, jobs can be controlled with a series of queue method calls. Do not attempt to modify jobs by directly writing to their fields, as this is likely to lead to unexpected results.</p> <p>Any method that accepts a job argument may raise an <code>UnknownJobIDException</code> if the job has not yet been submitted to the queue or was not created by this queue.</p>"},{"location":"contributing/MODEL_MANAGER/#queuejoin","title":"queue.join()","text":"<p>This method will block until all the active jobs in the queue have reached a terminal state (completed, errored or cancelled).</p>"},{"location":"contributing/MODEL_MANAGER/#queuewait_for_jobjob-timeout","title":"queue.wait_for_job(job, [timeout])","text":"<p>This method will block until the indicated job has reached a terminal state (completed, errored or cancelled). If the optional timeout is provided, the call will block for at most timeout seconds, and raise a TimeoutError otherwise.</p>"},{"location":"contributing/MODEL_MANAGER/#jobs-queuelist_jobs","title":"jobs = queue.list_jobs()","text":"<p>This will return a list of all jobs, including ones that have not yet been enqueued and those that have completed or errored out.</p>"},{"location":"contributing/MODEL_MANAGER/#job-queueid_to_jobint","title":"job = queue.id_to_job(int)","text":"<p>This method allows you to recover a submitted job using its ID.</p>"},{"location":"contributing/MODEL_MANAGER/#queueprune_jobs","title":"queue.prune_jobs()","text":"<p>Remove completed and errored jobs from the job list.</p>"},{"location":"contributing/MODEL_MANAGER/#queuestart_jobjob","title":"queue.start_job(job)","text":"<p>If the job was submitted with <code>start=False</code>, then it can be started using this method.</p>"},{"location":"contributing/MODEL_MANAGER/#queuepause_jobjob","title":"queue.pause_job(job)","text":"<p>This will temporarily pause the job, if possible. It can later be restarted and pick up where it left off using <code>queue.start_job()</code>.</p>"},{"location":"contributing/MODEL_MANAGER/#queuecancel_jobjob","title":"queue.cancel_job(job)","text":"<p>This will cancel the job if possible and clean up temporary files and other resources that it might have been using.</p>"},{"location":"contributing/MODEL_MANAGER/#queuestart_all_jobs-queuepause_all_jobs-queuecancel_all_jobs","title":"queue.start_all_jobs(), queue.pause_all_jobs(), queue.cancel_all_jobs()","text":"<p>This will start/pause/cancel all jobs that have been submitted to the queue and have not yet reached a terminal state.</p>"},{"location":"contributing/MODEL_MANAGER/#this-meta-be-good-model-metadata-storage","title":"This Meta be Good: Model Metadata Storage","text":"<p>The modules found under <code>invokeai.backend.model_manager.metadata</code> provide a straightforward API for fetching model metadatda from online repositories. Currently only HuggingFace is supported. However, the modules are easily extended for additional repos, provided that they have defined APIs for metadata access.</p> <p>Metadata comprises any descriptive information that is not essential for getting the model to run. For example \"author\" is metadata, while \"type\", \"base\" and \"format\" are not. The latter fields are part of the model's config, as defined in <code>invokeai.backend.model_manager.config</code>.</p>"},{"location":"contributing/MODEL_MANAGER/#example-usage","title":"Example Usage","text":"<pre><code>from invokeai.backend.model_manager.metadata import (\n   AnyModelRepoMetadata,\n)\n# to access the initialized sql database\nfrom invokeai.app.api.dependencies import ApiDependencies\n\nhf = HuggingFaceMetadataFetch()\n\n# fetch the metadata\nmodel_metadata = hf.from_id(\"&lt;repo_id&gt;\")\n\nassert isinstance(model_metadata, HuggingFaceMetadata)\n</code></pre>"},{"location":"contributing/MODEL_MANAGER/#structure-of-the-metadata-objects","title":"Structure of the Metadata objects","text":"<p>There is a short class hierarchy of Metadata objects, all of which descend from the Pydantic <code>BaseModel</code>.</p>"},{"location":"contributing/MODEL_MANAGER/#modelmetadatabase","title":"<code>ModelMetadataBase</code>","text":"<p>This is the common base class for metadata:</p> Field Name Type Description <code>name</code> str Repository's name for the model <code>author</code> str Model's author <code>tags</code> Set[str] Model tags <p>Note that the model config record also has a <code>name</code> field. It is intended that the config record version be locally customizable, while the metadata version is read-only. However, enforcing this is expected to be part of the business logic.</p> <p>Descendents of the base add additional fields.</p>"},{"location":"contributing/MODEL_MANAGER/#huggingfacemetadata","title":"<code>HuggingFaceMetadata</code>","text":"<p>This descends from <code>ModelMetadataBase</code> and adds the following fields:</p> Field Name Type Description <code>type</code> Literal[\"huggingface\"] Used for the discriminated union of metadata classes <code>id</code> str HuggingFace repo_id <code>tag_dict</code> Dict[str, Any] A dictionary of tag/value pairs provided in addition to <code>tags</code> <code>last_modified</code> datetime Date of last commit of this model to the repo <code>files</code> List[Path] List of the files in the model repo"},{"location":"contributing/MODEL_MANAGER/#anymodelrepometadata","title":"<code>AnyModelRepoMetadata</code>","text":"<p>This is a discriminated Union of <code>HuggingFaceMetadata</code>.</p>"},{"location":"contributing/MODEL_MANAGER/#fetching-metadata-from-online-repos","title":"Fetching Metadata from Online Repos","text":"<p>The <code>HuggingFaceMetadataFetch</code> class will retrieve metadata from its corresponding repository and return <code>AnyModelRepoMetadata</code> objects. Their base class <code>ModelMetadataFetchBase</code> is an abstract class that defines two methods: <code>from_url()</code> and <code>from_id()</code>. The former accepts the type of model URLs that the user will try to cut and paste into the model import form. The latter accepts a string ID in the format recognized by the repository of choice. Both methods return an <code>AnyModelRepoMetadata</code>.</p> <p>The base class also has a class method <code>from_json()</code> which will take the JSON representation of a <code>ModelMetadata</code> object, validate it, and return the corresponding <code>AnyModelRepoMetadata</code> object.</p> <p>When initializing one of the metadata fetching classes, you may provide a <code>requests.Session</code> argument. This allows you to customize the low-level HTTP fetch requests and is used, for instance, in the testing suite to avoid hitting the internet.</p> <p>The HuggingFace fetcher subclass add additional repo-specific fetching methods:</p>"},{"location":"contributing/MODEL_MANAGER/#huggingfacemetadatafetch","title":"HuggingFaceMetadataFetch","text":"<p>This overrides its base class <code>from_json()</code> method to return a <code>HuggingFaceMetadata</code> object directly.</p>"},{"location":"contributing/MODEL_MANAGER/#metadata-storage","title":"Metadata Storage","text":"<p>The <code>ModelConfigBase</code> stores this response in the <code>source_api_response</code> field as a JSON blob.</p>"},{"location":"contributing/MODEL_MANAGER/#the-lowdown-on-the-modelloadservice","title":"The Lowdown on the ModelLoadService","text":"<p>The <code>ModelLoadService</code> is responsible for loading a named model into memory so that it can be used for inference. Despite the fact that it does a lot under the covers, it is very straightforward to use.</p> <p>An application-wide model loader is created at API initialization time and stored in <code>ApiDependencies.invoker.services.model_loader</code>. However, you can create alternative instances if you wish.</p>"},{"location":"contributing/MODEL_MANAGER/#creating-a-modelloadservice-object","title":"Creating a ModelLoadService object","text":"<p>The class is defined in <code>invokeai.app.services.model_load</code>. It is initialized with an InvokeAIAppConfig object, from which it gets configuration information such as the user's desired GPU and precision, and with a previously-created <code>ModelRecordServiceBase</code> object, from which it loads the requested model's configuration information.</p> <p>Here is a typical initialization pattern:</p> <pre><code>from invokeai.app.services.config import InvokeAIAppConfig\nfrom invokeai.app.services.model_load import ModelLoadService, ModelLoaderRegistry\n\nconfig = InvokeAIAppConfig.get_config()\nram_cache = ModelCache(\n max_cache_size=config.ram_cache_size, logger=logger\n)\nconvert_cache = ModelConvertCache(\n cache_path=config.models_convert_cache_path, max_size=config.convert_cache_size\n)\nloader = ModelLoadService(\n app_config=config,\n ram_cache=ram_cache,\n convert_cache=convert_cache,\n registry=ModelLoaderRegistry\n)\n</code></pre>"},{"location":"contributing/MODEL_MANAGER/#load_modelmodel_config-submodel_type-context-loadedmodel","title":"load_model(model_config, [submodel_type], [context]) -&gt; LoadedModel","text":"<p>The <code>load_model()</code> method takes an <code>AnyModelConfig</code> returned by <code>ModelRecordService.get_model()</code> and returns the corresponding loaded model.  It loads the model into memory, gets the model ready for use, and returns a <code>LoadedModel</code> object.</p> <p>The optional second argument, <code>subtype</code> is a <code>SubModelType</code> string enum, such as \"vae\". It is mandatory when used with a main model, and is used to select which part of the main model to load.</p> <p>The optional third argument, <code>context</code> can be provided by an invocation to trigger model load event reporting. See below for details.</p> <p>The returned <code>LoadedModel</code> object contains a copy of the configuration record returned by the model record <code>get_model()</code> method, as well as the in-memory loaded model:</p> Attribute Name Type Description <code>config</code> AnyModelConfig A copy of the model's configuration record for retrieving base type, etc. <code>model</code> AnyModel The instantiated model (details below) <code>locker</code> ModelLockerBase A context manager that mediates the movement of the model into VRAM"},{"location":"contributing/MODEL_MANAGER/#get_model_by_keykey-submodel-loadedmodel","title":"get_model_by_key(key, [submodel]) -&gt; LoadedModel","text":"<p>The <code>get_model_by_key()</code> method will retrieve the model using its unique database key. For example:</p> <p>loaded_model = loader.get_model_by_key('f13dd932c0c35c22dcb8d6cda4203764', SubModelType('vae'))</p> <p><code>get_model_by_key()</code> may raise any of the following exceptions:</p> <ul> <li><code>UnknownModelException</code>   -- key not in database</li> <li><code>ModelNotFoundException</code>  -- key in database but model not found at path</li> <li><code>NotImplementedException</code> -- the loader doesn't know how to load this type of model</li> </ul>"},{"location":"contributing/MODEL_MANAGER/#using-the-loaded-model-in-inference","title":"Using the Loaded Model in Inference","text":"<p><code>LoadedModel</code> acts as a context manager. The context loads the model into the execution device (e.g. VRAM on CUDA systems), locks the model in the execution device for the duration of the context, and returns the model. Use it like this:</p> <pre><code>loaded_model_= loader.get_model_by_key('f13dd932c0c35c22dcb8d6cda4203764', SubModelType('vae'))\nwith loaded_model as vae:\n image = vae.decode(latents)[0]\n</code></pre> <p>The object returned by the LoadedModel context manager is an <code>AnyModel</code>, which is a Union of <code>ModelMixin</code>, <code>torch.nn.Module</code>, <code>IAIOnnxRuntimeModel</code>, <code>IPAdapter</code>, <code>IPAdapterPlus</code>, and <code>EmbeddingModelRaw</code>. <code>ModelMixin</code> is the base class of all diffusers models, <code>EmbeddingModelRaw</code> is used for LoRA and TextualInversion models. The others are obvious.</p> <p>In addition, you may call <code>LoadedModel.model_on_device()</code>, a context manager that returns a tuple of the model's state dict in CPU and the model itself in VRAM. It is used to optimize the LoRA patching and unpatching process:</p> <pre><code>loaded_model_= loader.get_model_by_key('f13dd932c0c35c22dcb8d6cda4203764', SubModelType('vae'))\nwith loaded_model.model_on_device() as (state_dict, vae):\n image = vae.decode(latents)[0]\n</code></pre> <p>Since not all models have state dicts, the <code>state_dict</code> return value can be None.</p>"},{"location":"contributing/MODEL_MANAGER/#emitting-model-loading-events","title":"Emitting model loading events","text":"<p>When the <code>context</code> argument is passed to <code>load_model_*()</code>, it will retrieve the invocation event bus from the passed <code>InvocationContext</code> object to emit events on the invocation bus. The two events are \"model_load_started\" and \"model_load_completed\". Both carry the following payload:</p> <pre><code>payload=dict(\n queue_id=queue_id,\n queue_item_id=queue_item_id,\n queue_batch_id=queue_batch_id,\n graph_execution_state_id=graph_execution_state_id,\n model_key=model_key,\n submodel_type=submodel,\n hash=model_info.hash,\n location=str(model_info.location),\n precision=str(model_info.precision),\n)\n</code></pre>"},{"location":"contributing/MODEL_MANAGER/#adding-model-loaders","title":"Adding Model Loaders","text":"<p>Model loaders are small classes that inherit from the <code>ModelLoader</code> base class. They typically implement one method <code>_load_model()</code> whose signature is:</p> <pre><code>def _load_model(\n    self,\n    model_path: Path,\n    model_variant: Optional[ModelRepoVariant] = None,\n    submodel_type: Optional[SubModelType] = None,\n) -&gt; AnyModel:\n</code></pre> <p><code>_load_model()</code> will be passed the path to the model on disk, an optional repository variant (used by the diffusers loaders to select, e.g.  the <code>fp16</code> variant, and an optional submodel_type for main and onnx models.</p> <p>To install a new loader, place it in <code>invokeai/backend/model_manager/load/model_loaders</code>. Inherit from <code>ModelLoader</code> and use the <code>@ModelLoaderRegistry.register()</code> decorator to indicate what type of models the loader can handle.</p> <p>Here is a complete example from <code>generic_diffusers.py</code>, which is able to load several different diffusers types:</p> <pre><code>from pathlib import Path\nfrom typing import Optional\n\nfrom invokeai.backend.model_manager import (\n    AnyModel,\n    BaseModelType,\n    ModelFormat,\n    ModelRepoVariant,\n    ModelType,\n    SubModelType,\n)\nfrom .. import ModelLoader, ModelLoaderRegistry\n\n\n@ModelLoaderRegistry.register(base=BaseModelType.Any, type=ModelType.CLIPVision, format=ModelFormat.Diffusers)\n@ModelLoaderRegistry.register(base=BaseModelType.Any, type=ModelType.T2IAdapter, format=ModelFormat.Diffusers)\nclass GenericDiffusersLoader(ModelLoader):\n    \"\"\"Class to load simple diffusers models.\"\"\"\n\n    def _load_model(\n        self,\n        model_path: Path,\n        model_variant: Optional[ModelRepoVariant] = None,\n        submodel_type: Optional[SubModelType] = None,\n    ) -&gt; AnyModel:\n        model_class = self._get_hf_load_class(model_path)\n        if submodel_type is not None:\n            raise Exception(f\"There are no submodels in models of type {model_class}\")\n        variant = model_variant.value if model_variant else None\n        result: AnyModel = model_class.from_pretrained(model_path, torch_dtype=self._torch_dtype, variant=variant)  # type: ignore\n        return result\n</code></pre> <p>Note that a loader can register itself to handle several different model types. An exception will be raised if more than one loader tries to register the same model type.</p>"},{"location":"contributing/MODEL_MANAGER/#conversion","title":"Conversion","text":"<p>Some models require conversion to diffusers format before they can be loaded. These loaders should override two additional methods:</p> <pre><code>_needs_conversion(self, config: AnyModelConfig, model_path: Path, dest_path: Path) -&gt; bool\n_convert_model(self, config: AnyModelConfig, model_path: Path, output_path: Path) -&gt; Path:\n</code></pre> <p>The first method accepts the model configuration, the path to where the unmodified model is currently installed, and a proposed destination for the converted model. This method returns True if the model needs to be converted. It typically does this by comparing the last modification time of the original model file to the modification time of the converted model. In some cases you will also want to check the modification date of the configuration record, in the event that the user has changed something like the scheduler prediction type that will require the model to be re-converted. See <code>controlnet.py</code> for an example of this logic.</p> <p>The second method accepts the model configuration, the path to the original model on disk, and the desired output path for the converted model. It does whatever it needs to do to get the model into diffusers format, and returns the Path of the resulting model. (The path should ordinarily be the same as <code>output_path</code>.)</p>"},{"location":"contributing/MODEL_MANAGER/#the-modelmanagerservice-object","title":"The ModelManagerService object","text":"<p>For convenience, the API provides a <code>ModelManagerService</code> object which gives a single point of access to the major model manager services. This object is created at initialization time and can be found in the global <code>ApiDependencies.invoker.services.model_manager</code> object, or in <code>context.services.model_manager</code> from within an invocation.</p> <p>In the examples below, we have retrieved the manager using:</p> <pre><code>mm = ApiDependencies.invoker.services.model_manager\n</code></pre> <p>The following properties and methods will be available:</p>"},{"location":"contributing/MODEL_MANAGER/#mmstore","title":"mm.store","text":"<p>This retrieves the <code>ModelRecordService</code> associated with the manager. Example:</p> <pre><code>configs = mm.store.get_model_by_attr(name='stable-diffusion-v1-5')\n</code></pre>"},{"location":"contributing/MODEL_MANAGER/#mminstall","title":"mm.install","text":"<p>This retrieves the <code>ModelInstallService</code> associated with the manager. Example:</p> <pre><code>job = mm.install.heuristic_import(`https://civitai.com/models/58390/detail-tweaker-lora-lora`)\n</code></pre>"},{"location":"contributing/MODEL_MANAGER/#mmload","title":"mm.load","text":"<p>This retrieves the <code>ModelLoaderService</code> associated with the manager. Example:</p> <pre><code>configs = mm.store.get_model_by_attr(name='stable-diffusion-v1-5')\nassert len(configs) &gt; 0\n\nloaded_model = mm.load.load_model(configs[0])\n</code></pre> <p>The model manager also offers a few convenience shortcuts for loading models:</p>"},{"location":"contributing/MODEL_MANAGER/#mmload_model_by_configmodel_config-submodel-context-loadedmodel","title":"mm.load_model_by_config(model_config, [submodel], [context]) -&gt; LoadedModel","text":"<p>Same as <code>mm.load.load_model()</code>.</p>"},{"location":"contributing/MODEL_MANAGER/#mmload_model_by_attrmodel_name-base_model-model_type-submodel-context-loadedmodel","title":"mm.load_model_by_attr(model_name, base_model, model_type, [submodel], [context]) -&gt; LoadedModel","text":"<p>This accepts the combination of the model's name, type and base, which it passes to the model record config store for retrieval. If a unique model config is found, this method returns a <code>LoadedModel</code>. It can raise the following exceptions:</p> <pre><code>UnknownModelException -- model with these attributes not known\nNotImplementedException -- the loader doesn't know how to load this type of model\nValueError -- more than one model matches this combination of base/type/name\n</code></pre>"},{"location":"contributing/MODEL_MANAGER/#mmload_model_by_keykey-submodel-context-loadedmodel","title":"mm.load_model_by_key(key, [submodel], [context]) -&gt; LoadedModel","text":"<p>This method takes a model key, looks it up using the <code>ModelRecordServiceBase</code> object in <code>mm.store</code>, and passes the returned model configuration to <code>load_model_by_config()</code>.  It may raise a <code>NotImplementedException</code>.</p>"},{"location":"contributing/MODEL_MANAGER/#invocation-context-model-manager-api","title":"Invocation Context Model Manager API","text":"<p>Within invocations, the following methods are available from the <code>InvocationContext</code> object:</p>"},{"location":"contributing/MODEL_MANAGER/#contextdownload_and_cache_modelsource-path","title":"context.download_and_cache_model(source) -&gt; Path","text":"<p>This method accepts a <code>source</code> of a remote model, downloads and caches it locally, and then returns a Path to the local model. The source can be a direct download URL or a HuggingFace repo_id.</p> <p>In the case of HuggingFace repo_id, the following variants are recognized:</p> <ul> <li>stabilityai/stable-diffusion-v4           -- default model</li> <li>stabilityai/stable-diffusion-v4:fp16      -- fp16 variant</li> <li>stabilityai/stable-diffusion-v4:fp16:vae  -- the fp16 vae subfolder</li> <li>stabilityai/stable-diffusion-v4:onnx:vae  -- the onnx variant vae subfolder</li> </ul> <p>You can also point at an arbitrary individual file within a repo_id directory using this syntax:</p> <ul> <li>stabilityai/stable-diffusion-v4::/checkpoints/sd4.safetensors</li> </ul>"},{"location":"contributing/MODEL_MANAGER/#contextload_local_modelmodel_path-loader-loadedmodel","title":"context.load_local_model(model_path, [loader]) -&gt; LoadedModel","text":"<p>This method loads a local model from the indicated path, returning a <code>LoadedModel</code>. The optional loader is a Callable that accepts a Path to the object, and returns a <code>AnyModel</code> object. If no loader is provided, then the method will use <code>torch.load()</code> for a .ckpt or .bin checkpoint file, <code>safetensors.torch.load_file()</code> for a safetensors checkpoint file, or <code>cls.from_pretrained()</code> for a directory that looks like a diffusers directory.</p>"},{"location":"contributing/MODEL_MANAGER/#contextload_remote_modelsource-loader-loadedmodel","title":"context.load_remote_model(source, [loader]) -&gt; LoadedModel","text":"<p>This method accepts a <code>source</code> of a remote model, downloads and caches it locally, loads it, and returns a <code>LoadedModel</code>. The source can be a direct download URL or a HuggingFace repo_id.</p> <p>In the case of HuggingFace repo_id, the following variants are recognized:</p> <ul> <li>stabilityai/stable-diffusion-v4           -- default model</li> <li>stabilityai/stable-diffusion-v4:fp16      -- fp16 variant</li> <li>stabilityai/stable-diffusion-v4:fp16:vae  -- the fp16 vae subfolder</li> <li>stabilityai/stable-diffusion-v4:onnx:vae  -- the onnx variant vae subfolder</li> </ul> <p>You can also point at an arbitrary individual file within a repo_id directory using this syntax:</p> <ul> <li>stabilityai/stable-diffusion-v4::/checkpoints/sd4.safetensors</li> </ul>"},{"location":"contributing/TESTS/","title":"InvokeAI Backend Tests","text":"<p>We use <code>pytest</code> to run the backend python tests. (See pyproject.toml for the default <code>pytest</code> options.)</p>"},{"location":"contributing/TESTS/#fast-vs-slow","title":"Fast vs. Slow","text":"<p>All tests are categorized as either 'fast' (no test annotation) or 'slow' (annotated with the <code>@pytest.mark.slow</code> decorator).</p> <p>'Fast' tests are run to validate every PR, and are fast enough that they can be run routinely during development.</p> <p>'Slow' tests are currently only run manually on an ad-hoc basis. In the future, they may be automated to run nightly. Most developers are only expected to run the 'slow' tests that directly relate to the feature(s) that they are working on.</p> <p>As a rule of thumb, tests should be marked as 'slow' if there is a chance that they take &gt;1s (e.g. on a CPU-only machine with slow internet connection). Common examples of slow tests are tests that depend on downloading a model, or running model inference.</p>"},{"location":"contributing/TESTS/#running-tests","title":"Running Tests","text":"<p>Below are some common test commands: <pre><code># Run the fast tests. (This implicitly uses the configured default option: `-m \"not slow\"`.)\npytest tests/\n\n# Equivalent command to run the fast tests.\npytest tests/ -m \"not slow\"\n\n# Run the slow tests.\npytest tests/ -m \"slow\"\n\n# Run the slow tests from a specific file.\npytest tests/path/to/slow_test.py -m \"slow\"\n\n# Run all tests (fast and slow).\npytest tests -m \"\"\n</code></pre></p>"},{"location":"contributing/TESTS/#test-organization","title":"Test Organization","text":"<p>All backend tests are in the <code>tests/</code> directory. This directory mirrors the organization of the <code>invokeai/</code> directory. For example, tests for <code>invokeai/model_management/model_manager.py</code> would be found in <code>tests/model_management/test_model_manager.py</code>.</p> <p>TODO: The above statement is aspirational. A re-organization of legacy tests is required to make it true.</p>"},{"location":"contributing/TESTS/#tests-that-depend-on-models","title":"Tests that depend on models","text":"<p>There are a few things to keep in mind when adding tests that depend on models.</p> <ol> <li>If a required model is not already present, it should automatically be downloaded as part of the test setup.</li> <li>If a model is already downloaded, it should not be re-downloaded unnecessarily.</li> <li>Take reasonable care to keep the total number of models required for the tests low. Whenever possible, re-use models that are already required for other tests. If you are adding a new model, consider including a comment to explain why it is required/unique.</li> </ol> <p>There are several utilities to help with model setup for tests. Here is a sample test that depends on a model: <pre><code>import pytest\nimport torch\n\nfrom invokeai.backend.model_management.models.base import BaseModelType, ModelType\nfrom invokeai.backend.util.test_utils import install_and_load_model\n\n@pytest.mark.slow\ndef test_model(model_installer, torch_device):\n    model_info = install_and_load_model(\n        model_installer=model_installer,\n        model_path_id_or_url=\"HF/dummy_model_id\",\n        model_name=\"dummy_model\",\n        base_model=BaseModelType.StableDiffusion1,\n        model_type=ModelType.Dummy,\n    )\n\n    dummy_input = build_dummy_input(torch_device)\n\n    with torch.no_grad(), model_info as model:\n        model.to(torch_device, dtype=torch.float32)\n        output = model(dummy_input)\n\n    # Validate output...\n</code></pre></p>"},{"location":"contributing/TESTS/#test-coverage","title":"Test Coverage","text":"<p>To review test coverage, append <code>--cov</code> to your pytest command: <pre><code>pytest tests/ --cov\n</code></pre></p> <p>Test outcomes and coverage will be reported in the terminal. In addition, a more detailed report is created in both XML and HTML format in the <code>./coverage</code> folder. The HTML output is particularly helpful in identifying untested statements where coverage should be improved. The HTML report can be viewed by opening <code>./coverage/html/index.html</code>.</p> HTML coverage report output <p></p> <p></p>"},{"location":"contributing/contribution_guides/development/","title":"Development","text":""},{"location":"contributing/contribution_guides/development/#what-do-i-need-to-know-to-help","title":"What do I need to know to help?","text":"<p>If you are looking to help to with a code contribution, InvokeAI uses several different technologies under the hood: Python (Pydantic, FastAPI, diffusers) and Typescript (React, Redux Toolkit, ChakraUI, Mantine, Konva). Familiarity with StableDiffusion and image generation concepts is helpful, but not essential. </p>"},{"location":"contributing/contribution_guides/development/#get-started","title":"Get Started","text":"<p>To get started, take a look at our new contributors checklist</p> <p>Once you're setup, for more information, you can review the documentation specific to your area of interest:</p> <p>If you don't feel ready to make a code contribution yet, no problem! You can also help out in other ways, such as documentation, translation or helping support other users and triage issues as they're reported in GitHub.</p> <p>There are two paths to making a development contribution: </p> <ol> <li>Choosing an open issue to address. Open issues can be found in the Issues section of the InvokeAI repository. These are tagged by the issue type (bug, enhancement, etc.) along with the \u201cgood first issues\u201d tag denoting if they are suitable for first time contributors.<ol> <li>Additional items can be found on our roadmap. The roadmap is organized in terms of priority, and contains features of varying size and complexity. If there is an inflight item you\u2019d like to help with, reach out to the contributor assigned to the item to see how you can help. </li> </ol> </li> <li>Opening a new issue or feature to add. Please make sure you have searched through existing issues before creating new ones.</li> </ol> <p>Regardless of what you choose, please post in the  #dev-chat channel of the Discord before you start development in order to confirm that the issue or feature is aligned with the current direction of the project. We value our contributors time and effort and want to ensure that no one\u2019s time is being misspent.</p>"},{"location":"contributing/contribution_guides/development/#invokeai-architecure","title":"InvokeAI Architecure","text":""},{"location":"contributing/contribution_guides/development/#frontend-documentation","title":"Frontend Documentation","text":""},{"location":"contributing/contribution_guides/development/#node-documentation","title":"Node Documentation","text":""},{"location":"contributing/contribution_guides/development/#local-development","title":"Local Development","text":""},{"location":"contributing/contribution_guides/development/#best-practices","title":"Best Practices:","text":"<ul> <li>Keep your pull requests small. Smaller pull requests are more likely to be accepted and merged</li> <li>Comments! Commenting your code helps reviewers easily understand your contribution</li> <li>Use Python and Typescript\u2019s typing systems, and consider using an editor with LSP support to streamline development</li> <li>Make all communications public. This ensure knowledge is shared with the whole community</li> </ul>"},{"location":"contributing/contribution_guides/development/#where-can-i-go-for-help","title":"Where can I go for help?","text":"<p>If you need help, you can ask questions in the #dev-chat channel of the Discord.</p> <p>For frontend related work, @psychedelicious is the best person to reach out to. </p> <p>For backend related work, please reach out to @blessedcoolant, @lstein, @StAlKeR7779 or @psychedelicious.</p>"},{"location":"contributing/contribution_guides/development/#what-does-the-code-of-conduct-mean-for-me","title":"What does the Code of Conduct mean for me?","text":"<p>Our Code of Conduct  means that you are responsible for treating everyone on the project with respect and courtesy regardless of their identity. If you are the victim of any inappropriate behavior or comments as described in our Code of Conduct, we are here for you and will do the best to ensure that the abuser is reprimanded appropriately, per our code.</p>"},{"location":"contributing/contribution_guides/documentation/","title":"Documentation","text":"<p>Documentation is an important part of any open source project. It provides a clear and concise way to communicate how the software works, how to use it, and how to troubleshoot issues. Without proper documentation, it can be difficult for users to understand the purpose and functionality of the project. </p>"},{"location":"contributing/contribution_guides/documentation/#contributing","title":"Contributing","text":"<p>All documentation is maintained in the InvokeAI GitHub repository. If you come across documentation that is out of date or incorrect, please submit a pull request with the necessary changes. </p> <p>When updating or creating documentation, please keep in mind InvokeAI is a tool for everyone, not just those who have familiarity with generative art. </p>"},{"location":"contributing/contribution_guides/documentation/#help-questions","title":"Help &amp; Questions","text":"<p>Please ping @imic or @hipsterusername in the Discord if you have any questions.</p>"},{"location":"contributing/contribution_guides/newContributorChecklist/","title":"New Contributor Guide","text":"<p>If you're a new contributor to InvokeAI or Open Source Projects, this is the guide for you. </p>"},{"location":"contributing/contribution_guides/newContributorChecklist/#new-contributor-checklist","title":"New Contributor Checklist","text":"<ul> <li> Set up your local development environment &amp; fork of InvokAI by following the steps outlined here</li> <li> Set up your local tooling with this guide. Feel free to skip this step if you already have tooling you're comfortable with. </li> <li> Familiarize yourself with Git &amp; our project structure by reading through the development documentation</li> <li> Join the #dev-chat channel of the Discord</li> <li> Choose an issue to work on! This can be achieved by asking in the #dev-chat channel, tackling a good first issue or finding an item on the roadmap. If nothing in any of those places catches your eye, feel free to work on something of interest to you! </li> <li> Make your first Pull Request with the guide below</li> <li> Happy development! Don't be afraid to ask for help - we're happy to help you contribute!</li> </ul>"},{"location":"contributing/contribution_guides/newContributorChecklist/#how-do-i-make-a-contribution","title":"How do I make a contribution?","text":"<p>Never made an open source contribution before? Wondering how contributions work in our project? Here's a quick rundown!</p> <p>Before starting these steps, ensure you have your local environment configured for development.</p> <ol> <li>Find a good first issue that you are interested in addressing or a feature that you would like to add. Then, reach out to our team in the #dev-chat channel of the Discord to ensure you are  setup for success. </li> <li>Fork the InvokeAI repository to your GitHub profile. This means that you will have a copy of the repository under\u00a0your-GitHub-username/InvokeAI.</li> <li>Clone the repository to your local machine using: <pre><code>git clone https://github.com/your-GitHub-username/InvokeAI.git\n</code></pre> If you're unfamiliar with using Git through the commandline, GitHub Desktop is a easy-to-use alternative with a UI. You can do all the same steps listed here, but through the interface. </li> <li>Create a new branch for your fix using: <pre><code>git checkout -b branch-name-here\n</code></pre></li> <li>Make the appropriate changes for the issue you are trying to address or the feature that you want to add.</li> <li>Add the file contents of the changed files to the \"snapshot\" git uses to manage the state of the project, also known as the index: <pre><code>git add -A\n</code></pre></li> <li>Store the contents of the index with a descriptive message. <pre><code>git commit -m \"Insert a short message of the changes made here\"\n</code></pre></li> <li>Push the changes to the remote repository using <pre><code>git push origin branch-name-here\n</code></pre></li> <li>Submit a pull request to the main branch of the InvokeAI repository. If you're not sure how to, follow this guide</li> <li>Title the pull request with a short description of the changes made and the issue or bug number associated with your change. For example, you can title an issue like so \"Added more log outputting to resolve #1234\".</li> <li>In the description of the pull request, explain the changes that you made, any issues you think exist with the pull request you made, and any questions you have for the maintainer. It's OK if your pull request is not perfect (no pull request is), the reviewer will be able to help you fix any problems and improve it!</li> <li>Wait for the pull request to be reviewed by other collaborators.</li> <li>Make changes to the pull request if the reviewer(s) recommend them.</li> <li>Celebrate your success after your pull request is merged!</li> </ol> <p>If you\u2019d like to learn more about contributing to Open Source projects, here is a\u00a0Getting Started Guide. </p>"},{"location":"contributing/contribution_guides/newContributorChecklist/#best-practices","title":"Best Practices:","text":"<ul> <li>Keep your pull requests small. Smaller pull requests are more likely to be accepted and merged</li> <li>Comments! Commenting your code helps reviewers easily understand your contribution</li> <li>Use Python and Typescript\u2019s typing systems, and consider using an editor with LSP support to streamline development</li> <li>Make all communications public. This ensure knowledge is shared with the whole community</li> </ul>"},{"location":"contributing/contribution_guides/newContributorChecklist/#where-can-i-go-for-help","title":"Where can I go for help?","text":"<p>If you need help, you can ask questions in the #dev-chat channel of the Discord.</p> <p>For frontend related work, @pyschedelicious is the best person to reach out to. </p> <p>For backend related work, please reach out to @blessedcoolant, @lstein, @StAlKeR7779 or @pyschedelicious.</p>"},{"location":"contributing/contribution_guides/translation/","title":"Translation","text":"<p>InvokeAI uses\u00a0Weblate\u00a0for translation. Weblate is a FOSS project providing a scalable translation service. Weblate automates the tedious parts of managing translation of a growing project, and the service is generously provided at no cost to FOSS projects like InvokeAI.</p>"},{"location":"contributing/contribution_guides/translation/#contributing","title":"Contributing","text":"<p>If you'd like to contribute by adding or updating a translation, please visit our\u00a0Weblate project. You'll need to sign in with your GitHub account (a number of other accounts are supported, including Google).</p> <p>Once signed in, select a language and then the Web UI component. From here you can Browse and Translate strings from English to your chosen language. Zen mode offers a simpler translation experience.</p> <p>Your changes will be attributed to you in the automated PR process; you don't need to do anything else.</p>"},{"location":"contributing/contribution_guides/translation/#help-questions","title":"Help &amp; Questions","text":"<p>Please check Weblate's\u00a0documentation\u00a0or ping @Harvestor on Discord if you have any questions.</p>"},{"location":"contributing/contribution_guides/translation/#thanks","title":"Thanks","text":"<p>Thanks to the InvokeAI community for their efforts to translate the project!</p>"},{"location":"contributing/contribution_guides/tutorials/","title":"Tutorials","text":"<p>Tutorials help new &amp; existing users expand their abilty to use InvokeAI to the full extent of our features and services.  </p> <p>Currently, we have a set of tutorials available on our YouTube channel, but as InvokeAI continues to evolve with new updates, we want to ensure that we are giving our users the resources they need to succeed. </p> <p>Tutorials can be in the form of videos or article walkthroughs on a subject of your choice. We recommend focusing tutorials on the key image generation methods, or on a specific component within one of the image generation methods.</p>"},{"location":"contributing/contribution_guides/tutorials/#contributing","title":"Contributing","text":"<p>Please reach out to @imic or @hipsterusername on Discord to help create tutorials for InvokeAI.</p>"},{"location":"contributing/frontend/OVERVIEW/","title":"Invoke UI","text":"<p>Invoke's UI is made possible by many contributors and open-source libraries. Thank you!</p>"},{"location":"contributing/frontend/OVERVIEW/#dev-environment","title":"Dev environment","text":""},{"location":"contributing/frontend/OVERVIEW/#setup","title":"Setup","text":"<ol> <li>Install node and pnpm.</li> <li>Run <code>pnpm i</code> to install all packages.</li> </ol>"},{"location":"contributing/frontend/OVERVIEW/#run-in-dev-mode","title":"Run in dev mode","text":"<ol> <li>From <code>invokeai/frontend/web/</code>, run <code>pnpm dev</code>.</li> <li>From repo root, run <code>python scripts/invokeai-web.py</code>.</li> <li>Point your browser to the dev server address, e.g. http://localhost:5173/</li> </ol>"},{"location":"contributing/frontend/OVERVIEW/#package-scripts","title":"Package scripts","text":"<ul> <li><code>dev</code>: run the frontend in dev mode, enabling hot reloading</li> <li><code>build</code>: run all checks (madge, eslint, prettier, tsc) and then build the frontend</li> <li><code>typegen</code>: generate types from the OpenAPI schema (see Type generation)</li> <li><code>lint:dpdm</code>: check circular dependencies</li> <li><code>lint:eslint</code>: check code quality</li> <li><code>lint:prettier</code>: check code formatting</li> <li><code>lint:tsc</code>: check type issues</li> <li><code>lint:knip</code>: check for unused exports or objects (failures here are just suggestions, not hard fails)</li> <li><code>lint</code>: run all checks concurrently</li> <li><code>fix</code>: run <code>eslint</code> and <code>prettier</code>, fixing fixable issues</li> </ul>"},{"location":"contributing/frontend/OVERVIEW/#type-generation","title":"Type generation","text":"<p>We use openapi-typescript to generate types from the app's OpenAPI schema.</p> <p>The generated types are committed to the repo in schema.ts.</p> <pre><code># from the repo root, start the server\npython scripts/invokeai-web.py\n# from invokeai/frontend/web/, run the script\npnpm typegen\n</code></pre>"},{"location":"contributing/frontend/OVERVIEW/#localization","title":"Localization","text":"<p>We use i18next for localization, but translation to languages other than English happens on our Weblate project.</p> <p>Only the English source strings should be changed on this repo.</p>"},{"location":"contributing/frontend/OVERVIEW/#vscode","title":"VSCode","text":""},{"location":"contributing/frontend/OVERVIEW/#example-debugger-config","title":"Example debugger config","text":"<pre><code>{\n  \"version\": \"0.2.0\",\n  \"configurations\": [\n    {\n      \"type\": \"chrome\",\n      \"request\": \"launch\",\n      \"name\": \"Invoke UI\",\n      \"url\": \"http://localhost:5173\",\n      \"webRoot\": \"${workspaceFolder}/invokeai/frontend/web\"\n    }\n  ]\n}\n</code></pre>"},{"location":"contributing/frontend/OVERVIEW/#remote-dev","title":"Remote dev","text":"<p>We've noticed an intermittent timeout issue with the VSCode remote dev port forwarding.</p> <p>We suggest disabling the editor's port forwarding feature and doing it manually via SSH:</p> <pre><code>ssh -L 9090:localhost:9090 -L 5173:localhost:5173 user@host\n</code></pre>"},{"location":"contributing/frontend/OVERVIEW/#contributing-guidelines","title":"Contributing Guidelines","text":"<p>Thanks for your interest in contributing to the Invoke Web UI!</p> <p>Please follow these guidelines when contributing.</p>"},{"location":"contributing/frontend/OVERVIEW/#check-in-before-investing-your-time","title":"Check in before investing your time","text":"<p>Please check in before you invest your time on anything besides a trivial fix, in case it conflicts with ongoing work or isn't aligned with the vision for the app.</p> <p>If a feature request or issue doesn't already exist for the thing you want to work on, please create one.</p> <p>Ping <code>@psychedelicious</code> on discord in the <code>#frontend-dev</code> channel or in the feature request / issue you want to work on - we're happy to chat.</p>"},{"location":"contributing/frontend/OVERVIEW/#code-conventions","title":"Code conventions","text":"<ul> <li>This is a fairly complex app with a deep component tree. Please use memoization (<code>useCallback</code>, <code>useMemo</code>, <code>memo</code>) with enthusiasm.</li> <li>If you need to add some global, ephemeral state, please use [nanostores] if possible.</li> <li>Be careful with your redux selectors. If they need to be parameterized, consider creating them inside a <code>useMemo</code>.</li> <li>Feel free to use <code>lodash</code> (via <code>lodash-es</code>) to make the intent of your code clear.</li> <li>Please add comments describing the \"why\", not the \"how\" (unless it is really arcane).</li> </ul>"},{"location":"contributing/frontend/OVERVIEW/#commit-format","title":"Commit format","text":"<p>Please use the conventional commits spec for the web UI, with a scope of \"ui\":</p> <ul> <li><code>chore(ui): bump deps</code></li> <li><code>chore(ui): lint</code></li> <li><code>feat(ui): add some cool new feature</code></li> <li><code>fix(ui): fix some bug</code></li> </ul>"},{"location":"contributing/frontend/OVERVIEW/#submitting-a-pr","title":"Submitting a PR","text":"<ul> <li>Ensure your branch is tidy. Use an interactive rebase to clean up the commit history and reword the commit messages if they are not descriptive.</li> <li>Run <code>pnpm lint</code>. Some issues are auto-fixable with <code>pnpm fix</code>.</li> <li>Fill out the PR form when creating the PR.</li> <li>It doesn't need to be super detailed, but a screenshot or video is nice if you changed something visually.</li> <li>If a section isn't relevant, delete it. There are no UI tests at this time.</li> </ul>"},{"location":"contributing/frontend/OVERVIEW/#other-docs","title":"Other docs","text":"<ul> <li>Workflows - Design and Implementation</li> <li>State Management</li> </ul>"},{"location":"contributing/frontend/STATE_MGMT/","title":"State Management","text":"<p>The app makes heavy use of Redux Toolkit, its Query library, and <code>nanostores</code>.</p>"},{"location":"contributing/frontend/STATE_MGMT/#redux","title":"Redux","text":"<p>TODO</p>"},{"location":"contributing/frontend/STATE_MGMT/#nanostores","title":"<code>nanostores</code>","text":"<p>nanostores is a tiny state management library. It provides both imperative and declarative APIs.</p>"},{"location":"contributing/frontend/STATE_MGMT/#example","title":"Example","text":"<pre><code>export const $myStringOption = atom&lt;string | null&gt;(null);\n\n// Outside a component, or within a callback for performance-critical logic\n$myStringOption.get();\n$myStringOption.set('new value');\n\n// Inside a component\nconst myStringOption = useStore($myStringOption);\n</code></pre>"},{"location":"contributing/frontend/STATE_MGMT/#where-to-put-nanostores","title":"Where to put nanostores","text":"<ul> <li>For global application state, export your stores from <code>invokeai/frontend/web/src/app/store/nanostores/</code>.</li> <li>For feature state, create a file for the stores next to the redux slice definition (e.g. <code>invokeai/frontend/web/src/features/myFeature/myFeatureNanostores.ts</code>).</li> <li>For hooks with global state, export the store from the same file the hook is in, or put it next to the hook.</li> </ul>"},{"location":"contributing/frontend/STATE_MGMT/#when-to-use-nanostores","title":"When to use nanostores","text":"<ul> <li>For non-serializable data that needs to be available throughout the app, use <code>nanostores</code> instead of a global.</li> <li>For ephemeral global state (i.e. state that does not need to be persisted), use <code>nanostores</code> instead of redux.</li> <li>For performance-critical code and in callbacks, redux selectors can be problematic due to the declarative reactivity system. Consider refactoring to use <code>nanostores</code> if there's a measurable performance issue.</li> </ul>"},{"location":"contributing/frontend/WORKFLOWS/","title":"Workflows - Design and Implementation","text":"<p>This document describes, at a high level, the design and implementation of workflows in the InvokeAI frontend. There are a substantial number of implementation details not included, but which are hopefully clear from the code.</p> <p>InvokeAI's backend uses graphs, composed of nodes and edges, to process data and generate images.</p> <p>Nodes have any number of input fields and output fields. Edges connect nodes together via their inputs and outputs. Fields have data types which dictate how they may be connected.</p> <p>During execution, a nodes' outputs may be passed along to any number of other nodes' inputs.</p> <p>Workflows are an enriched abstraction over a graph.</p>"},{"location":"contributing/frontend/WORKFLOWS/#design","title":"Design","text":"<p>InvokeAI provide two ways to build graphs in the frontend: the Linear UI and Workflow Editor.</p> <p>To better understand the use case and challenges related to workflows, we will review both of these modes.</p>"},{"location":"contributing/frontend/WORKFLOWS/#linear-ui","title":"Linear UI","text":"<p>This includes the Text to Image, Image to Image and Unified Canvas tabs.</p> <p>The user-managed parameters on these tabs are stored as simple objects in the application state. When the user invokes, adding a generation to the queue, we internally build a graph from these parameters.</p> <p>This logic can be fairly complex due to the range of features available and their interactions. Depending on the parameters selected, the graph may be very different. Building graphs in code can be challenging - you are trying to construct a non-linear structure in a linear context.</p> <p>The simplest graph building logic is for Text to Image with a SD1.5 model: buildLinearTextToImageGraph.ts</p> <p>There are many other graph builders in the same directory for different tabs or base models (e.g. SDXL). Some are pretty hairy.</p> <p>In the Linear UI, we go straight from simple application state to graph via these builders.</p>"},{"location":"contributing/frontend/WORKFLOWS/#workflow-editor","title":"Workflow Editor","text":"<p>The Workflow Editor is a visual graph editor, allowing users to draw edges from node to node to construct a graph. This far more approachable way to create complex graphs.</p> <p>InvokeAI uses the reactflow library to power the Workflow Editor. It provides both a graph editor UI and manages its own internal graph state.</p>"},{"location":"contributing/frontend/WORKFLOWS/#workflows","title":"Workflows","text":"<p>A workflow is a representation of a graph plus additional metadata:</p> <ul> <li>Name</li> <li>Description</li> <li>Version</li> <li>Notes</li> <li>Exposed fields</li> <li>Author, tags, category, etc.</li> </ul> <p>Workflows should have other qualities:</p> <ul> <li>Portable: you should be able to load a workflow created by another person.</li> <li>Resilient: you should be able to \"upgrade\" a workflow as the application changes.</li> <li>Abstract: as much as is possible, workflows should not be married to the specific implementation details of the application.</li> </ul> <p>To support these qualities, workflows are serializable, have a versioned schemas, and represent graphs as minimally as possible. Fortunately, the reactflow state for nodes and edges works perfectly for this.</p>"},{"location":"contributing/frontend/WORKFLOWS/#workflow-reactflow-state-invokeai-graph","title":"Workflow -&gt; reactflow state -&gt; InvokeAI graph","text":"<p>Given a workflow, we need to be able to derive reactflow state and/or an InvokeAI graph from it.</p> <p>The first step - workflow to reactflow state - is very simple. The logic is in nodesSlice.ts, in the <code>workflowLoaded</code> reducer.</p> <p>The reactflow state is, however, structurally incompatible with our backend's graph structure. When a user invokes on a Workflow, we need to convert the reactflow state into an InvokeAI graph. This is far simpler than the graph building logic from the Linear UI: buildNodesGraph.ts</p>"},{"location":"contributing/frontend/WORKFLOWS/#nodes-vs-invocations","title":"Nodes vs Invocations","text":"<p>We often use the terms \"node\" and \"invocation\" interchangeably, but they may refer to different things in the frontend.</p> <p>reactflow has its own definitions of \"node\", \"edge\" and \"handle\" which are closely related to InvokeAI graph concepts.</p> <ul> <li>A reactflow node is related to an InvokeAI invocation. It has a \"data\" property, which holds the InvokeAI-specific invocation data.</li> <li>A reactflow edge is roughly equivalent to an InvokeAI edge.</li> <li>A reactflow handle is roughly equivalent to an InvokeAI input or output field.</li> </ul>"},{"location":"contributing/frontend/WORKFLOWS/#workflow-linear-view","title":"Workflow Linear View","text":"<p>Graphs are very capable data structures, but not everyone wants to work with them all the time.</p> <p>To allow less technical users - or anyone who wants a less visually noisy workspace - to benefit from the power of nodes, InvokeAI has a workflow feature called the Linear View.</p> <p>A workflow input field can be added to this Linear View, and its input component can be presented similarly to the Linear UI tabs. Internally, we add the field to the workflow's list of exposed fields.</p>"},{"location":"contributing/frontend/WORKFLOWS/#openapi-schema","title":"OpenAPI Schema","text":"<p>OpenAPI is a schema specification that can represent complex data structures and relationships. The backend is capable of generating an OpenAPI schema for all invocations.</p> <p>When the UI connects, it requests this schema and parses each invocation into an invocation template. Invocation templates have a number of properties, like title, description and type, but the most important ones are their input and output field templates.</p> <p>Invocation and field templates are the \"source of truth\" for graphs, because they indicate what the backend is able to process.</p> <p>When a user adds a new node to their workflow, these templates are used to instantiate a node with fields instantiated from the input and output field templates.</p>"},{"location":"contributing/frontend/WORKFLOWS/#field-instances-and-templates","title":"Field Instances and Templates","text":"<p>Field templates consist of:</p> <ul> <li>Name: the identifier of the field, its variable name in python</li> <li>Type: derived from the field's type annotation in python (e.g. IntegerField, ImageField, MainModelField)</li> <li>Constraints: derived from the field's creation args in python (e.g. minimum value for an integer)</li> <li>Default value: optionally provided in the field's creation args (e.g. 42 for an integer)</li> </ul> <p>Field instances are created from the templates and have name, type and optionally a value.</p> <p>The type of the field determines the UI components that are rendered for it.</p> <p>A field instance's name associates it with its template.</p>"},{"location":"contributing/frontend/WORKFLOWS/#stateful-vs-stateless-fields","title":"Stateful vs Stateless Fields","text":"<p>Stateful fields store their value in the frontend graph. Think primitives, model identifiers, images, etc. Fields are only stateful if the frontend allows the user to directly input a value for them.</p> <p>Many field types, however, are stateless. An example is a <code>UNetField</code>, which contains some data describing a UNet. Users cannot directly provide this data - it is created and consumed in the backend.</p> <p>Stateless fields do not store their value in the node, so their field instances do not have values.</p> <p>\"Custom\" fields will always be treated as stateless fields.</p>"},{"location":"contributing/frontend/WORKFLOWS/#single-and-collection-fields","title":"Single and Collection Fields","text":"<p>Field types have a name and cardinality property which may identify it as a SINGLE, COLLECTION or SINGLE_OR_COLLECTION field.</p> <ul> <li>If a field is annotated in python as a singular value or class, its field type is parsed as a SINGLE type (e.g. <code>int</code>, <code>ImageField</code>, <code>str</code>).</li> <li>If a field is annotated in python as a list, its field type is parsed as a COLLECTION type (e.g. <code>list[int]</code>).</li> <li>If it is annotated as a union of a type and list, the type will be parsed as a SINGLE_OR_COLLECTION type (e.g. <code>Union[int, list[int]]</code>). Fields may not be unions of different types (e.g. <code>Union[int, list[str]]</code> and <code>Union[int, str]</code> are not allowed).</li> </ul>"},{"location":"contributing/frontend/WORKFLOWS/#implementation","title":"Implementation","text":"<p>The majority of data structures in the backend are pydantic models. Pydantic provides OpenAPI schemas for all models and we then generate TypeScript types from those.</p> <p>The OpenAPI schema is parsed at runtime into our invocation templates.</p> <p>Workflows and all related data are modeled in the frontend using zod. Related types are inferred from the zod schemas.</p> <p>In python, invocations are pydantic models with fields. These fields become node inputs. The invocation's <code>invoke()</code> function returns a pydantic model - its output. Like the invocation itself, the output model has any number of fields, which become node outputs.</p>"},{"location":"contributing/frontend/WORKFLOWS/#zod-schemas-and-types","title":"zod Schemas and Types","text":"<p>The zod schemas, inferred types, and type guards are in [types/].</p> <p>Roughly order from lowest-level to highest:</p> <ul> <li><code>common.ts</code>: stateful field data, and couple other misc types</li> <li><code>field.ts</code>: fields - types, values, instances, templates</li> <li><code>invocation.ts</code>: invocations and other node types</li> <li><code>workflow.ts</code>: workflows and constituents</li> </ul> <p>We customize the OpenAPI schema to include additional properties on invocation and field schemas. To facilitate parsing this schema into templates, we modify/wrap the types from openapi-types in <code>openapi.ts</code>.</p>"},{"location":"contributing/frontend/WORKFLOWS/#openapi-schema-parsing","title":"OpenAPI Schema Parsing","text":"<p>The entrypoint for OpenAPI schema parsing is parseSchema.ts.</p> <p>General logic flow:</p> <ul> <li>Iterate over all invocation schema objects</li> <li>Extract relevant invocation-level attributes (e.g. title, type, version, etc)</li> <li>Iterate over the invocation's input fields<ul> <li>Parse each field's type</li> <li>Build a field input template from the type - either a stateful template or \"generic\" stateless template</li> </ul> </li> <li>Iterate over the invocation's output fields<ul> <li>Parse the field's type (same as inputs)</li> <li>Build a field output template</li> </ul> </li> <li>Assemble the attributes and fields into an invocation template</li> </ul> <p>Most of these involve very straightforward <code>reduce</code>s, but the less intuitive steps are detailed below.</p>"},{"location":"contributing/frontend/WORKFLOWS/#parsing-field-types","title":"Parsing Field Types","text":"<p>Field types are represented as structured objects:</p> <pre><code>type FieldType = {\n  name: string;\n  cardinality: 'SINGLE' | 'COLLECTION' | 'SINGLE_OR_COLLECTION';\n};\n</code></pre> <p>The parsing logic is in <code>parseFieldType.ts</code>.</p> <p>There are 4 general cases for field type parsing.</p>"},{"location":"contributing/frontend/WORKFLOWS/#primitive-types","title":"Primitive Types","text":"<p>When a field is annotated as a primitive values (e.g. <code>int</code>, <code>str</code>, <code>float</code>), the field type parsing is fairly straightforward. The field is represented by a simple OpenAPI schema object, which has a <code>type</code> property.</p> <p>We create a field type name from this <code>type</code> string (e.g. <code>string</code> -&gt; <code>StringField</code>). The cardinality is <code>\"SINGLE\"</code>.</p>"},{"location":"contributing/frontend/WORKFLOWS/#complex-types","title":"Complex Types","text":"<p>When a field is annotated as a pydantic model (e.g. <code>ImageField</code>, <code>MainModelField</code>, <code>ControlField</code>), it is represented as a reference object. Reference objects are pointers to another schema or reference object within the schema.</p> <p>We need to dereference the schema to pull these out. Dereferencing may require recursion. We use the reference object's name directly for the field type name.</p> <p>Unfortunately, at this time, we've had limited success using external libraries to deference at runtime, so we do this ourselves.</p>"},{"location":"contributing/frontend/WORKFLOWS/#collection-types","title":"Collection Types","text":"<p>When a field is annotated as a list of a single type, the schema object has an <code>items</code> property. They may be a schema object or reference object and must be parsed to determine the item type.</p> <p>We use the item type for field type name. The cardinality is <code>\"COLLECTION\"</code>.</p>"},{"location":"contributing/frontend/WORKFLOWS/#single-or-collection-types","title":"Single or Collection Types","text":"<p>When a field is annotated as a union of a type and list of that type, the schema object has an <code>anyOf</code> property, which holds a list of valid types for the union.</p> <p>After verifying that the union has two members (a type and list of the same type), we use the type for field type name, with cardinality <code>\"SINGLE_OR_COLLECTION\"</code>.</p>"},{"location":"contributing/frontend/WORKFLOWS/#optional-fields","title":"Optional Fields","text":"<p>In OpenAPI v3.1, when an object is optional, it is put into an <code>anyOf</code> along with a primitive schema object with <code>type: 'null'</code>.</p> <p>Handling this adds a fair bit of complexity, as we now must filter out the <code>'null'</code> types and work with the remaining types as described above.</p> <p>If there is a single remaining schema object, we must recursively call to <code>parseFieldType()</code> to get parse it.</p>"},{"location":"contributing/frontend/WORKFLOWS/#building-field-input-templates","title":"Building Field Input Templates","text":"<p>Now that we have a field type, we can build an input template for the field.</p> <p>Stateful fields all get a function to build their template, while stateless fields are constructed directly. This is possible because stateless fields have no default value or constraints.</p> <p>See buildFieldInputTemplate.ts.</p>"},{"location":"contributing/frontend/WORKFLOWS/#building-field-output-templates","title":"Building Field Output Templates","text":"<p>Field outputs are similar to stateless fields - they do not have any value in the frontend. When building their templates, we don't need a special function for each field type.</p> <p>See buildFieldOutputTemplate.ts.</p>"},{"location":"contributing/frontend/WORKFLOWS/#managing-reactflow-state","title":"Managing reactflow State","text":"<p>As described above, the workflow editor state is the essentially the reactflow state, plus some extra metadata.</p> <p>We provide reactflow with an array of nodes and edges via redux, and a number of event handlers. These handlers dispatch redux actions, managing nodes and edges.</p> <p>The pieces of redux state relevant to workflows are:</p> <ul> <li><code>state.nodes.nodes</code>: the reactflow nodes state</li> <li><code>state.nodes.edges</code>: the reactflow edges state</li> <li><code>state.nodes.workflow</code>: the workflow metadata</li> </ul>"},{"location":"contributing/frontend/WORKFLOWS/#building-nodes-and-edges","title":"Building Nodes and Edges","text":"<p>A reactflow node has a few important top-level properties:</p> <ul> <li><code>id</code>: unique identifier</li> <li><code>type</code>: a string that maps to a react component to render the node</li> <li><code>position</code>: XY coordinates</li> <li><code>data</code>: arbitrary data</li> </ul> <p>When the user adds a node, we build invocation node data, storing it in <code>data</code>. Invocation properties (e.g. type, version, label, etc.) are copied from the invocation template. Inputs and outputs are built from the invocation template's field templates.</p> <p>See buildInvocationNode.ts.</p> <p>Edges are managed by reactflow, but briefly, they consist of:</p> <ul> <li><code>source</code>: id of the source node</li> <li><code>sourceHandle</code>: id of the source node handle (output field)</li> <li><code>target</code>: id of the target node</li> <li><code>targetHandle</code>: id of the target node handle (input field)</li> </ul> <p>Edge creation is gated behind validation logic. This validation compares the input and output field types and overall graph state.</p>"},{"location":"contributing/frontend/WORKFLOWS/#building-a-workflow","title":"Building a Workflow","text":"<p>Building a workflow entity is as simple as dropping the nodes, edges and metadata into an object.</p> <p>Each node and edge is parsed with a zod schema, which serves to strip out any unneeded data.</p> <p>See buildWorkflow.ts.</p>"},{"location":"contributing/frontend/WORKFLOWS/#loading-a-workflow","title":"Loading a Workflow","text":"<p>Workflows may be loaded from external sources or the user's local instance. In all cases, the workflow needs to be handled with care, as an untrusted object.</p> <p>Loading has a few stages which may throw or warn if there are problems:</p> <ul> <li>Parsing the workflow data structure itself, migrating it if necessary (throws)</li> <li>Check for a template for each node (warns)</li> <li>Check each node's version against its template (warns)</li> <li>Validate the source and target of each edge (warns)</li> </ul> <p>This validation occurs in validateWorkflow.ts.</p> <p>If there are no fatal errors, the workflow is then stored in redux state.</p>"},{"location":"contributing/frontend/WORKFLOWS/#workflow-migrations","title":"Workflow Migrations","text":"<p>When the workflow schema changes, we may need to perform some data migrations. This occurs as workflows are loaded. zod schemas for each workflow schema version is retained to facilitate migrations.</p> <p>Previous schemas are in folders in <code>invokeai/frontend/web/src/features/nodes/types/</code>, eg <code>v1/</code>.</p> <p>Migration logic is in migrations.ts.</p>"},{"location":"features/","title":"Overview","text":"<p>Here you can find the documentation for InvokeAI's various features.</p>"},{"location":"features/#the-getting-started-guide","title":"The Getting Started Guide","text":"<p>A getting started guide for those new to AI image generation. </p>"},{"location":"features/#the-basics","title":"The Basics","text":""},{"location":"features/#the-web-user-interface","title":"* The Web User Interface","text":"<p>Guide to the Web interface. Also see the WebUI Hotkeys Reference Guide</p>"},{"location":"features/#the-unified-canvas","title":"* The Unified Canvas","text":"<p>Build complex scenes by combine and modifying multiple images in a stepwise fashion. This feature combines img2img, inpainting and outpainting in a single convenient digital artist-optimized user interface.</p>"},{"location":"features/#image-generation","title":"Image Generation","text":""},{"location":"features/#prompt-engineering","title":"* Prompt Engineering","text":"<p>Get the images you want with the InvokeAI  prompt engineering language.</p>"},{"location":"features/#the-lora-lycoris-lcm-lora-models","title":"* The LoRA, LyCORIS, LCM-LoRA Models","text":"<p>Add custom subjects and styles using a variety of fine-tuned models.</p>"},{"location":"features/#controlnet","title":"* ControlNet","text":"<p>Learn how to install and use ControlNet models for fine control over image output.</p>"},{"location":"features/#image-to-image-guide","title":"* Image-to-Image Guide","text":"<p>Use a seed image to build new creations.</p>"},{"location":"features/#model-management","title":"Model Management","text":""},{"location":"features/#model-installation","title":"* Model Installation","text":"<p>Learn how to import third-party models and switch among them. This guide also covers optimizing models to load quickly.</p>"},{"location":"features/#merging-models","title":"* Merging Models","text":"<p>Teach an old model new tricks. Merge 2-3 models together to create a new model that combines characteristics of the originals.</p>"},{"location":"features/#textual-inversion","title":"* Textual Inversion","text":"<p>Personalize models by adding your own style or subjects.</p>"},{"location":"features/#other-features","title":"Other Features","text":""},{"location":"features/#the-nsfw-checker","title":"* The NSFW Checker","text":"<p>Prevent InvokeAI from displaying unwanted racy images.</p>"},{"location":"features/#controlling-logging","title":"* Controlling Logging","text":"<p>Control how InvokeAI logs status messages.</p>"},{"location":"features/#command-line-utilities","title":"* Command-line Utilities","text":"<p>A list of the command-line utilities available with InvokeAI.</p>"},{"location":"features/CONFIGURATION/","title":"InvokeAI Configuration","text":""},{"location":"features/CONFIGURATION/#intro","title":"Intro","text":"<p>Runtime settings, including the location of files and directories, memory usage, and performance, are managed via the <code>invokeai.yaml</code> config file or environment variables. A subset of settings may be set via commandline arguments.</p> <p>Settings sources are used in this order:</p> <ul> <li>CLI args</li> <li>Environment variables</li> <li><code>invokeai.yaml</code> settings</li> <li>Fallback: defaults</li> </ul>"},{"location":"features/CONFIGURATION/#invokeai-root-directory","title":"InvokeAI Root Directory","text":"<p>On startup, InvokeAI searches for its \"root\" directory. This is the directory that contains models, images, the database, and so on. It also contains a configuration file called <code>invokeai.yaml</code>.</p> <p>InvokeAI searches for the root directory in this order:</p> <ol> <li>The <code>--root &lt;path&gt;</code> CLI arg.</li> <li>The environment variable INVOKEAI_ROOT.</li> <li>The directory containing the currently active virtual environment.</li> <li>Fallback: a directory in the current user's home directory named <code>invokeai</code>.</li> </ol>"},{"location":"features/CONFIGURATION/#invokeai-configuration-file","title":"InvokeAI Configuration File","text":"<p>Inside the root directory, we read settings from the <code>invokeai.yaml</code> file.</p> <p>It has two sections - one for internal use and one for user settings:</p> <pre><code># Internal metadata - do not edit:\nschema_version: 4\n\n# Put user settings here - see https://invoke-ai.github.io/InvokeAI/features/CONFIGURATION/:\nhost: 0.0.0.0 # serve the app on your local network\nmodels_dir: D:\\invokeai\\models # store models on an external drive\nprecision: float16 # always use fp16 precision\n</code></pre> <p>The settings in this file will override the defaults. You only need to change this file if the default for a particular setting doesn't work for you.</p> <p>You'll find an example file next to <code>invokeai.yaml</code> that shows the default values.</p> <p>Some settings, like Model Marketplace API Keys, require the YAML to be formatted correctly. Here is a basic guide to YAML files.</p>"},{"location":"features/CONFIGURATION/#custom-config-file-location","title":"Custom Config File Location","text":"<p>You can use any config file with the <code>--config</code> CLI arg. Pass in the path to the <code>invokeai.yaml</code> file you want to use.</p> <p>Note that environment variables will trump any settings in the config file.</p>"},{"location":"features/CONFIGURATION/#environment-variables","title":"Environment Variables","text":"<p>All settings may be set via environment variables by prefixing <code>INVOKEAI_</code> to the variable name. For example, <code>INVOKEAI_HOST</code> would set the <code>host</code> setting.</p> <p>For non-primitive values, pass a JSON-encoded string:</p> <pre><code>export INVOKEAI_REMOTE_API_TOKENS='[{\"url_regex\":\"modelmarketplace\", \"token\": \"12345\"}]'\n</code></pre> <p>We suggest using <code>invokeai.yaml</code>, as it is more user-friendly.</p>"},{"location":"features/CONFIGURATION/#cli-args","title":"CLI Args","text":"<p>A subset of settings may be specified using CLI args:</p> <ul> <li><code>--root</code>: specify the root directory</li> <li><code>--config</code>: override the default <code>invokeai.yaml</code> file location</li> </ul>"},{"location":"features/CONFIGURATION/#all-settings","title":"All Settings","text":"<p>Following the table are additional explanations for certain settings.</p>"},{"location":"features/CONFIGURATION/#invokeai.app.services.config.config_default.InvokeAIAppConfig","title":"InvokeAIAppConfig","text":"<p>Attributes:</p> Name Type Description <code>host</code> <code>str</code> <p>IP address to bind to. Use <code>0.0.0.0</code> to serve to your local network.</p> <code>port</code> <code>int</code> <p>Port to bind to.</p> <code>allow_origins</code> <code>list[str]</code> <p>Allowed CORS origins.</p> <code>allow_credentials</code> <code>bool</code> <p>Allow CORS credentials.</p> <code>allow_methods</code> <code>list[str]</code> <p>Methods allowed for CORS.</p> <code>allow_headers</code> <code>list[str]</code> <p>Headers allowed for CORS.</p> <code>ssl_certfile</code> <code>Optional[Path]</code> <p>SSL certificate file for HTTPS. See https://www.uvicorn.org/settings/#https.</p> <code>ssl_keyfile</code> <code>Optional[Path]</code> <p>SSL key file for HTTPS. See https://www.uvicorn.org/settings/#https.</p> <code>log_tokenization</code> <code>bool</code> <p>Enable logging of parsed prompt tokens.</p> <code>patchmatch</code> <code>bool</code> <p>Enable patchmatch inpaint code.</p> <code>models_dir</code> <code>Path</code> <p>Path to the models directory.</p> <code>convert_cache_dir</code> <code>Path</code> <p>Path to the converted models cache directory (DEPRECATED, but do not delete because it is needed for migration from previous versions).</p> <code>download_cache_dir</code> <code>Path</code> <p>Path to the directory that contains dynamically downloaded models.</p> <code>legacy_conf_dir</code> <code>Path</code> <p>Path to directory of legacy checkpoint config files.</p> <code>db_dir</code> <code>Path</code> <p>Path to InvokeAI databases directory.</p> <code>outputs_dir</code> <code>Path</code> <p>Path to directory for outputs.</p> <code>custom_nodes_dir</code> <code>Path</code> <p>Path to directory for custom nodes.</p> <code>log_handlers</code> <code>list[str]</code> <p>Log handler. Valid options are \"console\", \"file=\", \"syslog=path|address:host:port\", \"http=\". <code>log_format</code> <code>LOG_FORMAT</code> <p>Log format. Use \"plain\" for text-only, \"color\" for colorized output, \"legacy\" for 2.3-style logging and \"syslog\" for syslog-style.Valid values: <code>plain</code>, <code>color</code>, <code>syslog</code>, <code>legacy</code></p> <code>log_level</code> <code>LOG_LEVEL</code> <p>Emit logging messages at this level or higher.Valid values: <code>debug</code>, <code>info</code>, <code>warning</code>, <code>error</code>, <code>critical</code></p> <code>log_sql</code> <code>bool</code> <p>Log SQL queries. <code>log_level</code> must be <code>debug</code> for this to do anything. Extremely verbose.</p> <code>use_memory_db</code> <code>bool</code> <p>Use in-memory database. Useful for development.</p> <code>dev_reload</code> <code>bool</code> <p>Automatically reload when Python sources are changed. Does not reload node definitions.</p> <code>profile_graphs</code> <code>bool</code> <p>Enable graph profiling using <code>cProfile</code>.</p> <code>profile_prefix</code> <code>Optional[str]</code> <p>An optional prefix for profile output files.</p> <code>profiles_dir</code> <code>Path</code> <p>Path to profiles output directory.</p> <code>ram</code> <code>float</code> <p>Maximum memory amount used by memory model cache for rapid switching (GB).</p> <code>vram</code> <code>float</code> <p>Amount of VRAM reserved for model storage (GB).</p> <code>lazy_offload</code> <code>bool</code> <p>Keep models in VRAM until their space is needed.</p> <code>log_memory_usage</code> <code>bool</code> <p>If True, a memory snapshot will be captured before and after every model cache operation, and the result will be logged (at debug level). There is a time cost to capturing the memory snapshots, so it is recommended to only enable this feature if you are actively inspecting the model cache's behaviour.</p> <code>devices</code> <code>Optional[list[DEVICE]]</code> <p>List of execution devices for rendering. Default will choose all available devices.</p> <code>precision</code> <code>PRECISION</code> <p>Floating point precision. <code>float16</code> will consume half the memory of <code>float32</code> but produce slightly lower-quality images. The <code>auto</code> setting will guess the proper precision based on your video card and operating system.Valid values: <code>auto</code>, <code>float16</code>, <code>bfloat16</code>, <code>float32</code></p> <code>sequential_guidance</code> <code>bool</code> <p>Whether to calculate guidance in serial instead of in parallel, lowering memory requirements.</p> <code>attention_type</code> <code>ATTENTION_TYPE</code> <p>Attention type.Valid values: <code>auto</code>, <code>normal</code>, <code>xformers</code>, <code>sliced</code>, <code>torch-sdp</code></p> <code>attention_slice_size</code> <code>ATTENTION_SLICE_SIZE</code> <p>Slice size, valid when attention_type==\"sliced\".Valid values: <code>auto</code>, <code>balanced</code>, <code>max</code>, <code>1</code>, <code>2</code>, <code>3</code>, <code>4</code>, <code>5</code>, <code>6</code>, <code>7</code>, <code>8</code></p> <code>force_tiled_decode</code> <code>bool</code> <p>Whether to enable tiled VAE decode (reduces memory consumption with some performance penalty).</p> <code>pil_compress_level</code> <code>int</code> <p>The compress_level setting of PIL.Image.save(), used for PNG encoding. All settings are lossless. 0 = no compression, 1 = fastest with slightly larger filesize, 9 = slowest with smallest filesize. 1 is typically the best setting.</p> <code>max_queue_size</code> <code>int</code> <p>Maximum number of items in the session queue.</p> <code>clear_queue_on_startup</code> <code>bool</code> <p>Empties session queue on startup.</p> <code>allow_nodes</code> <code>Optional[list[str]]</code> <p>List of nodes to allow. Omit to allow all.</p> <code>deny_nodes</code> <code>Optional[list[str]]</code> <p>List of nodes to deny. Omit to deny none.</p> <code>node_cache_size</code> <code>int</code> <p>How many cached nodes to keep in memory.</p> <code>hashing_algorithm</code> <code>HASHING_ALGORITHMS</code> <p>Model hashing algorthim for model installs. 'blake3_multi' is best for SSDs. 'blake3_single' is best for spinning disk HDDs. 'random' disables hashing, instead assigning a UUID to models. Useful when using a memory db to reduce model installation time, or if you don't care about storing stable hashes for models. Alternatively, any other hashlib algorithm is accepted, though these are not nearly as performant as blake3.Valid values: <code>blake3_multi</code>, <code>blake3_single</code>, <code>random</code>, <code>md5</code>, <code>sha1</code>, <code>sha224</code>, <code>sha256</code>, <code>sha384</code>, <code>sha512</code>, <code>blake2b</code>, <code>blake2s</code>, <code>sha3_224</code>, <code>sha3_256</code>, <code>sha3_384</code>, <code>sha3_512</code>, <code>shake_128</code>, <code>shake_256</code></p> <code>remote_api_tokens</code> <code>Optional[list[URLRegexTokenPair]]</code> <p>List of regular expression and token pairs used when downloading models from URLs. The download URL is tested against the regex, and if it matches, the token is provided in as a Bearer token.</p> <code>scan_models_on_startup</code> <code>bool</code> <p>Scan the models directory on startup, registering orphaned models. This is typically only used in conjunction with <code>use_memory_db</code> for testing purposes.</p>"},{"location":"features/CONFIGURATION/#model-marketplace-api-keys","title":"Model Marketplace API Keys","text":"<p>Some model marketplaces require an API key to download models. You can provide a URL pattern and appropriate token in your <code>invokeai.yaml</code> file to provide that API key.</p> <p>The pattern can be any valid regex (you may need to surround the pattern with quotes):</p> <pre><code>remote_api_tokens:\n  # Any URL containing `models.com` will automatically use `your_models_com_token`\n  - url_regex: models.com\n    token: your_models_com_token\n  # Any URL matching this contrived regex will use `some_other_token`\n  - url_regex: '^[a-z]{3}whatever.*\\.com$'\n    token: some_other_token\n</code></pre> <p>The provided token will be added as a <code>Bearer</code> token to the network requests to download the model files. As far as we know, this works for all model marketplaces that require authorization.</p>"},{"location":"features/CONFIGURATION/#model-hashing","title":"Model Hashing","text":"<p>Models are hashed during installation, providing a stable identifier for models across all platforms. Hashing is a one-time operation.</p> <pre><code>hashing_algorithm: blake3_single # default value\n</code></pre> <p>You might want to change this setting, depending on your system:</p> <ul> <li><code>blake3_single</code> (default): Single-threaded - best for spinning HDDs, still OK for SSDs</li> <li><code>blake3_multi</code>: Parallelized, memory-mapped implementation - best for SSDs, terrible for spinning disks</li> <li><code>random</code>: Skip hashing entirely - fastest but of course no hash</li> </ul> <p>During the first startup after upgrading to v4, all of your models will be hashed. This can take a few minutes.</p> <p>Most common algorithms are supported, like <code>md5</code>, <code>sha256</code>, and <code>sha512</code>. These are typically much, much slower than either of the BLAKE3 variants.</p>"},{"location":"features/CONFIGURATION/#path-settings","title":"Path Settings","text":"<p>These options set the paths of various directories and files used by InvokeAI. Any user-defined paths should be absolute paths.</p>"},{"location":"features/CONFIGURATION/#logging","title":"Logging","text":"<p>Several different log handler destinations are available, and multiple destinations are supported by providing a list:</p> <pre><code>log_handlers:\n  - console\n  - syslog=localhost\n  - file=/var/log/invokeai.log\n</code></pre> <ul> <li> <p><code>console</code> is the default. It prints log messages to the command-line window from which InvokeAI was launched.</p> </li> <li> <p><code>syslog</code> is only available on Linux and Macintosh systems. It uses   the operating system's \"syslog\" facility to write log file entries   locally or to a remote logging machine. <code>syslog</code> offers a variety   of configuration options:</p> </li> </ul> <pre><code>  syslog=/dev/log`      - log to the /dev/log device\n  syslog=localhost`     - log to the network logger running on the local machine\n  syslog=localhost:512` - same as above, but using a non-standard port\n  syslog=fredserver,facility=LOG_USER,socktype=SOCK_DRAM`\n                        - Log to LAN-connected server \"fredserver\" using the facility LOG_USER and datagram packets.\n</code></pre> <ul> <li><code>http</code> can be used to log to a remote web server. The server must be   properly configured to receive and act on log messages. The option   accepts the URL to the web server, and a <code>method</code> argument   indicating whether the message should be submitted using the GET or   POST method.</li> </ul> <pre><code> http=http://my.server/path/to/logger,method=POST\n</code></pre> <p>The <code>log_format</code> option provides several alternative formats:</p> <ul> <li><code>color</code> - default format providing time, date and a message, using text colors to distinguish different log severities</li> <li><code>plain</code> - same as above, but monochrome text only</li> <li><code>syslog</code> - the log level and error message only, allowing the syslog system to attach the time and date</li> <li><code>legacy</code> - a format similar to the one used by the legacy 2.3 InvokeAI releases.</li> </ul>"},{"location":"features/CONTROLNET/","title":"Control Adapters","text":""},{"location":"features/CONTROLNET/#controlnet","title":"ControlNet","text":"<p>ControlNet is a powerful set of features developed by the open-source community (notably, Stanford researcher @ilyasviel) that allows you to apply a secondary neural network model to your image generation process in Invoke.</p> <p>With ControlNet, you can get more control over the output of your image generation, providing you with a way to direct the network towards generating images that better fit your desired style or outcome.</p> <p>ControlNet works by analyzing an input image, pre-processing that image to identify relevant information that can be interpreted by each specific ControlNet model, and then inserting that control information into the generation process. This can be used to adjust the style, composition, or other aspects of the image to better achieve a specific result.</p>"},{"location":"features/CONTROLNET/#installation","title":"Installation","text":"<p>InvokeAI provides access to a series of ControlNet models that provide different effects or styles in your generated images.</p> <p>To install ControlNet Models:</p> <ol> <li>The easiest way to install them is to use the InvokeAI model installer application. Use the <code>invoke.sh</code>/<code>invoke.bat</code> launcher to select item [4] and then navigate to the CONTROLNETS section. Select the models you wish to install and press \"APPLY CHANGES\". You may also enter additional HuggingFace repo_ids in the \"Additional models\" textbox. </li> <li>Using the \"Add Model\" function of the  model manager, enter the HuggingFace Repo ID of the ControlNet. The ID is in the format \"author/repoName\"</li> </ol> <p>Be aware that some ControlNet models require additional code functionality in order to work properly, so just installing a third-party ControlNet model may not have the desired effect. Please read and follow the documentation for installing a third party model not currently included among InvokeAI's default list.</p> <p>Currently InvokeAI only supports \ud83e\udd17 Diffusers-format ControlNet models. These are folders that contain the files <code>config.json</code> and/or <code>diffusion_pytorch_model.safetensors</code> and <code>diffusion_pytorch_model.fp16.safetensors</code>. The name of the folder is the name of the model.</p> <p>\ud83e\udd17 Diffusers-format ControlNet models are available at HuggingFace (http://huggingface.co) and accessed via their repo IDs (identifiers in the format \"author/modelname\").</p>"},{"location":"features/CONTROLNET/#controlnet-models","title":"ControlNet Models","text":"<p>The models currently supported include:</p> <p>Canny:</p> <p>When the Canny model is used in ControlNet, Invoke will attempt to generate images that match the edges detected. </p> <p>Canny edge detection works by detecting the edges in an image by looking for abrupt changes in intensity. It is known for its ability to detect edges accurately while reducing noise and false edges, and the preprocessor can identify more information by decreasing the thresholds.</p> <p>M-LSD: </p> <p>M-LSD is another edge detection algorithm used in ControlNet. It stands for Multi-Scale Line Segment Detector. </p> <p>It detects straight line segments in an image by analyzing the local structure of the image at multiple scales.  It can be useful for architectural imagery, or anything where straight-line structural information is needed for the resulting output. </p> <p>Lineart: </p> <p>The Lineart model in ControlNet generates line drawings from an input image. The resulting pre-processed image is a simplified version of the original, with only the outlines of objects visible.The Lineart model in ControlNet is known for its ability to accurately capture the contours of the objects in an input sketch. </p> <p>Lineart Anime: </p> <p>A variant of the Lineart model that generates line drawings with a distinct style inspired by anime and manga art styles.</p> <p>Depth:  A model that generates depth maps of images, allowing you to create more realistic 3D models or to simulate depth effects in post-processing.</p> <p>Normal Map (BAE):  A model that generates normal maps from input images, allowing for more realistic lighting effects in 3D rendering.</p> <p>Image Segmentation:  A model that divides input images into segments or regions, each of which corresponds to a different object or part of the image. (More details coming soon)</p> <p>QR Code Monster: A model that helps generate creative QR codes that still scan. Can also be used to create images with text, logos or shapes within them. </p> <p>Openpose:  The OpenPose control model allows for the identification of the general pose of a character by pre-processing an existing image with a clear human structure. With advanced options, Openpose can also detect the face or hands in the image. </p> <p>Note: The DWPose Processor has replaced the OpenPose processor in Invoke. Workflows and generations that relied on the OpenPose Processor will need to be updated to use the DWPose Processor instead.</p> <p>Mediapipe Face:</p> <p>The MediaPipe Face identification processor is able to clearly identify facial features in order to capture vivid expressions of human faces.</p> <p>Tile:</p> <p>The Tile model fills out details in the image to match the image, rather than the prompt. The Tile Model is a versatile tool that offers a range of functionalities. Its primary capabilities can be boiled down to two main behaviors:</p> <ul> <li>It can reinterpret specific details within an image and create fresh, new elements.</li> <li>It has the ability to disregard global instructions if there's a discrepancy between them and the local context or specific parts of the image. In such cases, it uses the local context to guide the process.</li> </ul> <p>The Tile Model can be a powerful tool in your arsenal for enhancing image quality and details. If there are undesirable elements in your images, such as blurriness caused by resizing, this model can effectively eliminate these issues, resulting in cleaner, crisper images. Moreover, it can generate and add refined details to your images, improving their overall quality and appeal. </p> <p>Pix2Pix (experimental)</p> <p>With Pix2Pix, you can input an image into the controlnet, and then \"instruct\" the model to change it using your prompt. For example, you can say \"Make it winter\" to add more wintry elements to a scene.</p> <p>Each of these models can be adjusted and combined with other ControlNet models to achieve different results, giving you even more control over your image generation process.</p>"},{"location":"features/CONTROLNET/#using-controlnet","title":"Using ControlNet","text":"<p>To use ControlNet, you can simply select the desired model and adjust both the ControlNet and Pre-processor settings to achieve the desired result. You can also use multiple ControlNet models at the same time, allowing you to achieve even more complex effects or styles in your generated images.</p> <p>Each ControlNet has two settings that are applied to the ControlNet.</p> <p>Weight - Strength of the Controlnet model applied to the generation for the section, defined by start/end.</p> <p>Start/End  - 0 represents the start of the generation, 1 represents the end. The Start/end setting controls what steps during the generation process have the ControlNet applied.</p> <p>Additionally, each ControlNet section can be expanded in order to manipulate settings for the image pre-processor that adjusts your uploaded image before using it in when you Invoke.</p>"},{"location":"features/CONTROLNET/#t2i-adapter","title":"T2I-Adapter","text":"<p>T2I-Adapter is a tool similar to ControlNet that allows for control over the generation process by providing control information during the generation process. T2I-Adapter models tend to be smaller and more efficient than ControlNets. </p>"},{"location":"features/CONTROLNET/#installation_1","title":"Installation","text":"<p>To install T2I-Adapter Models:</p> <ol> <li>The easiest way to install models is to use the InvokeAI model installer application. Use the <code>invoke.sh</code>/<code>invoke.bat</code> launcher to select item [5] and then navigate to the T2I-Adapters section. Select the models you wish to install and press \"APPLY CHANGES\". You may also enter additional HuggingFace repo_ids in the \"Additional models\" textbox. </li> <li>Using the \"Add Model\" function of the  model manager, enter the HuggingFace Repo ID of the T2I-Adapter. The ID is in the format \"author/repoName\"</li> </ol>"},{"location":"features/CONTROLNET/#usage","title":"Usage","text":"<p>Each T2I Adapter has two settings that are applied.</p> <p>Weight - Strength of the model applied to the generation for the section, defined by start/end.</p> <p>Start/End  - 0 represents the start of the generation, 1 represents the end. The Start/end setting controls what steps during the generation process have the ControlNet applied.</p> <p>Additionally, each  section can be expanded with the \"Show Advanced\" button in order to manipulate settings for the image pre-processor that adjusts your uploaded image before using it in during the generation process.</p>"},{"location":"features/CONTROLNET/#ip-adapter","title":"IP-Adapter","text":"<p>IP-Adapter is a tooling that allows for image prompt capabilities with text-to-image diffusion models. IP-Adapter works by analyzing the given image prompt to extract features, then passing those features to the UNet along with any other conditioning provided. </p> <p></p> <p></p>"},{"location":"features/CONTROLNET/#installation_2","title":"Installation","text":"<p>There are several ways to install IP-Adapter models with an existing InvokeAI installation:</p> <ol> <li>Through the command line interface launched from the invoke.sh / invoke.bat scripts, option [4] to download models.</li> <li>Through the Model Manager UI with models from the Tools section of models.invoke.ai. To do this, copy the repo ID from the desired model page, and paste it in the Add Model field of the model manager. Note Both the IP-Adapter and the Image Encoder must be installed for IP-Adapter to work. For example, the SD 1.5 IP-Adapter and SD1.5 Image Encoder must be installed to use IP-Adapter with SD1.5 based models.  </li> <li>**Advanced -- Not recommended ** Manually downloading the IP-Adapter and Image Encoder files - Image Encoder folders shouid be placed in the <code>models\\any\\clip_vision</code> folders. IP Adapter Model folders should be placed in the relevant <code>ip-adapter</code> folder of relevant base model folder of Invoke root directory. For example, for the SDXL IP-Adapter, files should be added to the <code>model/sdxl/ip_adapter/</code> folder. </li> </ol>"},{"location":"features/CONTROLNET/#using-ip-adapter","title":"Using IP-Adapter","text":"<p>IP-Adapter can be used by navigating to the Control Adapters options and enabling IP-Adapter. </p> <p>IP-Adapter requires an image to be used as the Image Prompt. It can also be used in conjunction with text prompts, Image-to-Image, Inpainting, Outpainting, ControlNets and LoRAs.</p> <p>Each IP-Adapter has two settings that are applied to the IP-Adapter:</p> <ul> <li>Weight - Strength of the IP-Adapter model applied to the generation for the section, defined by start/end</li> <li>Start/End  - 0 represents the start of the generation, 1 represents the end. The Start/end setting controls what steps during the generation process have the IP-Adapter applied.</li> </ul>"},{"location":"features/DATABASE/","title":"Invoke's SQLite Database","text":"<p>Invoke uses a SQLite database to store image, workflow, model, and execution data.</p> <p>We take great care to ensure your data is safe, by utilizing transactions and a database migration system.</p> <p>Even so, when testing an prerelease version of the app, we strongly suggest either backing up your database or using an in-memory database. This ensures any prelease hiccups or databases schema changes will not cause problems for your data.</p>"},{"location":"features/DATABASE/#database-backup","title":"Database Backup","text":"<p>Backing up your database is very simple. Invoke's data is stored in an <code>$INVOKEAI_ROOT</code> directory - where your <code>invoke.sh</code>/<code>invoke.bat</code> and <code>invokeai.yaml</code> files live.</p> <p>To back up your database, copy the <code>invokeai.db</code> file from <code>$INVOKEAI_ROOT/databases/invokeai.db</code> to somewhere safe.</p> <p>If anything comes up during prelease testing, you can simply copy your backup back into <code>$INVOKEAI_ROOT/databases/</code>.</p>"},{"location":"features/DATABASE/#in-memory-database","title":"In-Memory Database","text":"<p>SQLite can run on an in-memory database. Your existing database is untouched when this mode is enabled, but your existing data won't be accessible.</p> <p>This is very useful for testing, as there is no chance of a database change modifying your \"physical\" database.</p> <p>To run Invoke with a memory database, edit your <code>invokeai.yaml</code> file, and add <code>use_memory_db: true</code> to the <code>Paths:</code> stanza:</p> <pre><code>InvokeAI:\n  Development:\n    use_memory_db: true\n</code></pre> <p>Delete this line (or set it to <code>false</code>) to use your main database.</p>"},{"location":"features/GALLERY/","title":"InvokeAI Gallery Panel","text":""},{"location":"features/GALLERY/#quick-guided-walkthrough-of-the-gallery-panels-features","title":"Quick guided walkthrough of the Gallery Panel's features","text":"<p>The Gallery Panel is a fast way to review, find, and make use of images you've generated and loaded. The Gallery is divided into Boards. The Uncategorized board is always  present but you can create your own for better organization.</p> <p></p>"},{"location":"features/GALLERY/#board-display-and-settings","title":"Board Display and Settings","text":"<p>At the very top of the Gallery Panel are the boards disclosure and settings buttons.</p> <p></p> <p>The disclosure button shows the name of the currently selected board and allows you to show and hide the board thumbnails (shown in the image below).</p> <p></p> <p>The settings button opens a list of options.</p> <p></p> <ul> <li>Image Size this slider lets you control the size of the image previews (images of three different sizes).</li> <li>Auto-Switch to New Images if you turn this on, whenever a new image is generated, it will automatically be loaded into the current image panel on the Text to Image tab and into the result panel on the Image to Image tab. This will happen invisibly if you are on any other tab when the image is generated.</li> <li>Auto-Assign Board on Click whenever an image is generated or saved, it always gets put in a board. The board it gets put into is marked with AUTO (image of board marked). Turning on Auto-Assign Board on Click will make whichever board you last selected be the destination when you click Invoke. That means you can click Invoke, select a different board, and then click Invoke again and the two images will be put in two different boards. (bold)It's the board selected when Invoke is clicked that's used, not the board that's selected when the image is finished generating.(bold) Turning this off, enables the Auto-Add Board drop down which lets you set one specific board to always put generated images into. This also enables and disables the Auto-add to this Board menu item described below.</li> <li>Always Show Image Size Badge this toggles whether to show image sizes for each image preview (show two images, one with sizes shown, one without)</li> </ul> <p>Below these two buttons, you'll see the Search Boards text entry area. You use this to search for specific boards by the name of the board. Next to it is the Add Board (+) button which lets you add new boards. Boards can be renamed by clicking on the name of the board under its thumbnail and typing in the new name.</p>"},{"location":"features/GALLERY/#board-thumbnail-menu","title":"Board Thumbnail Menu","text":"<p>Each board has a context menu (ctrl+click / right-click).</p> <p></p> <ul> <li>Auto-add to this Board if you've disabled Auto-Assign Board on Click in the board settings, you can use this option to set this board to be where new images are put.</li> <li>Download Board this will add all the images in the board into a zip file and provide a link to it in a notification (image of notification)</li> <li>Delete Board this will delete the board <p>[!CAUTION] This will delete all the images in the board and the board itself.</p> </li> </ul>"},{"location":"features/GALLERY/#board-contents","title":"Board Contents","text":"<p>Every board is organized by two tabs, Images and Assets.</p> <p></p> <p>Images are the Invoke-generated images that are placed into the board. Assets are images that you upload into Invoke to be used as an Image Prompt or in the Image to Image tab.</p>"},{"location":"features/GALLERY/#image-thumbnail-menu","title":"Image Thumbnail Menu","text":"<p>Every image generated by Invoke has its generation information stored as text inside the image file itself. This can be read directly by selecting the image and clicking on the Info button  in any of the image result panels. </p> <p>Each image also has a context menu (ctrl+click / right-click).</p> <p></p> <p>The options are (items marked with an * will not work with images that lack generation information): - Open in New Tab this will open the image alone in a new browser tab, separate from the Invoke interface. - Download Image this will trigger your browser to download the image. - Load Workflow **** this will load any workflow settings into the Workflow tab and automatically open it. - ***Remix Image **** this will load all of the image's generation information, (bold)excluding its Seed, into the left hand control panel - ***Use Prompt **** this will load only the image's text prompts into the left-hand control panel - ***Use Seed **** this will load only the image's Seed into the left-hand control panel - ***Use All **** this will load all of the image's generation information into the left-hand control panel - ***Send to Image to Image this will put the image into the left-hand panel in the Image to Image tab ana automatically open it - Send to Unified Canvas This will (bold)replace whatever is already present(bold) in the Unified Canvas tab with the image and automatically open the tab - Change Board this will oipen a small window that will let you move the image to a different board. This is the same as dragging the image to that board's thumbnail. - Star Image this will add the image to the board's list of starred images that are always kept at the top of the gallery. This is the same as clicking on the star on the top right-hand side of the image that appears when you hover over the image with the mouse - Delete Image this will delete the image from the board</p> <p>[!CAUTION]  This will delete the image entirely from Invoke.</p>"},{"location":"features/GALLERY/#summary","title":"Summary","text":"<p>This walkthrough only covers the Gallery interface and Boards. Actually generating images is handled by Prompts, the Image to Image tab, and the Unified Canvas.</p>"},{"location":"features/GALLERY/#acknowledgements","title":"Acknowledgements","text":"<p>A huge shout-out to the core team working to make the Web GUI a reality, including psychedelicious, Kyle0654 and blessedcoolant. hipsterusername was the team's unofficial cheerleader and added tooltips/docs.</p>"},{"location":"features/IMG2IMG/","title":"Image-to-Image","text":"<p>InvokeAI provides an \"img2img\" feature that lets you seed your creations with an initial drawing or photo. This is a really cool feature that tells stable diffusion to build the prompt on top of the image you provide, preserving the original's basic shape and layout.</p> <p>For a walkthrough of using Image-to-Image in the Web UI, see InvokeAI Web Server.</p> <p>The main difference between <code>img2img</code> and <code>prompt2img</code> is the starting point. While <code>prompt2img</code> always starts with pure gaussian noise and progressively refines it over the requested number of steps, <code>img2img</code> skips some of these earlier steps (how many it skips is indirectly controlled by the <code>--strength</code> parameter), and uses instead your initial image mixed with gaussian noise as the starting image.</p> <p>Let's start by thinking about vanilla <code>prompt2img</code>, just generating an image from a prompt. If the step count is 10, then the \"latent space\" (Stable Diffusion's internal representation of the image) for the prompt \"fire\" with seed <code>1592514025</code> develops something like this:</p> <p> </p> <p>Put simply: starting from a frame of fuzz/static, SD finds details in each frame that it thinks look like \"fire\" and brings them a little bit more into focus, gradually scrubbing out the fuzz until a clear image remains.</p> <p>When you use <code>img2img</code> some of the earlier steps are cut, and instead an initial image of your choice is used. But because of how the maths behind Stable Diffusion works, this image needs to be mixed with just the right amount of noise (fuzz/static) for where it is being inserted. This is where the strength parameter comes in. Depending on the set strength, your image will be inserted into the sequence at the appropriate point, with just the right amount of noise.</p>"},{"location":"features/IMG2IMG/#a-concrete-example","title":"A concrete example","text":"<p>I want SD to draw a fire based on this hand-drawn image</p> <p></p> <p>Let's only do 10 steps, to make it easier to see what's happening. If strength is <code>0.7</code>, this is what the internal steps the algorithm has to take will look like:</p> <p> </p> <p>With strength <code>0.4</code>, the steps look more like this:</p> <p> </p> <p>Notice how much more fuzzy the starting image is for strength <code>0.7</code> compared to <code>0.4</code>, and notice also how much longer the sequence is with <code>0.7</code>:</p> strength = 0.7 strength = 0.4 initial image that SD sees steps argument to <code>invoke&gt;</code> <code>-S10</code> <code>-S10</code> steps actually taken <code>7</code> <code>4</code> latent space at each step output <p>Both of the outputs look kind of like what I was thinking of. With the strength higher, my input becomes more vague, and Stable Diffusion has more steps to refine its output. But it's not really making what I want, which is a picture of cheery open fire. With the strength lower, my input is more clear, but Stable Diffusion has less chance to refine itself, so the result ends up inheriting all the problems of my bad drawing.</p> <p>If you want to try this out yourself, all of these are using a seed of <code>1592514025</code> with a width/height of <code>384</code>, step count <code>10</code>, the <code>k_lms</code> sampler, and the single-word prompt <code>\"fire\"</code>.</p>"},{"location":"features/IMG2IMG/#compensating-for-the-reduced-step-count","title":"Compensating for the reduced step count","text":"<p>After putting this guide together I was curious to see how the difference would be if I increased the step count to compensate, so that SD could have the same amount of steps to develop the image regardless of the strength. So I ran the generation again using the same seed, but this time adapting the step count to give each generation 20 steps.</p> <p>Here's strength <code>0.4</code> (note step count <code>50</code>, which is <code>20 \u00f7 0.4</code> to make sure SD does <code>20</code> steps from my image):</p> <p></p> <p>and here is strength <code>0.7</code> (note step count <code>30</code>, which is roughly <code>20 \u00f7 0.7</code> to make sure SD does <code>20</code> steps from my image):</p> <p></p> <p>In both cases the image is nice and clean and \"finished\", but because at strength <code>0.7</code> Stable Diffusion has been give so much more freedom to improve on my badly-drawn flames, they've come out looking much better. You can really see the difference when looking at the latent steps. There's more noise on the first image with strength <code>0.7</code>:</p> <p></p> <p>than there is for strength <code>0.4</code>:</p> <p></p> <p>and that extra noise gives the algorithm more choices when it is evaluating how to denoise any particular pixel in the image.</p> <p>Unfortunately, it seems that <code>img2img</code> is very sensitive to the step count. Here's strength <code>0.7</code> with a step count of <code>29</code> (SD did 19 steps from my image):</p> <p></p> <p>By comparing the latents we can sort of see that something got interpreted differently enough on the third or fourth step to lead to a rather different interpretation of the flames.</p> <p></p> <p></p> <p>This is the result of a difference in the de-noising \"schedule\" - basically the noise has to be cleaned by a certain degree each step or the model won't \"converge\" on the image properly (see stable diffusion blog for more about that). A different step count means a different schedule, which means things get interpreted slightly differently at every step.</p>"},{"location":"features/LOGGING/","title":"Controlling Logging","text":""},{"location":"features/LOGGING/#controlling-how-invokeai-logs-status-messages","title":"Controlling How InvokeAI Logs Status Messages","text":"<p>InvokeAI logs status messages using a configurable logging system. You can log to the terminal window, to a designated file on the local machine, to the syslog facility on a Linux or Mac, or to a properly configured web server. You can configure several logs at the same time, and control the level of message logged and the logging format (to a limited extent).</p> <p>Three command-line options control logging:</p>"},{"location":"features/LOGGING/#-log_handlers-handler1-handler2","title":"<code>--log_handlers &lt;handler1&gt; &lt;handler2&gt; ...</code>","text":"<p>This option activates one or more log handlers. Options are \"console\", \"file\", \"syslog\" and \"http\". To specify more than one, separate them by spaces:</p> <pre><code>invokeai-web --log_handlers console syslog=/dev/log file=C:\\Users\\fred\\invokeai.log\n</code></pre> <p>The format of these options is described below.</p>"},{"location":"features/LOGGING/#-log_format-plaincolorlegacysyslog","title":"<code>--log_format {plain|color|legacy|syslog}</code>","text":"<p>This controls the format of log messages written to the console. Only the \"console\" log handler is currently affected by this setting.</p> <ul> <li>\"plain\" provides formatted messages like this:</li> </ul> <pre><code>[2023-05-24 23:18:2[2023-05-24 23:18:50,352]::[InvokeAI]::DEBUG --&gt; this is a debug message\n[2023-05-24 23:18:50,352]::[InvokeAI]::INFO --&gt; this is an informational messages\n[2023-05-24 23:18:50,352]::[InvokeAI]::WARNING --&gt; this is a warning\n[2023-05-24 23:18:50,352]::[InvokeAI]::ERROR --&gt; this is an error\n[2023-05-24 23:18:50,352]::[InvokeAI]::CRITICAL --&gt; this is a critical error\n</code></pre> <ul> <li> <p>\"color\" produces similar output, but the text will be color coded to indicate the severity of the message.</p> </li> <li> <p>\"legacy\" produces output similar to InvokeAI versions 2.3 and earlier:</p> </li> </ul> <pre><code>### this is a critical error\n*** this is an error\n** this is a warning\n&gt;&gt; this is an informational messages\n   | this is a debug message\n</code></pre> <ul> <li>\"syslog\" produces messages suitable for syslog entries:</li> </ul> <pre><code>InvokeAI [2691178] &lt;CRITICAL&gt; this is a critical error\nInvokeAI [2691178] &lt;ERROR&gt; this is an error\nInvokeAI [2691178] &lt;WARNING&gt; this is a warning\nInvokeAI [2691178] &lt;INFO&gt; this is an informational messages\nInvokeAI [2691178] &lt;DEBUG&gt; this is a debug message\n</code></pre> <p>(note that the date, time and hostname will be added by the syslog system)</p>"},{"location":"features/LOGGING/#-log_level-debuginfowarningerrorcritical","title":"<code>--log_level {debug|info|warning|error|critical}</code>","text":"<p>Providing this command-line option will cause only messages at the specified level or above to be emitted.</p>"},{"location":"features/LOGGING/#console-logging","title":"Console logging","text":"<p>When \"console\" is provided to <code>--log_handlers</code>, messages will be written to the command line window in which InvokeAI was launched. By default, the color formatter will be used unless overridden by <code>--log_format</code>.</p>"},{"location":"features/LOGGING/#file-logging","title":"File logging","text":"<p>When \"file\" is provided to <code>--log_handlers</code>, entries will be written to the file indicated in the path argument. By default, the \"plain\" format will be used:</p> <pre><code>invokeai-web --log_handlers file=/var/log/invokeai.log\n</code></pre>"},{"location":"features/LOGGING/#syslog-logging","title":"Syslog logging","text":"<p>When \"syslog\" is requested, entries will be sent to the syslog system. There are a variety of ways to control where the log message is sent:</p> <ul> <li>Send to the local machine using the <code>/dev/log</code> socket:</li> </ul> <pre><code>invokeai-web --log_handlers syslog=/dev/log\n</code></pre> <ul> <li>Send to the local machine using a UDP message:</li> </ul> <pre><code>invokeai-web --log_handlers syslog=localhost\n</code></pre> <ul> <li>Send to the local machine using a UDP message on a nonstandard   port:</li> </ul> <pre><code>invokeai-web --log_handlers syslog=localhost:512\n</code></pre> <ul> <li>Send to a remote machine named \"loghost\" on the local LAN using   facility LOG_USER and UDP packets:</li> </ul> <pre><code>invokeai-web --log_handlers syslog=loghost,facility=LOG_USER,socktype=SOCK_DGRAM\n</code></pre> <p>This can be abbreviated <code>syslog=loghost</code>, as LOG_USER and SOCK_DGRAM are defaults.</p> <ul> <li>Send to a remote machine named \"loghost\" using the facility LOCAL0   and using a TCP socket:</li> </ul> <pre><code>invokeai-web --log_handlers syslog=loghost,facility=LOG_LOCAL0,socktype=SOCK_STREAM\n</code></pre> <p>If no arguments are specified (just a bare \"syslog\"), then the logging system will look for a UNIX socket named <code>/dev/log</code>, and if not found try to send a UDP message to <code>localhost</code>. The Macintosh OS used to support logging to a socket named <code>/var/run/syslog</code>, but this feature has since been disabled.</p>"},{"location":"features/LOGGING/#web-logging","title":"Web logging","text":"<p>If you have access to a web server that is configured to log messages when a particular URL is requested, you can log using the \"http\" method:</p> <pre><code>invokeai-web --log_handlers http=http://my.server/path/to/logger,method=POST\n</code></pre> <p>The optional [,method=] part can be used to specify whether the URL accepts GET (default) or POST messages.</p> <p>Currently password authentication and SSL are not supported.</p>"},{"location":"features/LOGGING/#using-the-configuration-file","title":"Using the configuration file","text":"<p>You can set and forget logging options by adding a \"Logging\" section to <code>invokeai.yaml</code>:</p> <pre><code>InvokeAI:\n  [... other settings...]\n  Logging:\n    log_handlers:\n       - console\n       - syslog=/dev/log\n    log_level: info\n    log_format: color\n</code></pre>"},{"location":"features/LORAS/","title":"LoRAs &amp; LCM-LoRAs","text":"<p>With the advances in research, many new capabilities are available to customize the knowledge and understanding of novel concepts not originally contained in the base model. </p>"},{"location":"features/LORAS/#loras","title":"LoRAs","text":"<p>Low-Rank Adaptation (LoRA) files are models that customize the output of Stable Diffusion image generation.  Larger than embeddings, but much smaller than full models, they augment SD with improved understanding of subjects and artistic styles.</p> <p>Unlike TI files, LoRAs do not introduce novel vocabulary into the model's known tokens. Instead, LoRAs augment the model's weights that are applied to generate imagery. LoRAs may be supplied with a \"trigger\" word that they have been explicitly trained on, or may simply apply their effect without being triggered.</p> <p>LoRAs are typically stored in .safetensors files, which are the most secure way to store and transmit these types of weights.</p> <p>To use these when generating, open the LoRA menu item in the options panel, select the LoRAs you want to apply and ensure that they have the appropriate weight recommended by the model provider. Typically, most LoRAs perform best at a weight of .75-1.</p>"},{"location":"features/LORAS/#lcm-loras","title":"LCM-LoRAs","text":"<p>Latent Consistency Models (LCMs) allowed a reduced number of steps to be used to generate images with Stable Diffusion. These are created by distilling base models, creating models that only require a small number of steps to generate images. However, LCMs require that any fine-tune of a base model be distilled to be used as an LCM. </p> <p>LCM-LoRAs are models that provide the benefit of LCMs but are able to be used as LoRAs and applied to any fine tune of a base model. LCM-LoRAs are created by training a small number of adapters, rather than distilling the entire fine-tuned base model. The resulting LoRA can be used the same way as a standard LoRA, but with a greatly reduced step count. This enables SDXL images to be generated up to 10x faster than without the use of LCM-LoRAs. </p> <p>Using LCM-LoRAs</p> <p>LCM-LoRAs are natively supported in InvokeAI throughout the application. To get started, install any diffusers format LCM-LoRAs using the model manager and select it in the LoRA field.</p> <p>There are a number parameter differences when using LCM-LoRAs and standard generation: </p> <ul> <li>When using LCM-LoRAs, the LoRA strength should be lower than if using a standard LoRA, with 0.35 recommended as a starting point.  </li> <li>The LCM scheduler should be used for generation</li> <li>CFG-Scale should be reduced to ~1</li> <li>Steps should be reduced in the range of 4-8</li> </ul> <p>Standard LoRAs can also be used alongside LCM-LoRAs, but will also require a lower strength, with 0.45 being recommended as a starting point. </p> <p>More information can be found here: https://huggingface.co/blog/lcm_lora#fast-inference-with-sdxl-lcm-loras</p>"},{"location":"features/MODEL_MERGING/","title":"Model Merging","text":"<p>InvokeAI provides the ability to merge two or three diffusers-type models into a new merged model. The resulting model will combine characteristics of the original, and can be used to teach an old model new tricks.</p>"},{"location":"features/MODEL_MERGING/#how-to-merge-models","title":"How to Merge Models","text":"<p>Model Merging can be be done by navigating to the Model Manager and clicking the \"Merge Models\" tab. From there, you can select the models and settings you want to use to merge th models. </p>"},{"location":"features/MODEL_MERGING/#settings","title":"Settings","text":"<ul> <li>Model Selection: there are three multiple choice fields that   display all the diffusers-style models that InvokeAI knows about.   If you do not see the model you are looking for, then it is probably   a legacy checkpoint model and needs to be converted using the   \"Convert\" option in the Web-based Model Manager tab.</li> </ul> <p>You must select at least two models to merge. The third can be left   at \"None\" if you desire.</p> <ul> <li> <p>Alpha: This is the ratio to use when combining models. It ranges   from 0 to 1. The higher the value, the more weight is given to the   2d and (optionally) 3d models. So if you have two models named \"A\"   and \"B\", an alpha value of 0.25 will give you a merged model that is   25% A and 75% B.</p> </li> <li> <p>Interpolation Method: This is the method used to combine   weights. The options are \"weighted_sum\" (the default), \"sigmoid\",   \"inv_sigmoid\" and \"add_difference\". Each produces slightly different   results. When three models are in use, only \"add_difference\" is   available.</p> </li> <li> <p>Save Location: The location you want the merged model to be saved in. Default is in the InvokeAI root folder</p> </li> <li> <p>Name for merged model: This is the name for the new model. Please   use InvokeAI conventions - only alphanumeric letters and the   characters \".+-\".</p> </li> <li> <p>Ignore Mismatches / Force: Not all models are compatible with each other. The merge   script will check for compatibility and refuse to merge ones that   are incompatible. Set this checkbox to try merging anyway.</p> </li> </ul> <p>You may run the merge script by starting the invoke launcher (<code>invoke.sh</code> or <code>invoke.bat</code>) and choosing the option (4) for merge models. This will launch a text-based interactive user interface that prompts you to select the models to merge, how to merge them, and the merged model name.</p> <p>Alternatively you may activate InvokeAI's virtual environment from the command line, and call the script via <code>merge_models --gui</code> to open up a version that has a nice graphical front end. To get the commandline- only version, omit <code>--gui</code>.</p> <p>The user interface for the text-based interactive script is straightforward. It shows you a series of setting fields. Use control-N (^N) to move to the next field, and control-P (^P) to move to the previous one. You can also use TAB and shift-TAB to move forward and backward. Once you are in a multiple choice field, use the up and down cursor arrows to move to your desired selection, and press  or  to select it. Change text fields by typing in them, and adjust scrollbars using the left and right arrow keys. <p>Once you are happy with your settings, press the OK button. Note that there may be two pages of settings, depending on the height of your screen, and the OK button may be on the second page. Advance past the last field of the first page to get to the second page, and reverse this to get back.</p> <p>If the merge runs successfully, it will create a new diffusers model under the selected name and register it with InvokeAI.</p>"},{"location":"features/OTHER/","title":"Others","text":""},{"location":"features/OTHER/#google-colab","title":"Google Colab","text":"<p>Open and follow instructions to use an isolated environment running Dream.</p> <p>Output Example:</p> <p></p>"},{"location":"features/OTHER/#invisible-watermark","title":"Invisible Watermark","text":"<p>In keeping with the principles for responsible AI generation, and to help AI researchers avoid synthetic images contaminating their training sets, InvokeAI adds an invisible watermark to each of the final images it generates. The watermark consists of the text \"InvokeAI\" and can be viewed using the invisible-watermarks tool.</p> <p>Watermarking is controlled using the <code>invisible-watermark</code> setting in <code>invokeai.yaml</code>. To turn it off, add the following line under the <code>Features</code> category.</p> <pre><code>invisible_watermark: false\n</code></pre>"},{"location":"features/OTHER/#weighted-prompts","title":"Weighted Prompts","text":"<p>You may weight different sections of the prompt to tell the sampler to attach different levels of priority to them, by adding <code>:&lt;percent&gt;</code> to the end of the section you wish to up- or downweight. For example consider this prompt:</p> <pre><code>(tabby cat):0.25 (white duck):0.75 hybrid\n</code></pre> <p>This will tell the sampler to invest 25% of its effort on the tabby cat aspect of the image and 75% on the white duck aspect (surprisingly, this example actually works). The prompt weights can use any combination of integers and floating point numbers, and they do not need to add up to 1.</p>"},{"location":"features/POSTPROCESS/","title":"Postprocessing","text":"<p>This sections details the ability to improve faces and upscale images.</p>"},{"location":"features/POSTPROCESS/#face-fixing","title":"Face Fixing","text":"<p>As of InvokeAI 3.0, the easiest way to improve faces created during image generation is through the Inpainting functionality of the Unified Canvas. Simply add the image containing the faces that you would like to improve to the canvas, mask the face to be improved and run the invocation. For best results, make sure to use an inpainting specific model; these are usually identified by the \"-inpainting\" term in the model name. </p>"},{"location":"features/POSTPROCESS/#upscaling","title":"Upscaling","text":"<p>Open the upscaling dialog by clicking on the \"expand\" icon located above the image display area in the Web UI:</p> <p></p> <p>The default upscaling option is Real-ESRGAN x2 Plus, which will scale your image by a factor of two. This means upscaling a 512x512 image will result in a new 1024x1024 image.</p> <p>Other options are the x4 upscalers, which will scale your image by a factor of 4. </p> <p>Note</p> <p>Real-ESRGAN is memory intensive. In order to avoid crashes and memory overloads during the Stable Diffusion process, these effects are applied after Stable Diffusion has completed its work.</p> <p>In single image generations, you will see the output right away but when you are using multiple iterations, the images will first be generated and then upscaled after that process is complete. While the image generation is taking place, you will still be able to preview the base images.</p>"},{"location":"features/POSTPROCESS/#how-to-disable","title":"How to disable","text":"<p>If, for some reason, you do not wish to load the ESRGAN libraries, you can disable them on the invoke.py command line with the <code>--no_esrgan</code> options.</p>"},{"location":"features/PROMPTS/","title":"Prompting-Features","text":""},{"location":"features/PROMPTS/#prompt-syntax-features","title":"Prompt Syntax Features","text":"<p>The InvokeAI prompting language has the following features:</p>"},{"location":"features/PROMPTS/#attention-weighting","title":"Attention weighting","text":"<p>Append a word or phrase with <code>-</code> or <code>+</code>, or a weight between <code>0</code> and <code>2</code> (<code>1</code>=default), to decrease or increase \"attention\" (= a mix of per-token CFG weighting multiplier and, for <code>-</code>, a weighted blend with the prompt without the term).</p> <p>The following syntax is recognised:</p> <ul> <li>single words without parentheses: <code>a tall thin man picking apricots+</code></li> <li>single or multiple words with parentheses:   <code>a tall thin man picking (apricots)+</code> <code>a tall thin man picking (apricots)-</code> <code>a tall thin man (picking apricots)+</code> <code>a tall thin man (picking apricots)-</code></li> <li>more effect with more symbols <code>a tall thin man (picking apricots)++</code></li> <li>nesting <code>a tall thin man (picking apricots+)++</code> (<code>apricots</code> effectively gets   <code>+++</code>)</li> <li>all of the above with explicit numbers <code>a tall thin man picking (apricots)1.1</code> <code>a tall thin man (picking (apricots)1.3)1.1</code>. (<code>+</code> is equivalent to 1.1, <code>++</code>   is pow(1.1,2), <code>+++</code> is pow(1.1,3), etc; <code>-</code> means 0.9, <code>--</code> means pow(0.9,2),   etc.)</li> </ul> <p>You can use this to increase or decrease the amount of something. Starting from this prompt of <code>a man picking apricots from a tree</code>, let's see what happens if we increase and decrease how much attention we want Stable Diffusion to pay to the word <code>apricots</code>:</p> <p></p> <p>Using <code>-</code> to reduce apricot-ness:</p> <code>a man picking apricots- from a tree</code> <code>a man picking apricots-- from a tree</code> <code>a man picking apricots--- from a tree</code> <p>Using <code>+</code> to increase apricot-ness:</p> <code>a man picking apricots+ from a tree</code> <code>a man picking apricots++ from a tree</code> <code>a man picking apricots+++ from a tree</code> <code>a man picking apricots++++ from a tree</code> <code>a man picking apricots+++++ from a tree</code> <p>You can also change the balance between different parts of a prompt. For example, below is a <code>mountain man</code>:</p> <p></p> <p>And here he is with more mountain:</p> <code>mountain+ man</code> <code>mountain++ man</code> <code>mountain+++ man</code> <p>Or, alternatively, with more man:</p> <code>mountain man+</code> <code>mountain man++</code> <code>mountain man+++</code> <code>mountain man++++</code>"},{"location":"features/PROMPTS/#prompt-blending","title":"Prompt Blending","text":"<ul> <li><code>(\"a tall thin man picking apricots\", \"a tall thin man picking pears\").blend(1,1)</code></li> <li>The existing prompt blending using <code>:&lt;weight&gt;</code> will continue to be supported -   <code>(\"a tall thin man picking apricots\", \"a tall thin man picking pears\").blend(1,1)</code>   is equivalent to   <code>a tall thin man picking apricots:1 a tall thin man picking pears:1</code> in the   old syntax.</li> <li>Attention weights can be nested inside blends.</li> <li>Non-normalized blends are supported by passing <code>no_normalize</code> as an additional   argument to the blend weights, eg   <code>(\"a tall thin man picking apricots\", \"a tall thin man picking pears\").blend(1,-1,no_normalize)</code>.   very fun to explore local maxima in the feature space, but also easy to   produce garbage output.</li> </ul> <p>See the section below on \"Prompt Blending\" for more information about how this works.</p>"},{"location":"features/PROMPTS/#prompt-conjunction","title":"Prompt Conjunction","text":"<p>Join multiple clauses together to create a conjoined prompt. Each clause will be passed to CLIP separately. </p> <p>For example, the prompt: </p> <pre><code>\"A mystical valley surround by towering granite cliffs, watercolor, warm\"\n</code></pre> <p>Can be used with .and(): <pre><code>(\"A mystical valley\", \"surround by towering granite cliffs\", \"watercolor\", \"warm\").and()\n</code></pre></p> <p>Each will give you different results - try them out and see what you prefer!</p>"},{"location":"features/PROMPTS/#escaping-parentheses-and-speech-marks","title":"Escaping parentheses and speech marks","text":"<p>If the model you are using has parentheses () or speech marks \"\" as part of its syntax, you will need to \"escape\" these using a backslash, so that<code>(my_keyword)</code> becomes <code>\\(my_keyword\\)</code>. Otherwise, the prompt parser will attempt to interpret the parentheses as part of the prompt syntax and it will get confused.</p>"},{"location":"features/PROMPTS/#prompt-blending_1","title":"Prompt Blending","text":"<p>You may blend together prompts to explore the AI's latent semantic space and generate interesting (and often surprising!) variations. The syntax is:</p> <pre><code>(\"prompt #1\", \"prompt #2\").blend(0.25, 0.75)\n</code></pre> <p>This will tell the sampler to blend 25% of the concept of prompt #1 with 75% of the concept of prompt #2. It is recommended to keep the sum of the weights to around 1.0, but interesting things might happen if you go outside of this range.</p> <p>Because you are exploring the \"mind\" of the AI, the AI's way of mixing two concepts may not match yours, leading to surprising effects. To illustrate, here are three images generated using various combinations of blend weights. As usual, unless you fix the seed, the prompts will give you different results each time you run them.</p> <p>Let's examine how this affects image generation results:</p> <pre><code>\"blue sphere, red cube, hybrid\"\n</code></pre> <p>This example doesn't use blending at all and represents the default way of mixing concepts.</p> <p></p> <p>It's interesting to see how the AI expressed the concept of \"cube\" within the sphere. If you look closely, there is depth there, so the enclosing frame is actually a cube.</p> <pre><code>(\"blue sphere\", \"red cube\").blend(0.25, 0.75)\n</code></pre> <p></p> <p>Now that's interesting. We get an image with a resemblance of a red cube, with a hint of blue shadows which represents a melding of concepts within the AI's \"latent space\" of semantic representations. </p> <pre><code>(\"blue sphere\", \"red cube\").blend(0.75, 0.25)\n</code></pre> <p></p> <p>Definitely more blue-spherey. </p> <pre><code>(\"blue sphere\", \"red cube\").blend(0.5, 0.5)\n</code></pre> <p></p> <p>Whoa...! I see blue and red, and if I squint, spheres and cubes.</p>"},{"location":"features/PROMPTS/#dynamic-prompts","title":"Dynamic Prompts","text":"<p>Dynamic Prompts are a powerful feature designed to produce a variety of prompts based on user-defined options. Using a special syntax, you can construct a prompt with multiple possibilities, and the system will automatically generate a series of permutations based on your settings. This is extremely beneficial for ideation, exploring various scenarios, or testing different concepts swiftly and efficiently.</p>"},{"location":"features/PROMPTS/#structure-of-a-dynamic-prompt","title":"Structure of a Dynamic Prompt","text":"<p>A Dynamic Prompt comprises of regular text, supplemented with alternatives enclosed within curly braces {} and separated by a vertical bar |. For example: {option1|option2|option3}. The system will then select one of the options to include in the final prompt. This flexible system allows for options to be placed throughout the text as needed.</p> <p>Furthermore, Dynamic Prompts can designate multiple selections from a single group of options. This feature is triggered by prefixing the options with a numerical value followed by \\(\\(. For example, in {2\\)\\)option1|option2|option3}, the system will select two distinct options from the set.</p>"},{"location":"features/PROMPTS/#creating-dynamic-prompts","title":"Creating Dynamic Prompts","text":"<p>To create a Dynamic Prompt, follow these steps:</p> <pre><code>Draft your sentence or phrase, identifying words or phrases with multiple possible options.\nEncapsulate the different options within curly braces {}.\nWithin the braces, separate each option using a vertical bar |.\nIf you want to include multiple options from a single group, prefix with the desired number and $$.\n</code></pre> <p>For instance: A {house|apartment|lodge|cottage} in {summer|winter|autumn|spring} designed in {style1|style2|style3}.</p>"},{"location":"features/PROMPTS/#how-dynamic-prompts-work","title":"How Dynamic Prompts Work","text":"<p>Once a Dynamic Prompt is configured, the system generates an array of combinations using the options provided. Each group of options in curly braces is treated independently, with the system selecting one option from each group. For a prefixed set (e.g., 2$$), the system will select two distinct options.</p> <p>For example, the following prompts could be generated from the above Dynamic Prompt:</p> <pre><code>A house in summer designed in style1, style2\nA lodge in autumn designed in style3, style1\nA cottage in winter designed in style2, style3\nAnd many more!\n</code></pre> <p>When the <code>Combinatorial</code> setting is on, Invoke will disable the \"Images\" selection, and generate every combination up until the setting for Max Prompts is reached. When the <code>Combinatorial</code> setting is off, Invoke will randomly generate combinations up until the setting for Images has been reached.</p>"},{"location":"features/PROMPTS/#tips-and-tricks-for-using-dynamic-prompts","title":"Tips and Tricks for Using Dynamic Prompts","text":"<p>Below are some useful strategies for creating Dynamic Prompts:</p> <pre><code>Utilize Dynamic Prompts to generate a wide spectrum of prompts, perfect for brainstorming and exploring diverse ideas.\nEnsure that the options within a group are contextually relevant to the part of the sentence where they are used. For instance, group building types together, and seasons together.\nApply the 2$$ prefix when you want to incorporate more than one option from a single group. This becomes quite handy when mixing and matching different elements.\nExperiment with different quantities for the prefix. For example, 3$$ will select three distinct options.\nBe aware of coherence in your prompts. Although the system can generate all possible combinations, not all may semantically make sense. Therefore, carefully choose the options for each group.\nAlways review and fine-tune the generated prompts as needed. While Dynamic Prompts can help you generate a multitude of combinations, the final polishing and refining remain in your hands.\n</code></pre>"},{"location":"features/PROMPTS/#sdxl-prompting","title":"SDXL Prompting","text":"<p>Prompting with SDXL is slightly different than prompting with SD1.5 or SD2.1 models - SDXL expects a prompt and a style. </p>"},{"location":"features/PROMPTS/#prompting","title":"Prompting","text":"<p>In the prompt box, enter a positive or negative prompt as you normally would. </p> <p>For the style box you can enter a style that you want the image to be generated in. You can use styles from this example list, or any other style you wish: anime, photographic, digital art, comic book, fantasy art, analog film, neon punk, isometric, low poly, origami, line art, cinematic, 3d model, pixel art, etc.</p>"},{"location":"features/PROMPTS/#concatenated-prompts","title":"Concatenated Prompts","text":"<p>InvokeAI also has the option to concatenate the prompt and style inputs, by pressing the \"link\" button in the Positive Prompt box. </p> <p>This concatenates the prompt &amp; style inputs, and passes the joined prompt and style to the SDXL model.  </p>"},{"location":"features/TEXTUAL_INVERSIONS/","title":"Textual Inversions","text":""},{"location":"features/TEXTUAL_INVERSIONS/#using-textual-inversion-files","title":"Using Textual Inversion Files","text":"<p>Textual inversion (TI) files are small models that customize the output of Stable Diffusion image generation. They can augment SD with specialized subjects and artistic styles. They are also known as \"embeds\" in the machine learning world.</p> <p>Each TI file introduces one or more vocabulary terms to the SD model. These are known in InvokeAI as \"triggers.\" Triggers are denoted using angle brackets  as in \"&lt;trigger-phrase&gt;\". The two most common type of TI files that you'll encounter are <code>.pt</code> and <code>.bin</code> files, which are produced by different TI training packages. InvokeAI supports both formats, but its built-in TI training system produces <code>.pt</code>.</p> <p>Hugging Face has amassed a large library of &gt;800 community-contributed TI files covering a broad range of subjects and styles. You can also install your own or others' TI files  by placing them in the designated directory for the compatible model type</p>"},{"location":"features/TEXTUAL_INVERSIONS/#an-example","title":"An Example","text":"<p>Here are a few examples to illustrate how it works. All these images were generated using the legacy command-line client and the Stable Diffusion 1.5 model:</p> Japanese gardener Japanese gardener &lt;ghibli-face&gt; Japanese gardener &lt;hoi4-leaders&gt; Japanese gardener &lt;cartoona-animals&gt; <p>You can also combine styles and concepts:</p> A portrait of &lt;alf&gt; in &lt;cartoona-animal&gt; style"},{"location":"features/TEXTUAL_INVERSIONS/#installing-your-own-ti-files","title":"Installing your Own TI Files","text":"<p>You may install any number of <code>.pt</code> and <code>.bin</code> files simply by copying them into the <code>embedding</code> directory of the corresponding InvokeAI models directory (usually <code>invokeai</code> in your home directory). For example, you can simply move a Stable Diffusion 1.5 embedding file to the <code>sd-1/embedding</code> folder. Be careful not to overwrite one file with another. For example, TI files generated by the Hugging Face toolkit share the named <code>learned_embedding.bin</code>. You can rename these, or use subdirectories to keep them distinct.</p> <p>At startup time, InvokeAI will scan the various <code>embedding</code> directories and load any TI files it finds there for compatible models. At startup you will see a message similar to this one:</p> <p><pre><code>&gt;&gt; Current embedding manager terms: &lt;HOI4-Leader&gt;, &lt;princess-knight&gt;\n</code></pre> To use these when generating, simply type the <code>&lt;</code> key in your prompt to open the Textual Inversion WebUI and  select the embedding you'd like to use. This UI has type-ahead support, so you can easily find supported embeddings.</p>"},{"location":"features/TRAINING/","title":"Training","text":"<p>Invoke Training has moved to its own repository, with a dedicated UI for accessing common scripts like Textual Inversion and LoRA training.</p> <p>You can find more by visiting the repo at https://github.com/invoke-ai/invoke-training</p>"},{"location":"features/UNIFIED_CANVAS/","title":"Unified Canvas","text":"<p>The Unified Canvas is a tool designed to streamline and simplify the process of composing an image using Stable Diffusion. It offers artists all of the available Stable Diffusion generation modes (Text To Image, Image To Image, Inpainting, and Outpainting) as a single unified workflow. The flexibility of the tool allows you to tweak and edit image generations, extend images beyond their initial size, and to create new content in a freeform way both inside and outside of existing images.</p> <p>This document explains the basics of using the Unified Canvas, introducing you to its features and tools one by one. It also describes some of the more advanced tools available to power users of the Canvas.</p>"},{"location":"features/UNIFIED_CANVAS/#basics","title":"Basics","text":"<p>The Unified Canvas consists of two layers: the Base Layer and the Mask Layer. You can swap from one layer to the other by selecting the layer you want in the drop-down menu on the top left corner of the Unified Canvas, or by pressing the (Q) hotkey.</p>"},{"location":"features/UNIFIED_CANVAS/#base-layer","title":"Base Layer","text":"<p>The Base Layer is the image content currently managed by the Canvas, and can be exported at any time to the gallery by using the Save to Gallery option. When the Base Layer is selected, the Brush (B) and Eraser (E) tools will directly manipulate the base layer. Any images uploaded to the Canvas, or sent to the Unified Canvas from the gallery, will clear out all existing content and set the Base layer to the new image.</p>"},{"location":"features/UNIFIED_CANVAS/#staging-area","title":"Staging Area","text":"<p>When you generate images, they will display in the Canvas's Staging Area, alongside the Staging Area toolbar buttons. While the Staging Area is active, you cannot interact with the Canvas itself.</p> <p></p> <p>Accepting generations will commit the new generation to the Base Layer. You can review all generated images using the Prev/Next arrows, save any individual generations to your gallery (without committing to the Base layer) or discard generations. While you can Undo a discard in an individual Canvas session, any generations that are not saved will be lost when the Canvas resets.</p>"},{"location":"features/UNIFIED_CANVAS/#mask-layer","title":"Mask Layer","text":"<p>The Mask Layer consists of any masked sections that have been created to inform Inpainting generations. You can paint a new mask, or edit an existing mask, using the Brush tool and the Eraser with the Mask layer set as your Active layer. Any masked areas will only affect generation inside of the current bounding box.</p>"},{"location":"features/UNIFIED_CANVAS/#bounding-box","title":"Bounding Box","text":"<p>When generating a new image, Invoke will process and apply new images within the area denoted by the Bounding Box. The Width &amp; Height settings of the Bounding Box, as well as its location within the Unified Canvas and pixels or empty space that it encloses, determine how new invocations are generated - see Inpainting &amp; Outpainting below. The Bounding Box can be moved and resized using the Move (V) tool. It can also be resized using the Bounding Box options in the Options Panel. By using these controls you can generate larger or smaller images, control which sections of the image are being processed, as well as control Bounding Box tools like the Bounding Box fill/erase.</p>"},{"location":"features/UNIFIED_CANVAS/#inpainting-outpainting","title":"Inpainting &amp; Outpainting","text":"<p>\"Inpainting\" means asking the AI to refine part of an image while leaving the rest alone. For example, updating a portrait of your grandmother to have her wear a biker's jacket.</p> masked original inpaint result <p>\"Outpainting\" means asking the AI to expand the original image beyond its original borders, making a bigger image that's still based on the original. For example, extending the above image of your Grandmother in a biker's jacket to include her wearing jeans (and while we're at it, a motorcycle!)</p> <p></p> <p>When you are using the Unified Canvas, Invoke decides automatically whether to do Inpainting, Outpainting, ImageToImage, or TextToImage by looking inside the area enclosed by the Bounding Box. It chooses the appropriate type of generation based on whether the Bounding Box contains empty (transparent) areas on the Base layer, or whether it contains colored areas from previous generations (or from painted brushstrokes) on the Base layer, and/or whether the Mask layer contains any brushstrokes. See Generation Methods below for more information.</p>"},{"location":"features/UNIFIED_CANVAS/#getting-started","title":"Getting Started","text":"<p>To get started with the Unified Canvas, you will want to generate a new base layer using Txt2Img or importing an initial image. We'll refer to either of these methods as the \"initial image\" in the below guide.</p> <p>From there, you can consider the following techniques to augment your image:</p> <ul> <li>New Images: Move the bounding box to an empty area of the Canvas, type in   your prompt, and Invoke, to generate a new image using the Text to Image   function.</li> <li>Image Correction: Use the color picker and brush tool to paint corrections   on the image, switch to the Mask layer, and brush a mask over your painted   area to use Inpainting. You can also use the ImageToImage generation   method to invoke new interpretations of the image.</li> <li>Image Expansion: Move the bounding box to include a portion of your   initial image, and a portion of transparent/empty pixels, then Invoke using a   prompt that describes what you'd like to see in that area. This will Outpaint   the image. You'll typically find more coherent results if you keep about   50-60% of the original image in the bounding box. Make sure that the Image To   Image Strength slider is set to a high value - you may need to set it higher   than you are used to.</li> <li>New Content on Existing Images: If you want to add new details or objects   into your image, use the brush tool to paint a sketch of what you'd like to   see on the image, switch to the Mask layer, and brush a mask over your painted   area to use Inpainting. If the masked area is small, consider using a   smaller bounding box to take advantage of Invoke's automatic Scaling features,   which can help to produce better details.</li> <li>And more: There are a number of creative ways to use the Canvas, and the   above are just starting points. We're excited to see what you come up with!</li> </ul>"},{"location":"features/UNIFIED_CANVAS/#generation-methods","title":"Generation Methods","text":"<p>The Canvas can use all generation methods available (Txt2Img, Img2Img, Inpainting, and Outpainting), and these will be automatically selected and used based on the current selection area within the Bounding Box.</p>"},{"location":"features/UNIFIED_CANVAS/#text-to-image","title":"Text to Image","text":"<p>If the Bounding Box is placed over an area of Canvas with an empty Base Layer, invoking a new image will use TextToImage. This generates an entirely new image based on your prompt.</p>"},{"location":"features/UNIFIED_CANVAS/#image-to-image","title":"Image to Image","text":"<p>If the Bounding Box is placed over an area of Canvas with an existing Base Layer area with no transparent pixels or masks, invoking a new image will use ImageToImage. This uses the image within the bounding box and your prompt to interpret a new image. The image will be closer to your original image at lower Image to Image strengths.</p>"},{"location":"features/UNIFIED_CANVAS/#inpainting","title":"Inpainting","text":"<p>If the Bounding Box is placed over an area of Canvas with an existing Base Layer and any pixels selected using the Mask layer, invoking a new image will use Inpainting. Inpainting uses the existing colors/forms in the masked area in order to generate a new image for the masked area only. The unmasked portion of the image will remain the same. Image to Image strength applies to the inpainted area.</p> <p>If you desire something completely different from the original image in your new generation (i.e., if you want Invoke to ignore existing colors/forms), consider toggling the Inpaint Replace setting on, and use high values for both Inpaint Replace and Image To Image Strength.</p> <p>Note</p> <p>By default, the Scale Before Processing option \u2014 which inpaints more coherent details by generating at a larger resolution and then scaling \u2014 is only activated when the Bounding Box is relatively small. To get the best inpainting results you should therefore resize your Bounding Box to the smallest area that contains your mask and enough surrounding detail to help Stable Diffusion understand the context of what you want it to draw. You should also update your prompt so that it describes just the area within the Bounding Box.</p>"},{"location":"features/UNIFIED_CANVAS/#outpainting","title":"Outpainting","text":"<p>If the Bounding Box is placed over an area of Canvas partially filled by an existing Base Layer area and partially by transparent pixels or masks, invoking a new image will use Outpainting, as well as Inpainting any masked areas.</p>"},{"location":"features/UNIFIED_CANVAS/#advanced-features","title":"Advanced Features","text":"<p>Features with non-obvious behavior are detailed below, in order to provide clarity on the intent and common use cases we expect for utilizing them.</p>"},{"location":"features/UNIFIED_CANVAS/#toolbar","title":"Toolbar","text":""},{"location":"features/UNIFIED_CANVAS/#mask-options","title":"Mask Options","text":"<ul> <li>Enable Mask - This flag can be used to Enable or Disable the currently   painted mask. If you have painted a mask, but you don't want it affect the   next invocation, but you also don't want to delete it, then you can set this   option to Disable. When you want the mask back, set this back to Enable.</li> <li>Preserve Masked Area - When enabled, Preserve Masked Area inverts the   effect of the Mask on the Inpainting process. Pixels in masked areas will be   kept unchanged, and unmasked areas will be regenerated.</li> </ul>"},{"location":"features/UNIFIED_CANVAS/#creative-tools","title":"Creative Tools","text":"<ul> <li>Brush - Base/Mask Modes - The Brush tool switches automatically between   different modes of operation for the Base and Mask layers respectively.<ul> <li>On the Base layer, the brush will directly paint on the Canvas using the   color selected on the Brush Options menu.</li> <li>On the Mask layer, the brush will create a new mask. If you're finding the   mask difficult to see over the existing content of the Unified Canvas, you   can change the color it is drawn with using the color selector on the Mask   Options dropdown.</li> </ul> </li> <li>Erase Bounding Box - On the Base layer, erases all pixels within the   Bounding Box.</li> <li>Fill Bounding Box - On the Base layer, fills all pixels within the   Bounding Box with the currently selected color.</li> </ul>"},{"location":"features/UNIFIED_CANVAS/#canvas-tools","title":"Canvas Tools","text":"<ul> <li>Move Tool - Allows for manipulation of the Canvas view (by dragging on the   Canvas, outside the bounding box), the Bounding Box (by dragging the edges of   the box), or the Width/Height of the Bounding Box (by dragging one of the 9   directional handles).</li> <li>Reset View - Click to re-orients the view to the center of the Bounding   Box.</li> <li>Merge Visible - If your browser is having performance problems drawing the   image in the Unified Canvas, click this to consolidate all of the information   currently being rendered by your browser into a merged copy of the image. This   lowers the resource requirements and should improve performance.</li> </ul>"},{"location":"features/UNIFIED_CANVAS/#compositing-seam-correction","title":"Compositing / Seam Correction","text":"<p>When doing Inpainting or Outpainting, Invoke needs to merge the pixels generated by Stable Diffusion into your existing image. This is achieved through compositing - the area around the the boundary between your image and the new generation is automatically blended to produce a seamless output. In a fully automatic process, a mask is generated to cover the boundary, and then the area of the boundary is Inpainted.</p> <p>Although the default options should work well most of the time, sometimes it can help to alter the parameters that control the Compositing. A larger blur and a blur setting  have been noted as producing consistently strong results . Strength of 0.7 is best for reducing hard seams.</p> <ul> <li>Mode - What part of the image will have the the Compositing applied to it.</li> <li>Mask edge will apply Compositing to the edge of the masked area</li> <li>Mask will apply Compositing to the entire masked area</li> <li>Unmasked will apply Compositing to the entire image</li> <li>Steps - Number of generation steps that will occur during the Coherence Pass, similar to Denoising Steps. Higher step counts will generally have better results.</li> <li>Strength - How much noise is added for the Coherence Pass, similar to Denoising Strength. A strength of 0 will result in an unchanged image, while a strength of 1 will result in an image with a completely new area as defined by the Mode setting.</li> <li>Blur - Adjusts the pixel radius of the the mask. A larger blur radius will cause the mask to extend past the visibly masked area, while too small of a blur radius will result in a mask that is smaller than the visibly masked area.</li> <li>Blur Method - The method of blur applied to the masked area. </li> </ul>"},{"location":"features/UNIFIED_CANVAS/#infill-scaling","title":"Infill &amp; Scaling","text":"<ul> <li>Scale Before Processing &amp; W/H: When generating images with a bounding box   smaller than the optimized W/H of the model (e.g., 512x512 for SD1.5), this   feature first generates at a larger size with the same aspect ratio, and then   scales that image down to fill the selected area. This is particularly useful   when inpainting very small details. Scaling is optional but is enabled by   default.</li> <li>Inpaint Replace: When Inpainting, the default method is to utilize the   existing RGB values of the Base layer to inform the generation process. If   Inpaint Replace is enabled, noise is generated and blended with the existing   pixels (completely replacing the original RGB values at an Inpaint Replace   value of 1). This can help generate more variation from the pixels on the Base   layers.<ul> <li>When using Inpaint Replace you should use a higher Image To Image Strength   value, especially at higher Inpaint Replace values</li> </ul> </li> <li>Infill Method: Invoke currently supports two methods for producing RGB   values for use in the Outpainting process: Patchmatch and Tile. We believe   that Patchmatch is the superior method, however we provide support for Tile in   case Patchmatch cannot be installed or is unavailable on your computer.</li> <li>Tile Size: The Tile method for Outpainting sources small portions of the   original image and randomly place these into the areas being Outpainted. This   value sets the size of those tiles.</li> </ul>"},{"location":"features/UNIFIED_CANVAS/#hot-keys","title":"Hot Keys","text":"<p>The Unified Canvas is a tool that excels when you use hotkeys. You can view the full list of keyboard shortcuts, updated with all new features, by clicking the Keyboard Shortcuts icon at the top right of the InvokeAI WebUI.</p>"},{"location":"features/UTILITIES/","title":"Utilities","text":""},{"location":"features/UTILITIES/#command-line-utilities","title":"Command-line Utilities","text":"<p>InvokeAI comes with several scripts that are accessible via the command line. To access these commands, start the \"developer's console\" from the launcher (<code>invoke.bat</code> menu item [7]). Users who are familiar with Python can alternatively activate InvokeAI's virtual environment (typically, but not necessarily <code>invokeai/.venv</code>).</p> <p>In the developer's console, type the script's name to run it. To get a synopsis of what a utility does and the command-line arguments it accepts, pass it the <code>-h</code> argument, e.g.</p> <pre><code>invokeai-merge -h\n</code></pre>"},{"location":"features/UTILITIES/#invokeai-web","title":"invokeai-web","text":"<p>This script launches the web server and is effectively identical to selecting option [1] in the launcher. An advantage of launching the server from the command line is that you can override any setting configuration option in <code>invokeai.yaml</code> using like-named command-line arguments. For example, to temporarily change the size of the RAM cache to 7 GB, you can launch as follows:</p> <pre><code>invokeai-web --ram 7\n</code></pre>"},{"location":"features/UTILITIES/#invokeai-merge","title":"invokeai-merge","text":"<p>This is the model merge script, the same as launcher option [3]. Call it with the <code>--gui</code> command-line argument to start the interactive console-based GUI. Alternatively, you can run it non-interactively using command-line arguments as illustrated in the example below which merges models named <code>stable-diffusion-1.5</code> and <code>inkdiffusion</code> into a new model named <code>my_new_model</code>:</p> <pre><code>invokeai-merge --force --base-model sd-1 --models stable-diffusion-1.5 inkdiffusion --merged_model_name my_new_model\n</code></pre>"},{"location":"features/UTILITIES/#invokeai-ti","title":"invokeai-ti","text":"<p>This is the textual inversion training script that is run by launcher option [2]. Call it with <code>--gui</code> to run the interactive console-based front end. It can also be run non-interactively. It has about a zillion arguments, but a typical training session can be launched with:</p> <pre><code>invokeai-ti --model stable-diffusion-1.5 \\\n            --placeholder_token 'jello' \\\n            --learnable_property object \\\n            --num_train_epochs 50 \\\n            --train_data_dir /path/to/training/images \\\n            --output_dir /path/to/trained/model\n</code></pre> <p>(Note that \\ is the Linux/Mac long-line continuation character. Use ^ in Windows).</p>"},{"location":"features/UTILITIES/#invokeai-install","title":"invokeai-install","text":"<p>This is the console-based model install script that is run by launcher option [4]. If called without arguments, it will launch the interactive console-based interface. It can also be used non-interactively to list, add and remove models as shown by these examples:</p> <ul> <li>This will download and install three models from CivitAI, HuggingFace, and local disk:</li> </ul> <p><pre><code>invokeai-install --add https://civitai.com/api/download/models/161302 ^\n                  gsdf/Counterfeit-V3.0  ^\n                  D:\\Models\\merge_model_two.safetensors\n</code></pre> (Note that ^ is the Windows long-line continuation character. Use \\ on Linux/Mac).</p> <ul> <li>This will list installed models of type <code>main</code>:</li> </ul> <pre><code>invokeai-model-install --list-models main\n</code></pre> <ul> <li>This will delete the models named <code>voxel-ish</code> and <code>realisticVision</code>:</li> </ul> <pre><code>invokeai-model-install --delete voxel-ish realisticVision\n</code></pre>"},{"location":"features/UTILITIES/#invokeai-configure","title":"invokeai-configure","text":"<p>This is the console-based configure script that ran when InvokeAI was first installed. You can run it again at any time to change the configuration, repair a broken install.</p> <p>Called without any arguments, <code>invokeai-configure</code> enters interactive mode with two screens. The first screen is a form that provides access to most of InvokeAI's configuration options. The second screen lets you download, add, and delete models interactively. When you exit the second screen, the script will add any missing \"support models\" needed for core functionality, and any selected \"sd weights\" which are the model checkpoint/diffusers files.</p> <p>This behavior can be changed via a series of command-line arguments. Here are some of the useful ones:</p> <ul> <li> <p><code>invokeai-configure --skip-sd-weights --skip-support-models</code> This will run just the configuration part of the utility, skipping downloading of support models and stable diffusion weights.</p> </li> <li> <p><code>invokeai-configure --yes</code> This will run the configure script non-interactively. It will set the configuration options to their default values, install/repair support models, and download the \"recommended\" set of SD models.</p> </li> <li> <p><code>invokeai-configure --yes --default_only</code>  This will run the configure script non-interactively. In contrast to the previous command, it will only download the default SD model, Stable Diffusion v1.5</p> </li> <li> <p><code>invokeai-configure --yes --default_only --skip-sd-weights</code>  This is similar to the previous command, but will not download any SD models at all. It is usually used to repair a broken install.</p> </li> </ul> <p>By default, <code>invokeai-configure</code> runs on the currently active InvokeAI root folder. To run it against a different root, pass it the <code>--root &lt;/path/to/root&gt;</code> argument.</p> <p>Lastly, you can use <code>invokeai-configure</code> to create a working root directory entirely from scratch. Assuming you wish to make a root directory named <code>InvokeAI-New</code>, run this command:</p> <p><pre><code>invokeai-configure --root InvokeAI-New --yes --default_only\n</code></pre> This will create a minimally functional root directory. You can now launch the web server against it with <code>invokeai-web --root InvokeAI-New</code>.</p>"},{"location":"features/UTILITIES/#invokeai-update","title":"invokeai-update","text":"<p>This is the interactive console-based script that is run by launcher menu item [8] to update to a new version of InvokeAI. It takes no command-line arguments.</p>"},{"location":"features/UTILITIES/#invokeai-metadata","title":"invokeai-metadata","text":"<p>This is a script which takes a list of InvokeAI-generated images and outputs their metadata in the same JSON format that you get from the `` button in the Web GUI. For example:</p> <pre><code>$ invokeai-metadata ffe2a115-b492-493c-afff-7679aa034b50.png\nffe2a115-b492-493c-afff-7679aa034b50.png:\n{\n    \"app_version\": \"3.1.0\",\n    \"cfg_scale\": 8.0,\n    \"clip_skip\": 0,\n    \"controlnets\": [],\n    \"generation_mode\": \"sdxl_txt2img\",\n    \"height\": 1024,\n    \"loras\": [],\n    \"model\": {\n        \"base_model\": \"sdxl\",\n        \"model_name\": \"stable-diffusion-xl-base-1.0\",\n        \"model_type\": \"main\"\n    },\n    \"negative_prompt\": \"\",\n    \"negative_style_prompt\": \"\",\n    \"positive_prompt\": \"military grade sushi dinner for shock troopers\",\n    \"positive_style_prompt\": \"\",\n    \"rand_device\": \"cpu\",\n    \"refiner_cfg_scale\": 7.5,\n    \"refiner_model\": {\n        \"base_model\": \"sdxl-refiner\",\n        \"model_name\": \"sd_xl_refiner_1.0\",\n        \"model_type\": \"main\"\n    },\n    \"refiner_negative_aesthetic_score\": 2.5,\n    \"refiner_positive_aesthetic_score\": 6.0,\n    \"refiner_scheduler\": \"euler\",\n    \"refiner_start\": 0.8,\n    \"refiner_steps\": 20,\n    \"scheduler\": \"euler\",\n    \"seed\": 387129902,\n    \"steps\": 25,\n    \"width\": 1024\n}\n</code></pre> <p>You may list multiple files on the command line.</p>"},{"location":"features/UTILITIES/#invokeai-import-images","title":"invokeai-import-images","text":"<p>InvokeAI uses a database to store information about images it generated, and just copying the image files from one InvokeAI root directory to another does not automatically import those images into the destination's gallery. This script allows you to bulk import images generated by one instance of InvokeAI into a gallery maintained by another. It also works on images generated by older versions of InvokeAI, going way back to version 1.</p> <p>This script has an interactive mode only. The following example shows it in action:</p> <pre><code>$ invokeai-import-images\n===============================================================================\nThis script will import images generated by earlier versions of\nInvokeAI into the currently installed root directory:\n   /home/XXXX/invokeai-main\nIf this is not what you want to do, type ctrl-C now to cancel.\n===============================================================================\n= Configuration &amp; Settings\nFound invokeai.yaml file at /home/XXXX/invokeai-main/invokeai.yaml:\n  Database : /home/XXXX/invokeai-main/databases/invokeai.db\n  Outputs  : /home/XXXX/invokeai-main/outputs/images\n\nUse these paths for import (yes) or choose different ones (no) [Yn]:\nInputs: Specify absolute path containing InvokeAI .png images to import: /home/XXXX/invokeai-2.3/outputs/images/\nInclude files from subfolders recursively [yN]?\n\nOptions for board selection for imported images:\n1) Select an existing board name. (found 4)\n2) Specify a board name to create/add to.\n3) Create/add to board named 'IMPORT'.\n4) Create/add to board named 'IMPORT' with the current datetime string appended (.e.g IMPORT_20230919T203519Z).\n5) Create/add to board named 'IMPORT' with a the original file app_version appended (.e.g IMPORT_2.2.5).\nSpecify desired board option: 3\n\n===============================================================================\n= Import Settings Confirmation\n\nDatabase File Path               : /home/XXXX/invokeai-main/databases/invokeai.db\nOutputs/Images Directory         : /home/XXXX/invokeai-main/outputs/images\nImport Image Source Directory    : /home/XXXX/invokeai-2.3/outputs/images/\n  Recurse Source SubDirectories  : No\nCount of .png file(s) found      : 5785\nBoard name option specified      : IMPORT\nDatabase backup will be taken at : /home/XXXX/invokeai-main/databases/backup\n\nNotes about the import process:\n- Source image files will not be modified, only copied to the outputs directory.\n- If the same file name already exists in the destination, the file will be skipped.\n- If the same file name already has a record in the database, the file will be skipped.\n- Invoke AI metadata tags will be updated/written into the imported copy only.\n- On the imported copy, only Invoke AI known tags (latest and legacy) will be retained (dream, sd-metadata, invokeai, invokeai_metadata)\n- A property 'imported_app_version' will be added to metadata that can be viewed in the UI's metadata viewer.\n- The new 3.x InvokeAI outputs folder structure is flat so recursively found source imges will all be placed into the single outputs/images folder.\n\nDo you wish to continue with the import [Yn] ?\n\nMaking DB Backup at /home/lstein/invokeai-main/databases/backup/backup-20230919T203519Z-invokeai.db...Done!\n\n===============================================================================\nImporting /home/XXXX/invokeai-2.3/outputs/images/17d09907-297d-4db3-a18a-60b337feac66.png\n... (5785 more lines) ...\n===============================================================================\n= Import Complete - Elpased Time: 0.28 second(s)\n\nSource File(s)                          : 5785\nTotal Imported                          : 5783\nSkipped b/c file already exists on disk : 1\nSkipped b/c file already exists in db   : 0\nErrors during import                    : 1\n</code></pre>"},{"location":"features/UTILITIES/#invokeai-db-maintenance","title":"invokeai-db-maintenance","text":"<p>This script helps maintain the integrity of your InvokeAI database by finding and fixing three problems that can arise over time:</p> <ol> <li> <p>An image was manually deleted from the outputs directory, leaving a    dangling image record in the InvokeAI database. This will cause a    black image to appear in the gallery. This is an \"orphaned database    image record.\" The script can fix this by running a \"clean\"    operation on the database, removing the orphaned entries.</p> </li> <li> <p>An image is present in the outputs directory but there is no    corresponding entry in the database. This can happen when the image    is added manually to the outputs directory, or if a crash occurred    after the image was generated but before the database was    completely updated. The symptom is that the image is present in the    outputs folder but doesn't appear in the InvokeAI gallery. This is    called an \"orphaned image file.\" The script can fix this problem by    running an \"archive\" operation in which orphaned files are moved    into a directory named <code>outputs/images-archive</code>. If you wish, you    can then run <code>invokeai-image-import</code> to reimport these images back    into the database.</p> </li> <li> <p>The thumbnail for an image is missing, again causing a black    gallery thumbnail. This is fixed by running the \"thumbnaiils\"    operation, which simply regenerates and re-registers the missing    thumbnail.</p> </li> </ol> <p>You can find and fix all three of these problems in a single go by executing this command:</p> <pre><code>invokeai-db-maintenance --operation all\n</code></pre> <p>Or you can run just the clean and thumbnail operations like this:</p> <pre><code>invokeai-db-maintenance -operation clean, thumbnail\n</code></pre> <p>If called without any arguments, the script will ask you which operations you wish to perform.</p>"},{"location":"features/UTILITIES/#invokeai-migrate3","title":"invokeai-migrate3","text":"<p>This script will migrate settings and models (but not images!) from an InvokeAI v2.3 root folder to an InvokeAI 3.X folder. Call it with the source and destination root folders like this:</p> <pre><code>invokeai-migrate3 --from ~/invokeai-2.3 --to invokeai-3.1.1\n</code></pre> <p>Both directories must previously have been properly created and initialized by <code>invokeai-configure</code>. If you wish to migrate the images contained in the older root as well, you can use the <code>invokeai-image-migrate</code> script described earlier.</p> <p>Copyright \u00a9 2023, Lincoln Stein and the InvokeAI Development Team</p>"},{"location":"features/WATERMARK%2BNSFW/","title":"Invisible Watermark and the NSFW Checker","text":""},{"location":"features/WATERMARK%2BNSFW/#watermarking","title":"Watermarking","text":"<p>InvokeAI does not apply watermarking to images by default. However, many computer scientists working in the field of generative AI worry that a flood of computer-generated imagery will contaminate the image data sets needed to train future generations of generative models.</p> <p>InvokeAI offers an optional watermarking mode that writes a small bit of text, InvokeAI, into each image that it generates using an \"invisible\" watermarking library that spreads the information throughout the image in a way that is not perceptible to the human eye. If you are planning to share your generated images on internet-accessible services, we encourage you to activate the invisible watermark mode in order to help preserve the digital image environment.</p> <p>The downside of watermarking is that it increases the size of the image moderately, and has been reported by some individuals to degrade image quality. Your mileage may vary.</p> <p>To read the watermark in an image, activate the InvokeAI virtual environment (called the \"developer's console\" in the launcher) and run the command:</p> <pre><code>invisible-watermark -a decode -t bytes -m dwtDct -l 64 /path/to/image.png\n</code></pre>"},{"location":"features/WATERMARK%2BNSFW/#the-nsfw-safety-checker","title":"The NSFW (\"Safety\") Checker","text":"<p>Stable Diffusion 1.5-based image generation models will produce sexual imagery if deliberately prompted, and will occasionally produce such images when this is not intended. Such images are colloquially known as \"Not Safe for Work\" (NSFW). This behavior is due to the nature of the training set that Stable Diffusion was trained on, which culled millions of \"aesthetic\" images from the Internet.</p> <p>You may not wish to be exposed to these images, and in some jurisdictions it may be illegal to publicly distribute such imagery, including mounting a publicly-available server that provides unfiltered images to the public. Furthermore, the Stable Diffusion weights License, and the Stable Diffusion XL License both forbid the models from being used to \"exploit any of the vulnerabilities of a specific group of persons.\"</p> <p>For these reasons Stable Diffusion offers a \"safety checker,\" a machine learning model trained to recognize potentially disturbing imagery. When a potentially NSFW image is detected, the checker will blur the image and paste a warning icon on top. The checker can be turned on and off in the Web interface under Settings.</p>"},{"location":"features/WATERMARK%2BNSFW/#caveats","title":"Caveats","text":"<p>There are a number of caveats that you need to be aware of.</p>"},{"location":"features/WATERMARK%2BNSFW/#accuracy","title":"Accuracy","text":"<p>The checker is not perfect.It will occasionally flag innocuous images (false positives), and will frequently miss violent and gory imagery (false negatives). It rarely fails to flag sexual imagery, but this has been known to happen. For these reasons, the InvokeAI team prefers to refer to the software as a \"NSFW Checker\" rather than \"safety checker.\"</p>"},{"location":"features/WATERMARK%2BNSFW/#memory-usage-and-performance","title":"Memory Usage and Performance","text":"<p>The NSFW checker consumes an additional 1.2G of GPU VRAM on top of the 3.4G of VRAM used by Stable Diffusion v1.5 (this is with half-precision arithmetic). This means that the checker will not run successfully on GPU cards with less than 6GB VRAM, and will reduce the size of the images that you can produce.</p> <p>The checker also introduces a slight performance penalty. Images will take ~1 second longer to generate when the checker is activated. Generally this is not noticeable.</p>"},{"location":"features/WATERMARK%2BNSFW/#intermediate-images-in-the-web-ui","title":"Intermediate Images in the Web UI","text":"<p>The checker only operates on the final image produced by the Stable Diffusion algorithm. If you are using the Web UI and have enabled the display of intermediate images, you will briefly be exposed to a low-resolution (mosaicized) version of the final image before it is flagged by the checker and replaced by a fully blurred version. You are encouraged to turn off intermediate image rendering when you are using the checker. Future versions of InvokeAI will apply additional blurring to intermediate images when the checker is active.</p>"},{"location":"features/WEB/","title":"InvokeAI Web Server","text":""},{"location":"features/WEB/#quick-guided-walkthrough-of-the-webuis-features","title":"Quick guided walkthrough of the WebUI's features","text":"<p>While most of the WebUI's features are intuitive, here is a guided walkthrough through its various components.</p>"},{"location":"features/WEB/#launching-the-webui","title":"Launching the WebUI","text":"<p>To run the InvokeAI web server, start the <code>invoke.sh</code>/<code>invoke.bat</code> script and select option (1). Alternatively, with the InvokeAI environment active, run <code>invokeai-web</code>:</p> <pre><code>invokeai-web\n</code></pre> <p>You can then connect to the server by pointing your web browser at http://localhost:9090. To reach the server from a different machine on your LAN, you may launch the web server with the <code>--host</code> argument and either the IP address of the host you are running it on, or the wildcard <code>0.0.0.0</code>. For example:</p> <pre><code>invoke.sh --host 0.0.0.0\n</code></pre> <p>or</p> <pre><code>invokeai-web --host 0.0.0.0\n</code></pre>"},{"location":"features/WEB/#the-invokeai-web-interface","title":"The InvokeAI Web Interface","text":"<p>The screenshot above shows the Text to Image tab of the WebUI. There are three main sections:</p> <ol> <li> <p>A control panel on the left, which contains various settings    for text to image generation. The most important part is the text    field (currently showing <code>fantasy painting, horned demon</code>) for    entering the positive text prompt, another text field right below it for an    optional negative text prompt (concepts to exclude), and a Invoke button     to begin the image rendering process.</p> </li> <li> <p>The current image section in the middle, which shows a large    format version of the image you are currently working on. A series    of buttons at the top lets you modify and manipulate the image in    various ways.</p> </li> <li> <p>A gallery section on the right that contains a history of the images you    have generated. These images are read and written to the directory specified    in the <code>INVOKEAIROOT/invokeai.yaml</code> initialization file, usually a directory    named <code>outputs</code> in <code>INVOKEAIROOT</code>.</p> </li> </ol> <p>In addition to these three elements, there are a series of icons for changing global settings, reporting bugs, and changing the theme on the upper right.</p> <p>There are also a series of icons to the left of the control panel (see highlighted area in the screenshot below) which select among a series of tabs for performing different types of operations.</p> <p></p> <p>From top to bottom, these are:</p> <ol> <li>Text to Image - generate images from text</li> <li>Image to Image - from an uploaded starting image (drawing or photograph)    generate a new one, modified by the text prompt</li> <li>Unified Canvas - Interactively combine multiple images, extend them    with outpainting,and modify interior portions of the image with    inpainting, erase portions of a starting image and have the AI fill in    the erased region from a text prompt.</li> <li>Node Editor - (experimental) this panel allows you to create    pipelines of common operations and combine them into workflows.</li> <li>Model Manager - this panel allows you to import and configure new    models using URLs, local paths, or HuggingFace diffusers repo_ids.</li> </ol>"},{"location":"features/WEB/#walkthrough","title":"Walkthrough","text":"<p>The following walkthrough will exercise most (but not all) of the WebUI's feature set.</p>"},{"location":"features/WEB/#text-to-image","title":"Text to Image","text":"<ol> <li> <p>Launch the WebUI using launcher option [1] and connect to it with    your browser by accessing <code>http://localhost:9090</code>. If the browser    and server are running on different machines on your LAN, add the    option <code>--host 0.0.0.0</code> to the <code>invoke.sh</code> launch command line and connect to    the machine hosting the web server using its IP address or domain    name.</p> </li> <li> <p>If all goes well, the WebUI should come up and you'll see a green dot    meaning <code>connected</code>  on the upper right.</p> </li> </ol> <p></p>"},{"location":"features/WEB/#basics","title":"Basics","text":"<ol> <li> <p>Generate an image by typing bluebird into the large prompt field     on the upper left and then clicking on the Invoke button or pressing     the return button.     After a short wait, you'll see a large image of a bluebird in the     image panel, and a new thumbnail in the gallery on the right.</p> <p>If you need more room on the screen, you can turn the gallery off by typing the g hotkey. You can turn it back on later by clicking the image icon that appears in the gallery's place. The list of hotkeys can be found by clicking on the keyboard icon above the image gallery.</p> </li> <li> <p>Generate a bunch of bluebird images by increasing the number of     requested images by adjusting the Images counter just below the Invoke     button. As each is generated, it will be added to the gallery. You can     switch the active image by clicking on the gallery thumbnails.</p> <p>If you'd like to watch the image generation progress, click the hourglass icon above the main image area. As generation progresses, you'll see increasingly detailed versions of the ultimate image.</p> </li> <li> <p>Try playing with different settings, including changing the main     model, the image width and height, the Scheduler, the Steps and     the CFG scale.</p> <p>The Model changes the main model. Thousands of custom models are now available, which generate a variety of image styles and subjects. While InvokeAI comes with a few starter models, it is easy to import new models into the application. See Installing Models for more details.</p> <p>Image Width and Height do what you'd expect. However, be aware that larger images consume more VRAM memory and take longer to generate.</p> <p>The Scheduler controls how the AI selects the image to display. Some samplers are more \"creative\" than others and will produce a wider range of variations (see next section). Some samplers run faster than others.</p> <p>Steps controls how many noising/denoising/sampling steps the AI will take. The higher this value, the more refined the image will be, but the longer the image will take to generate. A typical strategy is to generate images with a low number of steps in order to select one to work on further, and then regenerate it using a higher number of steps.</p> <p>The CFG Scale controls how hard the AI tries to match the generated image to the input prompt. You can go as high or low as you like, but generally values greater than 20 won't improve things much, and values lower than 5 will produce unexpected images. There are complex interactions between Steps, CFG Scale and the Scheduler, so experiment to find out what works for you.</p> <p>The Seed controls the series of values returned by InvokeAI's random number generator. Each unique seed value will generate a different image. To regenerate a previous image, simply use the original image's seed value. A slider to the right of the Seed field will change the seed each time an image is generated.</p> </li> </ol> <p></p> <ol> <li> <p>To regenerate a previously-generated image, select the image you     want and click the asterisk (\"*\") button at the top of the     image. This loads the text prompt and other original settings into     the control panel. If you then press Invoke it will regenerate     the image exactly. You can also selectively modify the prompt or     other settings to tweak the image.</p> <p>Alternatively, you may click on the \"sprouting plant icon\" to load just the image's seed, and leave other settings unchanged or the quote icon to load just the positive and negative prompts.</p> </li> <li> <p>To regenerate a Stable Diffusion image that was generated by another SD     package, you need to know its text prompt and its Seed. Copy-paste the     prompt into the prompt box, unset the Randomize Seed control in the     control panel, and copy-paste the desired Seed into its text field. When     you Invoke, you will get something similar to the original image. It will     not be exact unless you also set the correct values for the original     sampler, CFG, steps and dimensions, but it will (usually) be close.</p> </li> <li> <p>To save an image, right click on it to bring up a menu that will     let you download the image, save it to a named image gallery, and     copy it to the clipboard, among other things.</p> </li> </ol>"},{"location":"features/WEB/#upscaling","title":"Upscaling","text":"<p>\"Upscaling\" is the process of increasing the size of an image while     retaining the sharpness. InvokeAI uses an external library called     \"ESRGAN\" to do this. To invoke upscaling, simply select an image     and press the \"expanding arrows\" button above it. You can select     between 2X and 4X upscaling, and adjust the upscaling strength,     which has much the same meaning as in facial reconstruction. Try     running this on one of your previously-generated images.</p>"},{"location":"features/WEB/#image-to-image","title":"Image to Image","text":"<p>InvokeAI lets you take an existing image and use it as the basis for a new creation. You can use any sort of image, including a photograph, a scanned sketch, or a digital drawing, as long as it is in PNG or JPEG format.</p> <p>For this tutorial, we'll use the file named Lincoln-and-Parrot-512.png.</p> <ol> <li> <p>Click on the Image to Image tab icon, which is the second icon     from the top on the left-hand side of the screen. This will bring     you to a screen similar to the one shown here:</p> <p></p> </li> <li> <p>Drag-and-drop the Lincoln-and-Parrot image into the Image panel, or click     the blank area to get an upload dialog. The image will load into an area     marked Initial Image. (The WebUI will also load the most     recently-generated image from the gallery into a section on the left, but     this image will be replaced in the next step.)</p> </li> <li> <p>Go to the prompt box and type old sea captain with raven on shoulder and     press Invoke. A derived image will appear to the right of the original one:</p> <p></p> </li> <li> <p>Experiment with the different settings. The most influential one in Image to     Image is Denoising Strength located about midway down the control     panel. By default it is set to 0.75, but can range from 0.0 to 0.99. The     higher the value, the more of the original image the AI will replace. A     value of 0 will leave the initial image completely unchanged, while 0.99     will replace it completely. However, the Scheduler and CFG Scale also     influence the final result. You can also generate variations in the same way     as described in Text to Image.</p> </li> <li> <p>What if we only want to change certain part(s) of the image and     leave the rest intact? This is called Inpainting, and you can do     it in the Unified Canvas. The Unified Canvas     also allows you to extend borders of the image and fill in the     blank areas, a process called outpainting.</p> </li> <li> <p>Would you like to modify a previously-generated image using the Image to     Image facility? Easy! While in the Image to Image panel, drag and drop any     image in the gallery into the Initial Image area, and it will be ready for     use. You can do the same thing with the main image display. Click on the     Send to icon to get a menu of     commands and choose \"Send to Image to Image\".</p> <p> </p> </li> </ol>"},{"location":"features/WEB/#textual-inversion-lora-and-controlnet","title":"Textual Inversion, LoRA and ControlNet","text":"<p>InvokeAI supports several different types of model files that extending the capabilities of the main model by adding artistic styles, special effects, or subjects. By mixing and matching textual inversion, LoRA and ControlNet models, you can achieve many interesting and beautiful effects.</p> <p>We will give an example using a LoRA model named \"Ink Scenery\". This LoRA, which can be downloaded from Civitai (civitai.com), is specialized to paint landscapes that look like they were made with dripping india ink. To install this LoRA, we first download it and  put it into the <code>autoimport/lora</code> folder located inside the <code>invokeai</code> root directory. After restarting the web server, the LoRA will now become available for use.</p> <p>To see this LoRA at work, we'll first generate an image without it using the standard <code>stable-diffusion-v1-5</code> model. Choose this model and enter the prompt \"mountains, ink\". Here is a typical generated image, a mountain range rendered in ink and watercolor wash:</p> <p></p> <p>Now let's install and activate the Ink Scenery LoRA. Go to https://civitai.com/models/78605/ink-scenery-or and download the LoRA model file to <code>invokeai/autoimport/lora</code> and restart the web server. (Alternatively, you can use InvokeAI's Web Model Manager to download and install the LoRA directly by typing its URL into the Import Models-&gt;Location field).</p> <p>Scroll down the control panel until you get to the LoRA accordion section, and open it:</p> <p></p> <p>Click the popup menu and select \"Ink scenery\". (If it isn't there, then the model wasn't installed to the right place, or perhaps you forgot to restart the web server.) The LoRA section will change to look like this:</p> <p></p> <p>Note that there is now a slider control for Ink scenery. The slider controls how much influence the LoRA model will have on the generated image.</p> <p>Run the \"mountains, ink\" prompt again and observe the change in style:</p> <p></p> <p>Try adjusting the weight slider for larger and smaller weights and generate the image after each adjustment. The higher the weight, the more influence the LoRA will have.</p> <p>To remove the LoRA completely, just click on its trash can icon.</p> <p>Multiple LoRAs can be added simultaneously and combined with textual inversions and ControlNet models. Please see Textual Inversions and LoRAs and Using ControlNet for details.</p>"},{"location":"features/WEB/#summary","title":"Summary","text":"<p>This walkthrough just skims the surface of the many things InvokeAI can do. Please see Features for more detailed reference guides.</p>"},{"location":"features/WEB/#acknowledgements","title":"Acknowledgements","text":"<p>A huge shout-out to the core team working to make the Web GUI a reality, including psychedelicious, Kyle0654 and blessedcoolant. hipsterusername was the team's unofficial cheerleader and added tooltips/docs.</p>"},{"location":"features/WEBUIHOTKEYS/","title":"WebUI Hotkey List","text":""},{"location":"features/WEBUIHOTKEYS/#app-hotkeys","title":"App Hotkeys","text":"Setting Hotkey Ctrl+Enter Invoke Shift+X Cancel Alt+A Focus Prompt O Toggle Options Shift+O Pin Options Z Toggle Viewer G Toggle Gallery F Maximize Workspace 1 - 5 Change Tabs ` Toggle Console"},{"location":"features/WEBUIHOTKEYS/#general-hotkeys","title":"General Hotkeys","text":"Setting Hotkey P Set Prompt S Set Seed A Set Parameters Shift+R Restore Faces Shift+U Upscale I Show Info Shift+I Send To Image To Image Del Delete Image Esc Close Panels"},{"location":"features/WEBUIHOTKEYS/#gallery-hotkeys","title":"Gallery Hotkeys","text":"Setting Hotkey Left Previous Image Right Next Image Shift+G Toggle Gallery Pin Shift+Up Increase Gallery Image Size Shift+Down Decrease Gallery Image Size"},{"location":"features/WEBUIHOTKEYS/#unified-canvas-hotkeys","title":"Unified Canvas Hotkeys","text":"Setting Hotkey B Select Brush E Select Eraser [ Decrease Brush Size ] Increase Brush Size Shift+[ Decrease Brush Opacity Shift+] Increase Brush Opacity V Move Tool Shift+F Fill Bounding Box Del / Backspace Erase Bounding Box C Select Color Picker N Toggle Snap Hold Space Quick Toggle Move Q Toggle Layer Shift+C Clear Mask H Hide Mask Shift+H Show/Hide Bounding Box Shift+M Merge Visible Shift+S Save To Gallery Ctrl+C Copy To Clipboard Shift+D Download Image Ctrl+Z Undo Ctrl+Y / Ctrl+Shift+Z Redo R Reset View Left Previous Staging Image Right Next Staging Image Enter Accept Staging Image"},{"location":"help/FAQ/","title":"FAQ","text":"<p>How to Reinstall</p> <p>Many issues can be resolved by re-installing the application. You won't lose any data by re-installing. We suggest downloading the latest release and using it to re-install the application. Consult the installer guide for more information.</p> <p>When you run the installer, you'll have an option to select the version to install. If you aren't ready to upgrade, you choose the current version to fix a broken install.</p> <p>If the troubleshooting steps on this page don't get you up and running, please either create an issue or hop on discord for help.</p>"},{"location":"help/FAQ/#how-to-install","title":"How to Install","text":"<p>You can download the latest installers here.</p> <p>Note that any releases marked as pre-release are in a beta state. You may experience some issues, but we appreciate your help testing those! For stable/reliable installations, please install the latest release.</p>"},{"location":"help/FAQ/#downloading-models-and-using-existing-models","title":"Downloading models and using existing models","text":"<p>The Model Manager tab in the UI provides a few ways to install models, including using your already-downloaded models. You'll see a popup directing you there on first startup. For more information, see the model install docs.</p>"},{"location":"help/FAQ/#missing-models-after-updating-to-v4","title":"Missing models after updating to v4","text":"<p>If you find some models are missing after updating to v4, it's likely they weren't correctly registered before the update and didn't get picked up in the migration.</p> <p>You can use the <code>Scan Folder</code> tab in the Model Manager UI to fix this. The models will either be in the old, now-unused <code>autoimport</code> folder, or your <code>models</code> folder.</p> <ul> <li>Find and copy your install's old <code>autoimport</code> folder path, install the main install folder.</li> <li>Go to the Model Manager and click <code>Scan Folder</code>.</li> <li>Paste the path and scan.</li> <li>IMPORTANT: Uncheck <code>Inplace install</code>.</li> <li>Click <code>Install All</code> to install all found models, or just install the models you want.</li> </ul> <p>Next, find and copy your install's <code>models</code> folder path (this could be your custom models folder path, or the <code>models</code> folder inside the main install folder).</p> <p>Follow the same steps to scan and import the missing models.</p>"},{"location":"help/FAQ/#slow-generation","title":"Slow generation","text":"<ul> <li>Check the system requirements to ensure that your system is capable of generating images.</li> <li>Check the <code>ram</code> setting in <code>invokeai.yaml</code>. This setting tells Invoke how much of your system RAM can be used to cache models. Having this too high or too low can slow things down. That said, it's generally safest to not set this at all and instead let Invoke manage it.</li> <li>Check the <code>vram</code> setting in <code>invokeai.yaml</code>. This setting tells Invoke how much of your GPU VRAM can be used to cache models. Counter-intuitively, if this setting is too high, Invoke will need to do a lot of shuffling of models as it juggles the VRAM cache and the currently-loaded model. The default value of 0.25 is generally works well for GPUs without 16GB or more VRAM. Even on a 24GB card, the default works well.</li> <li>Check that your generations are happening on your GPU (if you have one). InvokeAI will log what is being used for generation upon startup. If your GPU isn't used, re-install to ensure the correct versions of torch get installed.</li> <li>If you are on Windows, you may have exceeded your GPU's VRAM capacity and are using slower shared GPU memory. There's a guide to opt out of this behaviour in the linked FAQ entry.</li> </ul>"},{"location":"help/FAQ/#shared-gpu-memory-windows","title":"Shared GPU Memory (Windows)","text":"<p>Nvidia GPUs with driver 536.40</p> <p>This only applies to current Nvidia cards with driver 536.40 or later, released in June 2023.</p> <p>When the GPU doesn't have enough VRAM for a task, Windows is able to allocate some of its CPU RAM to the GPU. This is much slower than VRAM, but it does allow the system to generate when it otherwise might no have enough VRAM.</p> <p>When shared GPU memory is used, generation slows down dramatically - but at least it doesn't crash.</p> <p>If you'd like to opt out of this behavior and instead get an error when you exceed your GPU's VRAM, follow this guide from Nvidia.</p> <p>Here's how to get the python path required in the linked guide:</p> <ul> <li>Run <code>invoke.bat</code>.</li> <li>Select option 2 for developer console.</li> <li>At least one python path will be printed. Copy the path that includes your invoke installation directory (typically the first).</li> </ul>"},{"location":"help/FAQ/#installer-cannot-find-python-windows","title":"Installer cannot find python (Windows)","text":"<p>Ensure that you checked Add python.exe to PATH when installing Python. This can be found at the bottom of the Python Installer window. If you already have Python installed, you can re-run the python installer, choose the Modify option and check the box.</p>"},{"location":"help/FAQ/#triton-error-on-startup","title":"Triton error on startup","text":"<p>This can be safely ignored. InvokeAI doesn't use Triton, but if you are on Linux and wish to dismiss the error, you can install Triton.</p>"},{"location":"help/FAQ/#updated-to-340-and-xformers-cant-load-ccuda","title":"Updated to 3.4.0 and xformers can\u2019t load C++/CUDA","text":"<p>An issue occurred with your PyTorch update. Follow these steps to fix :</p> <ol> <li>Launch your invoke.bat / invoke.sh and select the option to open the developer console</li> <li>Run:<code>pip install \".[xformers]\" --upgrade --force-reinstall --extra-index-url https://download.pytorch.org/whl/cu121</code></li> <li>If you run into an error with <code>typing_extensions</code>, re-open the developer console and run: <code>pip install -U typing-extensions</code></li> </ol> <p>Note that v3.4.0 is an old, unsupported version. Please upgrade to the latest release.</p>"},{"location":"help/FAQ/#install-failed-and-says-pip-is-out-of-date","title":"Install failed and says <code>pip</code> is out of date","text":"<p>An out of date <code>pip</code> typically won't cause an installation to fail. The cause of the error can likely be found above the message that says <code>pip</code> is out of date.</p> <p>If you saw that warning but the install went well, don't worry about it (but you can update <code>pip</code> afterwards if you'd like).</p>"},{"location":"help/FAQ/#replicate-image-found-online","title":"Replicate image found online","text":"<p>Most example images with prompts that you'll find on the internet have been generated using different software, so you can't expect to get identical results. In order to reproduce an image, you need to replicate the exact settings and processing steps, including (but not limited to) the model, the positive and negative prompts, the seed, the sampler, the exact image size, any upscaling steps, etc.</p>"},{"location":"help/FAQ/#oserrors-on-windows-while-installing-dependencies","title":"OSErrors on Windows while installing dependencies","text":"<p>During a zip file installation or an update, installation stops with an error like this:</p> <p></p> <p>To resolve this, re-install the application as described above.</p>"},{"location":"help/FAQ/#huggingface-install-failed-due-to-invalid-access-token","title":"HuggingFace install failed due to invalid access token","text":"<p>Some HuggingFace models require you to authenticate using an access token.</p> <p>Invoke doesn't manage this token for you, but it's easy to set it up:</p> <ul> <li>Follow the instructions in the link above to create an access token. Copy it.</li> <li>Run the launcher script.</li> <li>Select option 2 (developer console).</li> <li>Paste the following command:</li> </ul> <pre><code>python -c \"import huggingface_hub; huggingface_hub.login()\"\n</code></pre> <ul> <li>Paste your access token when prompted and press Enter. You won't see anything when you paste it.</li> <li>Type <code>n</code> if prompted about git credentials.</li> </ul> <p>If you get an error, try the command again - maybe the token didn't paste correctly.</p> <p>Once your token is set, start Invoke and try downloading the model again. The installer will automatically use the access token.</p> <p>If the install still fails, you may not have access to the model.</p>"},{"location":"help/FAQ/#stable-diffusion-xl-generation-fails-after-trying-to-load-unet","title":"Stable Diffusion XL generation fails after trying to load UNet","text":"<p>InvokeAI is working in other respects, but when trying to generate images with Stable Diffusion XL you get a \"Server Error\". The text log in the launch window contains this log line above several more lines of error messages:</p> <p><code>INFO --&gt; Loading model:D:\\LONG\\PATH\\TO\\MODEL, type sdxl:main:unet</code></p> <p>This failure mode occurs when there is a network glitch during downloading the very large SDXL model.</p> <p>To address this, first go to the Model Manager and delete the Stable-Diffusion-XL-base-1.X model. Then, click the HuggingFace tab, paste the Repo ID stabilityai/stable-diffusion-xl-base-1.0 and install the model.</p>"},{"location":"help/FAQ/#package-dependency-conflicts-during-installation-or-update","title":"Package dependency conflicts during installation or update","text":"<p>If you have previously installed InvokeAI or another Stable Diffusion package, the installer may occasionally pick up outdated libraries and either the installer or <code>invoke</code> will fail with complaints about library conflicts.</p> <p>To resolve this, re-install the application as described above.</p>"},{"location":"help/FAQ/#invalid-configuration-file","title":"Invalid configuration file","text":"<p>Everything seems to install ok, you get a <code>ValidationError</code> when starting up the app.</p> <p>This is caused by an invalid setting in the <code>invokeai.yaml</code> configuration file. The error message should tell you what is wrong.</p> <p>Check the configuration docs for more detail about the settings and how to specify them.</p>"},{"location":"help/FAQ/#modulenotfounderror-no-module-named-controlnet_aux","title":"<code>ModuleNotFoundError: No module named 'controlnet_aux'</code>","text":"<p><code>controlnet_aux</code> is a dependency of Invoke and appears to have been packaged or distributed strangely. Sometimes, it doesn't install correctly. This is outside our control.</p> <p>If you encounter this error, the solution is to remove the package from the <code>pip</code> cache and re-run the Invoke installer so a fresh, working version of <code>controlnet_aux</code> can be downloaded and installed:</p> <ul> <li>Run the Invoke launcher</li> <li>Choose the developer console option</li> <li>Run this command: <code>pip cache remove controlnet_aux</code></li> <li>Close the terminal window</li> <li>Download and run the installer, selecting your current install location</li> </ul>"},{"location":"help/FAQ/#out-of-memory-issues","title":"Out of Memory Issues","text":"<p>The models are large, VRAM is expensive, and you may find yourself faced with Out of Memory errors when generating images. Here are some tips to reduce the problem:</p> <p>Optimizing for GPU VRAM</p> 4GB VRAM GPU6GB VRAM GPU12GB VRAM GPU <p>This should be adequate for 512x512 pixel images using Stable Diffusion 1.5 and derived models, provided that you do not use the NSFW checker. It won't be loaded unless you go into the UI settings and turn it on.</p> <p>If you are on a CUDA-enabled GPU, we will automatically use xformers or torch-sdp to reduce VRAM requirements, though you can explicitly configure this. See the configuration docs.</p> <p>This is a border case. Using the SD 1.5 series you should be able to generate images up to 640x640 with the NSFW checker enabled, and up to 1024x1024 with it disabled.</p> <p>If you run into persistent memory issues there are a series of environment variables that you can set before launching InvokeAI that alter how the PyTorch machine learning library manages memory. See https://pytorch.org/docs/stable/notes/cuda.html#memory-management for a list of these tweaks.</p> <p>This should be sufficient to generate larger images up to about 1280x1280.</p>"},{"location":"help/FAQ/#memory-leak-linux","title":"Memory Leak (Linux)","text":"<p>If you notice a memory leak, it could be caused to memory fragmentation as models are loaded and/or moved from CPU to GPU.</p> <p>A workaround is to tune memory allocation with an environment variable:</p> <pre><code># Force blocks &gt;1MB to be allocated with `mmap` so that they are released to the system immediately when they are freed.\nMALLOC_MMAP_THRESHOLD_=1048576\n</code></pre> <p>Speed vs Memory Tradeoff</p> <p>Your generations may be slower overall when setting this environment variable.</p> <p>Possibly dependent on <code>libc</code> implementation</p> <p>It's not known if this issue occurs with other <code>libc</code> implementations such as <code>musl</code>.</p> <p>If you encounter this issue and your system uses a different implementation, please try this environment variable and let us know if it fixes the issue.</p> Detailed Discussion <p>Python (and PyTorch) relies on the memory allocator from the C Standard Library (<code>libc</code>). On linux, with the GNU C Standard Library implementation (<code>glibc</code>), our memory access patterns have been observed to cause severe memory fragmentation.</p> <p>This fragmentation results in large amounts of memory that has been freed but can't be released back to the OS. Loading models from disk and moving them between CPU/CUDA seem to be the operations that contribute most to the fragmentation.</p> <p>This memory fragmentation issue can result in OOM crashes during frequent model switching, even if <code>ram</code> (the max RAM cache size) is set to a reasonable value (e.g. a OOM crash with <code>ram=16</code> on a system with 32GB of RAM).</p> <p>This problem may also exist on other OSes, and other <code>libc</code> implementations. But, at the time of writing, it has only been investigated on linux with <code>glibc</code>.</p> <p>To better understand how the <code>glibc</code> memory allocator works, see these references:</p> <ul> <li>Basics: https://www.gnu.org/software/libc/manual/html_node/The-GNU-Allocator.html</li> <li>Details: https://sourceware.org/glibc/wiki/MallocInternals</li> </ul> <p>Note the differences between memory allocated as chunks in an arena vs. memory allocated with <code>mmap</code>. Under <code>glibc</code>'s default configuration, most model tensors get allocated as chunks in an arena making them vulnerable to the problem of fragmentation.</p>"},{"location":"help/SAMPLER_CONVERGENCE/","title":"Sampler Convergence","text":"<p>As features keep increasing, making the right choices for your needs can become increasingly difficult. What sampler to use? And for how many steps? Do you change the CFG value? Do you use prompt weighting? Do you allow variations?</p> <p>Even once you have a result, do you blend it with other images? Pass it through <code>img2img</code>? With what strength? Do you use inpainting to correct small details? Outpainting to extend cropped sections?</p> <p>The purpose of this series of documents is to help you better understand these tools, so you can make the best out of them. Feel free to contribute with your own findings!</p> <p>In this document, we will talk about sampler convergence.</p> <p>Looking for a short version? Here's a TL;DR in 3 tables.</p> <p>Remember</p> <ul> <li>Results converge as steps (<code>-s</code>) are increased (except for <code>K_DPM_2_A</code> and <code>K_EULER_A</code>). Often at \u2265 <code>-s100</code>, but may require \u2265 <code>-s700</code>).</li> <li>Producing a batch of candidate images at low (<code>-s8</code> to <code>-s30</code>) step counts can save you hours of computation.</li> <li><code>K_HEUN</code> and <code>K_DPM_2</code>  converge in less steps (but are slower).</li> <li><code>K_DPM_2_A</code> and <code>K_EULER_A</code> incorporate a lot of creativity/variability.</li> </ul> Sampler (3 sample avg) it/s (M1 Max 64GB, 512x512) <code>DDIM</code> 1.89 <code>PLMS</code> 1.86 <code>K_EULER</code> 1.86 <code>K_LMS</code> 1.91 <code>K_HEUN</code> 0.95 (slower) <code>K_DPM_2</code> 0.95 (slower) <code>K_DPM_2_A</code> 0.95 (slower) <code>K_EULER_A</code> 1.86 <p>suggestions</p> <p>For most use cases, <code>K_LMS</code>, <code>K_HEUN</code> and <code>K_DPM_2</code> are the best choices (the latter 2 run 0.5x as quick, but tend to converge 2x as quick as <code>K_LMS</code>). At very low steps (\u2264 <code>-s8</code>), <code>K_HEUN</code> and <code>K_DPM_2</code> are not recommended. Use <code>K_LMS</code> instead.</p> <p>For variability, use <code>K_EULER_A</code> (runs 2x as quick as <code>K_DPM_2_A</code>).</p>"},{"location":"help/SAMPLER_CONVERGENCE/#sampler-results","title":"Sampler results","text":"<p>Let's start by choosing a prompt and using it with each of our 8 samplers, running it for 10, 20, 30, 40, 50 and 100 steps.</p> <p>Anime. <code>\"an anime girl\" -W512 -H512 -C7.5 -S3031912972</code></p> <p></p>"},{"location":"help/SAMPLER_CONVERGENCE/#sampler-convergence_1","title":"Sampler convergence","text":"<p>Immediately, you can notice results tend to converge -that is, as <code>-s</code> (step) values increase, images look more and more similar until there comes a point where the image no longer changes.</p> <p>You can also notice how <code>DDIM</code> and <code>PLMS</code> eventually tend to converge to K-sampler results as steps are increased. Among K-samplers, <code>K_HEUN</code> and <code>K_DPM_2</code> seem to require the fewest steps to converge, and even at low step counts they are good indicators of the final result. And finally, <code>K_DPM_2_A</code> and <code>K_EULER_A</code> seem to do a bit of their own thing and don't keep much similarity with the rest of the samplers.</p>"},{"location":"help/SAMPLER_CONVERGENCE/#batch-generation-speedup","title":"Batch generation speedup","text":"<p>This realization is very useful because it means you don't need to create a batch of 100 images (<code>-n100</code>) at <code>-s100</code> to choose your favorite 2 or 3 images. You can produce the same 100 images at <code>-s10</code> to <code>-s30</code> using a K-sampler (since they converge faster), get a rough idea of the final result, choose your 2 or 3 favorite ones, and then run <code>-s100</code> on those images to polish some details. The latter technique is 3-8x as quick.</p> <p>Example</p> <p>At 60s per 100 steps.</p> <p>A) 60s * 100 images = 6000s (100 images at <code>-s100</code>, manually picking 3 favorites)</p> <p>B) 6s 100 images + 60s 3 images = 780s (100 images at <code>-s10</code>, manually picking 3 favorites, and running those 3 at <code>-s100</code> to polish details)</p> <p>The result is 1 hour and 40 minutes for Variant A, vs 13 minutes for Variant B.</p>"},{"location":"help/SAMPLER_CONVERGENCE/#topic-convergance","title":"Topic convergance","text":"<p>Now, these results seem interesting, but do they hold for other topics? How about nature? Food? People? Animals? Let's try!</p> <p>Nature. <code>\"valley landscape wallpaper, d&amp;d art, fantasy, painted, 4k, high detail, sharp focus, washed colors, elaborate excellent painted illustration\" -W512 -H512 -C7.5 -S1458228930</code></p> <p></p> <p>With nature, you can see how initial results are even more indicative of final result -more so than with characters/people. <code>K_HEUN</code> and <code>K_DPM_2</code> are again the quickest indicators, almost right from the start. Results also converge faster (e.g. <code>K_HEUN</code> converged at <code>-s21</code>).</p> <p>Food. <code>\"a hamburger with a bowl of french fries\" -W512 -H512 -C7.5 -S4053222918</code></p> <p></p> <p>Again, <code>K_HEUN</code> and <code>K_DPM_2</code> take the fewest number of steps to be good indicators of the final result. <code>K_DPM_2_A</code> and <code>K_EULER_A</code> seem to incorporate a lot of creativity/variability, capable of producing rotten hamburgers, but also of adding lettuce to the mix. And they're the only samplers that produced an actual 'bowl of fries'!</p> <p>Animals. <code>\"grown tiger, full body\" -W512 -H512 -C7.5 -S3721629802</code></p> <p></p> <p><code>K_HEUN</code> and <code>K_DPM_2</code> once again require the least number of steps to be indicative of the final result (around <code>-s30</code>), while other samplers are still struggling with several tails or malformed back legs.</p> <p>It also takes longer to converge (for comparison, <code>K_HEUN</code> required around 150 steps to converge). This is normal, as producing human/animal faces/bodies is one of the things the model struggles the most with. For these topics, running for more steps will often increase coherence within the composition.</p> <p>People. <code>\"Ultra realistic photo, (Miranda Bloom-Kerr), young, stunning model, blue eyes, blond hair, beautiful face, intricate, highly detailed, smooth, art by artgerm and greg rutkowski and alphonse mucha, stained glass\" -W512 -H512 -C7.5 -S2131956332</code>. This time, we will go up to 300 steps.</p> <p></p> <p>Observing the results, it again takes longer for all samplers to converge (<code>K_HEUN</code> took around 150 steps), but we can observe good indicative results much earlier (see: <code>K_HEUN</code>). Conversely, <code>DDIM</code> and <code>PLMS</code> are still undergoing moderate changes (see: lace around her neck), even at <code>-s300</code>.</p> <p>In fact, as we can see in this other experiment, some samplers can take 700+ steps to converge when generating people.</p> <p></p> <p>Note also the point of convergence may not be the most desirable state (e.g. I prefer an earlier version of the face, more rounded), but it will probably be the most coherent arms/hands/face attributes-wise. You can always merge different images with a photo editing tool and pass it through <code>img2img</code> to smoothen the composition.</p>"},{"location":"help/SAMPLER_CONVERGENCE/#sampler-generation-times","title":"Sampler generation times","text":"<p>Once we understand the concept of sampler convergence, we must look into the performance of each sampler in terms of steps (iterations) per second, as not all samplers run at the same speed.</p> <p>On my M1 Max with 64GB of RAM, for a 512x512 image</p> Sampler (3 sample average) it/s <code>DDIM</code> 1.89 <code>PLMS</code> 1.86 <code>K_EULER</code> 1.86 <code>K_LMS</code> 1.91 <code>K_HEUN</code> 0.95 (slower) <code>K_DPM_2</code> 0.95 (slower) <code>K_DPM_2_A</code> 0.95 (slower) <code>K_EULER_A</code> 1.86 <p>Combining our results with the steps per second of each sampler, three choices come out on top: <code>K_LMS</code>, <code>K_HEUN</code> and <code>K_DPM_2</code> (where the latter two run 0.5x as quick but tend to converge 2x as quick as <code>K_LMS</code>). For creativity and a lot of variation between iterations, <code>K_EULER_A</code> can be a good choice (which runs 2x as quick as <code>K_DPM_2_A</code>).</p> <p>Additionally, image generation at very low steps (\u2264 <code>-s8</code>) is not recommended for <code>K_HEUN</code> and <code>K_DPM_2</code>. Use <code>K_LMS</code> instead.</p> <p></p>"},{"location":"help/SAMPLER_CONVERGENCE/#three-key-points","title":"Three key points","text":"<p>Finally, it is relevant to mention that, in general, there are 3 important moments in the process of image formation as steps increase:</p> <ul> <li> <p>The (earliest) point at which an image becomes a good indicator of the final result (useful for batch generation at low step values, to then improve the quality/coherence of the chosen images via running the same prompt and seed for more steps).</p> </li> <li> <p>The (earliest) point at which an image becomes coherent, even if different from the result if steps are increased (useful for batch generation at low step values, where quality/coherence is improved via techniques other than increasing the steps -e.g. via inpainting).</p> </li> <li> <p>The point at which an image fully converges.</p> </li> </ul> <p>Hence, remember that your workflow/strategy should define your optimal number of steps, even for the same prompt and seed (for example, if you seek full convergence, you may run <code>K_LMS</code> for <code>-s200</code> in the case of the red-haired girl, but <code>K_LMS</code> and <code>-s20</code>-taking one tenth the time- may do as well if your workflow includes adding small details, such as the missing shoulder strap, via <code>img2img</code>).</p>"},{"location":"help/diffusion/","title":"Diffusion Overview","text":"<p>Taking the time to understand the diffusion process will help you to understand how to more effectively use InvokeAI.</p> <p>There are two main ways Stable Diffusion works - with images, and latents.</p> <p>Image space represents images in pixel form that you look at. Latent space represents compressed inputs. It\u2019s in latent space that Stable Diffusion processes images. A VAE (Variational Auto Encoder) is responsible for compressing and encoding inputs into latent space, as well as decoding outputs back into image space.</p> <p>To fully understand the diffusion process, we need to understand a few more terms: UNet, CLIP, and conditioning.</p> <p>A U-Net is a model trained on a large number of latent images with with known amounts of random noise added.  This means that the U-Net can be given a slightly noisy image and it will predict the pattern of noise needed to subtract from the image in order to recover the original. </p> <p>CLIP is a model that tokenizes and encodes text into conditioning. This conditioning guides the model during the denoising steps to produce a new image. </p> <p>The U-Net and CLIP work together during the image generation process at each denoising step, with the U-Net removing noise in such a way that the result is similar to images in the U-Net\u2019s training set, while CLIP guides the U-Net towards creating images that are most similar to the prompt.</p> <p>When you generate an image using text-to-image, multiple steps occur in latent space: 1. Random noise is generated at the chosen height and width. The noise\u2019s characteristics are dictated by  seed. This noise tensor is passed into latent space. We\u2019ll call this noise A. 2. Using a model\u2019s U-Net, a noise predictor examines noise A, and the words tokenized by CLIP from your prompt (conditioning). It generates its own noise tensor to predict what the final image might look like in latent space. We\u2019ll call this noise B. 3. Noise B is subtracted from noise A in an attempt to create a latent image consistent with the prompt. This step is repeated for the number of sampler steps chosen. 4. The VAE decodes the final latent image from latent space into image space.</p> <p>Image-to-image is a similar process, with only step 1 being different: 1. The input image is encoded from image space into latent space by the VAE. Noise is then added to the input latent image. Denoising Strength dictates how many noise steps are added, and the amount of noise added at each step. A Denoising Strength of 0 means there are 0 steps and no noise added, resulting in an unchanged image, while a Denoising Strength of 1 results in the image being completely replaced with noise and a full set of denoising steps are performance. The process is then the same as steps 2-4 in the text-to-image process. </p> <p>Furthermore, a model provides the CLIP prompt tokenizer, the VAE, and a U-Net (where noise prediction occurs given a prompt and initial noise tensor).</p> <p>A noise scheduler (eg. DPM++ 2M Karras) schedules the subtraction of noise from the latent image across the sampler steps chosen (step 3 above). Less noise is usually subtracted at higher sampler steps. </p>"},{"location":"help/gettingStartedWithAI/","title":"Getting Started with AI Image Generation","text":"<p>New to image generation with AI? You\u2019re in the right place! </p> <p>This is a high level walkthrough of some of the concepts and terms you\u2019ll see as you start using InvokeAI. Please note, this is not an exhaustive guide and may be out of date due to the rapidly changing nature of the space. </p>"},{"location":"help/gettingStartedWithAI/#using-invokeai","title":"Using InvokeAI","text":""},{"location":"help/gettingStartedWithAI/#prompt-crafting","title":"Prompt Crafting","text":"<ul> <li>Prompts are the basis of using InvokeAI, providing the models directions on what to generate. As a general rule of thumb, the more detailed your prompt is, the better your result will be.</li> </ul> <p>To get started, here\u2019s an easy template to use for structuring your prompts:</p> <ul> <li>Subject, Style, Quality, Aesthetic<ul> <li>Subject: What your image will be about. E.g. \u201ca futuristic city with trains\u201d,  \u201cpenguins floating on icebergs\u201d, \u201cfriends sharing beers\u201d</li> <li>Style: The style or medium in which your image will be in. E.g. \u201cphotograph\u201d, \u201cpencil sketch\u201d, \u201coil paints\u201d, or \u201cpop art\u201d, \u201ccubism\u201d, \u201cabstract\u201d</li> <li>Quality: A particular aspect or trait that you would like to see emphasized in your image. E.g. \"award-winning\", \"featured in {relevant set of high quality works}\", \"professionally acclaimed\". Many people often use \"masterpiece\".</li> <li>Aesthetics: The visual impact and design of the artwork. This can be colors, mood, lighting, setting, etc.</li> </ul> </li> <li>There are two prompt boxes: Positive Prompt &amp; Negative Prompt.<ul> <li>A Positive Prompt includes words you want the model to reference when creating an image.</li> <li>Negative Prompt is for anything you want the model to eliminate when creating an image. It doesn\u2019t always interpret things exactly the way you would, but helps control the generation process. Always try to include a few terms - you can typically use lower quality image terms like \u201cblurry\u201d or \u201cdistorted\u201d with good success.</li> </ul> </li> <li>Some examples prompts you can try on your own:<ul> <li>A detailed oil painting of a tranquil forest at sunset with vibrant+ colors and soft, golden light filtering through the trees</li> <li>friends sharing beers in a busy city, realistic colored pencil sketch, twilight, masterpiece, bright, lively</li> </ul> </li> </ul>"},{"location":"help/gettingStartedWithAI/#generation-workflows","title":"Generation Workflows","text":"<ul> <li>Invoke offers a number of different workflows for interacting with models to produce images. Each is extremely powerful on its own, but together provide you an unparalleled way of producing high quality creative outputs that align with your vision.<ul> <li>Text to Image: The text to image tab focuses on the key workflow of using a prompt to generate a new image. It includes other features that help control the generation process as well.</li> <li>Image to Image: With image to image, you provide an image as a reference (called the \u201cinitial image\u201d), which provides more guidance around color and structure to the AI as it generates a new image. This is provided alongside the same features as Text to Image.</li> <li>Unified Canvas: The Unified Canvas is an advanced AI-first image editing tool that is easy to use, but hard to master. Drag an image onto the canvas from your gallery in order to regenerate certain elements, edit content or colors (known as inpainting), or extend the image with an exceptional degree of consistency and clarity (called outpainting).</li> </ul> </li> </ul>"},{"location":"help/gettingStartedWithAI/#improving-image-quality","title":"Improving Image Quality","text":"<ul> <li>Fine tuning your prompt - the more specific you are, the closer the image will turn out to what is in your head!  Adding more details in the Positive Prompt or Negative Prompt can help add / remove pieces of your image to improve it - You can also use advanced techniques like upweighting and downweighting to control the influence of certain words. Learn more here.<ul> <li>Tip: If you\u2019re seeing poor results, try adding the things you don\u2019t like about the image to your negative prompt may help. E.g. distorted, low quality, unrealistic, etc.</li> </ul> </li> <li>Explore different models - Other models can produce different results due to the data they\u2019ve been trained on. Each model has specific language and settings it works best with; a model\u2019s documentation is your friend here.  Play around with some and see what works best for you!</li> <li>Increasing Steps - The number of steps used controls how much time the model is given to produce an image, and depends on the \u201cScheduler\u201d used. The schedule controls how each step is processed by the model. More steps tends to mean better results, but will take longer - We recommend at least 30 steps for most</li> <li>Tweak and Iterate - Remember, it\u2019s best to change one thing at a time so you know what is working and what isn't. Sometimes you just need to try a new image, and other times using a new prompt might be the ticket. For testing, consider turning off the \u201crandom\u201d Seed - Using the same seed with the same settings will produce the same image, which makes it the perfect way to learn exactly what your changes are doing.</li> <li>Explore Advanced Settings - InvokeAI has a full suite of tools available to allow you complete control over your image creation process - Check out our docs if you want to learn more.</li> </ul>"},{"location":"help/gettingStartedWithAI/#terms-concepts","title":"Terms &amp; Concepts","text":"<p>If you're interested in learning more, check out this presentation from one of our maintainers (@lstein). </p>"},{"location":"help/gettingStartedWithAI/#stable-diffusion","title":"Stable Diffusion","text":"<p>Stable Diffusion is deep learning, text-to-image model that is the foundation of the capabilities found in InvokeAI. Since the release of Stable Diffusion, there have been many subsequent models created based on Stable Diffusion that are designed to generate specific types of images. </p>"},{"location":"help/gettingStartedWithAI/#prompts","title":"Prompts","text":"<p>Prompts provide the models directions on what to generate. As a general rule of thumb, the more detailed your prompt is, the better your result will be.</p>"},{"location":"help/gettingStartedWithAI/#models","title":"Models","text":"<p>Models are the magic that power InvokeAI. These files represent the output of training a machine on understanding massive amounts of images - providing them with the capability to generate new images using just a text description of what you\u2019d like to see. (Like Stable Diffusion!)</p> <p>Invoke offers a simple way to download several different models upon installation, but many more can be discovered online, including at https://models.invoke.ai </p> <p>Each model can produce a unique style of output, based on the images it was trained on - Try out different models to see which best fits your creative vision!</p> <ul> <li>Models that contain \u201cinpainting\u201d in the name are designed for use with the inpainting feature of the Unified Canvas</li> </ul>"},{"location":"help/gettingStartedWithAI/#scheduler","title":"Scheduler","text":"<p>Schedulers guide the process of removing noise (de-noising) from data. They determine:</p> <ol> <li>The number of steps to take to remove the noise.</li> <li>Whether the steps are random (stochastic) or predictable (deterministic).</li> <li>The specific method (algorithm) used for de-noising.</li> </ol> <p>Experimenting with different schedulers is recommended as each will produce different outputs!</p>"},{"location":"help/gettingStartedWithAI/#steps","title":"Steps","text":"<p>The number of de-noising steps each generation through. </p> <p>Schedulers can be intricate and there's often a balance to strike between how quickly they can de-noise data and how well they can do it. It's typically advised to experiment with different schedulers to see which one gives the best results. There has been a lot written on the internet about different schedulers, as well as exploring what the right level of \"steps\" are for each. You can save generation time by reducing the number of steps used, but you'll want to make sure that you are satisfied with the quality of images produced!</p>"},{"location":"help/gettingStartedWithAI/#low-rank-adaptations-loras","title":"Low-Rank Adaptations / LoRAs","text":"<p>Low-Rank Adaptations (LoRAs) are like a smaller, more focused version of models, intended to focus on training a better understanding of how a specific character, style, or concept looks.</p>"},{"location":"help/gettingStartedWithAI/#textual-inversion-embeddings","title":"Textual Inversion Embeddings","text":"<p>Textual Inversion Embeddings, like LoRAs, assist with more easily prompting for certain characters, styles, or concepts. However, embeddings are trained to update the relationship between a specific word (known as the \u201ctrigger\u201d) and the intended output. </p>"},{"location":"help/gettingStartedWithAI/#controlnet","title":"ControlNet","text":"<p>ControlNets are neural network models that are able to extract key features from an existing image and use these features to guide the output of the image generation model. </p>"},{"location":"help/gettingStartedWithAI/#vae","title":"VAE","text":"<p>Variational auto-encoder (VAE) is a encode/decode model that translates the \"latents\" image produced during the image generation procees to the large pixel images that we see. </p>"},{"location":"installation/010_INSTALL_AUTOMATED/","title":"Automatic Install &amp; Updates","text":"<p>The same packaged installer file can be used for both new installs and updates. Using the installer for updates will leave everything you've added since installation, and just update the core libraries used to run Invoke. Simply use the same path you installed to originally.</p> <p>Both release and pre-release versions can be installed using the installer. It also supports install through a wheel if needed.</p> <p>Be sure to review the installation requirements and ensure your system has everything it needs to install Invoke.</p>"},{"location":"installation/010_INSTALL_AUTOMATED/#getting-the-latest-installer","title":"Getting the Latest Installer","text":"<p>Download the <code>InvokeAI-installer-vX.Y.Z.zip</code> file from the latest release page. It is at the bottom of the page, under Assets.</p> <p>After unzipping the installer, you should have a <code>InvokeAI-Installer</code> folder with some files inside, including <code>install.bat</code> and <code>install.sh</code>.</p>"},{"location":"installation/010_INSTALL_AUTOMATED/#running-the-installer","title":"Running the Installer","text":"<p>Tip</p> <p>Windows users should first double-click the <code>WinLongPathsEnabled.reg</code> file to prevent a failed installation due to long file paths.</p> <p>Double-click the install script:</p> WindowsLinux/macOS <pre><code>install.bat\n</code></pre> <pre><code>install.sh\n</code></pre> <p>Running the Installer from the commandline</p> <p>You can also run the install script from cmd/powershell (Windows) or terminal (Linux/macOS).</p> <p>Untrusted Publisher (Windows)</p> <p>You may get a popup saying the file comes from an <code>Untrusted Publisher</code>. Click <code>More Info</code> and <code>Run Anyway</code> to get past this.</p> <p>The installation process is simple, with a few prompts:</p> <ul> <li>Select the version to install. Unless you have a specific reason to install a specific version, select the default (the latest version).</li> <li>Select location for the install. Be sure you have enough space in this folder for the base application, as described in the installation requirements.</li> <li>Select a GPU device.</li> </ul> <p>Slow Installation</p> <p>The installer needs to download several GB of data and install it all. It may appear to get stuck at 99.9% when installing <code>pytorch</code> or during a step labeled \"Installing collected packages\".</p> <p>If it is stuck for over 10 minutes, something has probably gone wrong and you should close the window and restart.</p>"},{"location":"installation/010_INSTALL_AUTOMATED/#running-the-application","title":"Running the Application","text":"<p>Find the install location you selected earlier. Double-click the launcher script to run the app:</p> WindowsLinux/macOS <pre><code>invoke.bat\n</code></pre> <pre><code>invoke.sh\n</code></pre> <p>Choose the first option to run the UI. After a series of startup messages, you'll see something like this:</p> <pre><code>Uvicorn running on http://127.0.0.1:9090 (Press CTRL+C to quit)\n</code></pre> <p>Copy the URL into your browser and you should see the UI.</p>"},{"location":"installation/010_INSTALL_AUTOMATED/#first-time-setup","title":"First-time Setup","text":"<p>You will need to install some models before you can generate.</p> <p>Check the configuration docs for details on configuring the application.</p>"},{"location":"installation/010_INSTALL_AUTOMATED/#updating","title":"Updating","text":"<p>Updating is exactly the same as installing - download the latest installer, choose the latest version and off you go.</p> <p>Dependency Resolution Issues</p> <p>We've found that pip's dependency resolution can cause issues when upgrading packages. One very common problem was pip \"downgrading\" torch from CUDA to CPU, but things broke in other novel ways.</p> <p>The installer doesn't have this kind of problem, so we use it for updating as well.</p>"},{"location":"installation/010_INSTALL_AUTOMATED/#installation-issues","title":"Installation Issues","text":"<p>If you have installation issues, please review the FAQ. You can also create an issue or ask for help on discord.</p>"},{"location":"installation/020_INSTALL_MANUAL/","title":"Manual Install","text":"<p>This is for Advanced Users</p> <p>Python experience is mandatory.</p>"},{"location":"installation/020_INSTALL_MANUAL/#introduction","title":"Introduction","text":"<p>InvokeAI is distributed as a python package on PyPI, installable with <code>pip</code>. There are a few things that are handled by the installer and launcher that you'll need to manage manually, described in this guide.</p>"},{"location":"installation/020_INSTALL_MANUAL/#requirements","title":"Requirements","text":"<p>Before you start, go through the installation requirements.</p>"},{"location":"installation/020_INSTALL_MANUAL/#installation-walkthrough","title":"Installation Walkthrough","text":"<ol> <li> <p>Create a directory to contain your InvokeAI library, configuration     files, and models. This is known as the \"runtime\" or \"root\"     directory, and often lives in your home directory under the name <code>invokeai</code>.</p> <p>We will refer to this directory as <code>INVOKEAI_ROOT</code>. For convenience, create an environment variable pointing to the directory.</p> Linux/macOSWindows (PowerShell) <pre><code>export INVOKEAI_ROOT=~/invokeai\nmkdir $INVOKEAI_ROOT\n</code></pre> <pre><code>Set-Variable -Name INVOKEAI_ROOT -Value $Home/invokeai\nmkdir $INVOKEAI_ROOT\n</code></pre> </li> <li> <p>Enter the root (invokeai) directory and create a virtual Python environment within it named <code>.venv</code>.</p> <p>Virtual Environment Location</p> <p>While you may create the virtual environment anywhere in the file system, we recommend that you create it within the root directory as shown here. This allows the application to automatically detect its data directories.</p> <p>If you choose a different location for the venv, then you must set the <code>INVOKEAI_ROOT</code> environment variable or specify the root directory using the <code>--root</code> CLI arg.</p> <pre><code>cd $INVOKEAI_ROOT\npython3 -m venv .venv --prompt InvokeAI\n</code></pre> </li> <li> <p>Activate the new environment:</p> Linux/macOSWindows <pre><code>source .venv/bin/activate\n</code></pre> <pre><code>.venv\\Scripts\\activate\n</code></pre> <p>Permissions Error (Windows)</p> <p>If you get a permissions error at this point, run this command and try again</p> <p><code>Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser</code></p> <p>The command-line prompt should change to to show <code>(InvokeAI)</code> at the beginning of the prompt.</p> <p>The following steps should be run while inside the <code>INVOKEAI_ROOT</code> directory.</p> </li> <li> <p>Make sure that pip is installed in your virtual environment and up to date:</p> <pre><code>python3 -m pip install --upgrade pip\n</code></pre> </li> <li> <p>Install the InvokeAI Package. The base command is <code>pip install InvokeAI --use-pep517</code>, but you may need to change this depending on your system and the desired features.</p> <ul> <li> <p>You may need to provide an extra index URL. Select your platform configuration using this tool on the PyTorch website. Copy the <code>--extra-index-url</code> string from this and append it to your install command.</p> <p>Install with an extra index URL</p> <pre><code>pip install InvokeAI --use-pep517 --extra-index-url https://download.pytorch.org/whl/cu121\n</code></pre> </li> <li> <p>If you have a CUDA GPU and want to install with <code>xformers</code>, you need to add an option to the package name. Note that <code>xformers</code> is not necessary. PyTorch includes an implementation of the SDP attention algorithm with the same performance.</p> <p>Install with <code>xformers</code></p> <pre><code>pip install \"InvokeAI[xformers]\" --use-pep517\n</code></pre> </li> </ul> </li> <li> <p>Deactivate and reactivate your runtime directory so that the invokeai-specific commands become available in the environment:</p> Linux/macOSWindows <pre><code>deactivate &amp;&amp; source .venv/bin/activate\n</code></pre> <pre><code>deactivate\n.venv\\Scripts\\activate\n</code></pre> </li> <li> <p>Run the application:</p> <p>Run <code>invokeai-web</code> to start the UI. You must activate the virtual environment before running the app.</p> <p>Warning</p> <p>If the virtual environment is not inside the root directory, then you must specify the path to the root directory with <code>--root \\path\\to\\invokeai</code> or the <code>INVOKEAI_ROOT</code> environment variable.</p> </li> </ol>"},{"location":"installation/040_INSTALL_DOCKER/","title":"Docker","text":"<p>macOS users</p> <p>Docker can not access the GPU on macOS, so your generation speeds will be slow. Install InvokeAI instead.</p> <p>Linux and Windows Users</p> <p>Configure Docker to access your machine's GPU. Docker Desktop on Windows includes GPU support. Linux users should follow the NVIDIA or AMD documentation.</p>"},{"location":"installation/040_INSTALL_DOCKER/#tldr","title":"TL;DR","text":"<p>Ensure your Docker setup is able to use your GPU. Then:</p> <pre><code>```bash\ndocker run --runtime=nvidia --gpus=all --publish 9090:9090 ghcr.io/invoke-ai/invokeai\n```\n</code></pre> <p>Once the container starts up, open http://localhost:9090 in your browser, install some models, and start generating.</p>"},{"location":"installation/040_INSTALL_DOCKER/#build-it-yourself","title":"Build-It-Yourself","text":"<p>All the docker materials are located inside the docker directory in the Git repo.</p> <pre><code>```bash\ncd docker\ncp .env.sample .env\ndocker compose up\n```\n</code></pre> <p>We also ship the <code>run.sh</code> convenience script. See the <code>docker/README.md</code> file for detailed instructions on how to customize the docker setup to your needs.</p>"},{"location":"installation/040_INSTALL_DOCKER/#prerequisites","title":"Prerequisites","text":""},{"location":"installation/040_INSTALL_DOCKER/#install-docker","title":"Install Docker","text":"<p>On the Docker Desktop app, go to Preferences, Resources, Advanced. Increase the CPUs and Memory to avoid this Issue. You may need to increase Swap and Disk image size too.</p>"},{"location":"installation/040_INSTALL_DOCKER/#setup","title":"Setup","text":"<p>Set up your environment variables. In the <code>docker</code> directory, make a copy of <code>.env.sample</code> and name it <code>.env</code>. Make changes as necessary.</p> <p>Any environment variables supported by InvokeAI can be set here - please see the CONFIGURATION for further detail.</p> <p>At a minimum, you might want to set the <code>INVOKEAI_ROOT</code> environment variable to point to the location where you wish to store your InvokeAI models, configuration, and outputs.</p> Environment-Variable  Default value  Description <code>INVOKEAI_ROOT</code> <code>~/invokeai</code> Required - the location of your InvokeAI root directory. It will be created if it does not exist. <code>HUGGING_FACE_HUB_TOKEN</code> InvokeAI will work without it, but some of the integrations with HuggingFace (like downloading from models from private repositories) may not work <code>GPU_DRIVER</code> <code>cuda</code> Optionally change this to <code>rocm</code> to build the image for AMD GPUs. NOTE: Use the <code>build.sh</code> script to build the image for this to take effect."},{"location":"installation/040_INSTALL_DOCKER/#build-the-image","title":"Build the Image","text":"<p>Use the standard <code>docker compose build</code> command from within the <code>docker</code> directory.</p> <p>If using an AMD GPU:     a: set the <code>GPU_DRIVER=rocm</code> environment variable in <code>docker-compose.yml</code> and continue using <code>docker compose build</code> as usual, or     b: set <code>GPU_DRIVER=rocm</code> in the <code>.env</code> file and use the <code>build.sh</code> script, provided for convenience</p>"},{"location":"installation/040_INSTALL_DOCKER/#run-the-container","title":"Run the Container","text":"<p>Use the standard <code>docker compose up</code> command, and generally the <code>docker compose</code> CLI as usual.</p> <p>Once the container starts up (and configures the InvokeAI root directory if this is a new installation), you can access InvokeAI at http://localhost:9090</p>"},{"location":"installation/040_INSTALL_DOCKER/#troubleshooting-faq","title":"Troubleshooting / FAQ","text":"<ul> <li>Q: I am running on Windows under WSL2, and am seeing a \"no such file or directory\" error.</li> <li>A: Your <code>docker-entrypoint.sh</code> might have has Windows (CRLF) line endings, depending how you cloned the repository.     To solve this, change the line endings in the <code>docker-entrypoint.sh</code> file to <code>LF</code>. You can do this in VSCode     (<code>Ctrl+P</code> and search for \"line endings\"), or by using the <code>dos2unix</code> utility in WSL.     Finally, you may delete <code>docker-entrypoint.sh</code> followed by  <code>git pull; git checkout docker/docker-entrypoint.sh</code>     to reset the file to its most recent version.     For more information on this issue, see Docker Desktop documentation</li> </ul>"},{"location":"installation/050_INSTALLING_MODELS/","title":"Installing Models","text":""},{"location":"installation/050_INSTALLING_MODELS/#checkpoint-and-diffusers-models","title":"Checkpoint and Diffusers Models","text":"<p>The model checkpoint files (<code>*.ckpt</code>) are the Stable Diffusion \"secret sauce\". They are the product of training the AI on millions of captioned images gathered from multiple sources.</p> <p>Originally there was only a single Stable Diffusion weights file, which many people named <code>model.ckpt</code>.</p> <p>Today, there are thousands of models, fine tuned to excel at specific styles, genres, or themes.</p> <p>Model Formats</p> <p>We also have two more popular model formats, both created HuggingFace:</p> <ul> <li><code>safetensors</code>: Single file, like <code>.ckpt</code> files. Prevents malware from lurking in a model.</li> <li><code>diffusers</code>: Splits the model components into separate files, allowing very fast loading.</li> </ul> <p>InvokeAI supports all three formats. Our backend will convert models to <code>diffusers</code> format before running them. This is a transparent process.</p>"},{"location":"installation/050_INSTALLING_MODELS/#starter-models","title":"Starter Models","text":"<p>When you first start InvokeAI, you'll see a popup prompting you to install some starter models from the Model Manager. Click the <code>Starter Models</code> tab to see the list.</p> <p>You'll find a collection of popular and high-quality models available for easy download.</p> <p>Some models carry license terms that limit their use in commercial applications or on public servers. It's your responsibility to adhere to the license terms.</p>"},{"location":"installation/050_INSTALLING_MODELS/#other-models","title":"Other Models","text":"<p>You can install other models using the Model Manager. You'll find tabs for the following install methods:</p> <ul> <li>URL or Local Path: Provide the path to a model on your computer, or a direct link to the model. Some sites require you to use an API token to download models, which you can set up in the config file.</li> <li>HuggingFace: Paste a HF Repo ID to install it. If there are multiple models in the repo, you'll get a list to choose from. Repo IDs look like this: <code>XpucT/Deliberate</code>. There is a copy button on each repo to copy the ID.</li> <li>Scan Folder: Scan a local folder for models. You can install all of the detected models in one click.</li> </ul> <p>Autoimport</p> <p>The dedicated autoimport folder is removed as of v4.0.0. You can do the same thing on the Scan Folder tab - paste the folder you'd like to import from and then click <code>Install All</code>.</p>"},{"location":"installation/050_INSTALLING_MODELS/#diffusers-models-in-hf-repo-subfolders","title":"Diffusers models in HF repo subfolders","text":"<p>HuggingFace repos can be structured in any way. Some model authors include multiple models within the same folder.</p> <p>In this situation, you may need to provide some additional information to identify the model you want, by adding <code>:subfolder_name</code> to the repo ID.</p> <p>Example</p> <p>Say you have a repo ID <code>monster-labs/control_v1p_sd15_qrcode_monster</code>, and the model you want is inside the <code>v2</code> subfolder.</p> <p>Add <code>:v2</code> to the repo ID and use that when installing the model: <code>monster-labs/control_v1p_sd15_qrcode_monster:v2</code></p>"},{"location":"installation/060_INSTALL_PATCHMATCH/","title":"Installing PyPatchMatch","text":"<p>pypatchmatch is a Python module for inpainting images. It is not needed to run InvokeAI, but it greatly improves the quality of inpainting and outpainting and is recommended.</p> <p>Unfortunately, it is a C++ optimized module and installation can be somewhat challenging. This guide leads you through the steps.</p>"},{"location":"installation/060_INSTALL_PATCHMATCH/#windows","title":"Windows","text":"<p>You're in luck! On Windows platforms PyPatchMatch will install automatically on Windows systems with no extra intervention.</p>"},{"location":"installation/060_INSTALL_PATCHMATCH/#macintosh","title":"Macintosh","text":"<p>You need to have opencv installed so that pypatchmatch can be built:</p> <pre><code>brew install opencv\n</code></pre> <p>The next time you start <code>invoke</code>, after successfully installing opencv, pypatchmatch will be built.</p>"},{"location":"installation/060_INSTALL_PATCHMATCH/#linux","title":"Linux","text":"<p>Prior to installing PyPatchMatch, you need to take the following steps:</p>"},{"location":"installation/060_INSTALL_PATCHMATCH/#debian-based-distros","title":"Debian Based Distros","text":"<ol> <li> <p>Install the <code>build-essential</code> tools:</p> <pre><code>sudo apt update\nsudo apt install build-essential\n</code></pre> </li> <li> <p>Install <code>opencv</code>:</p> <pre><code>sudo apt install python3-opencv libopencv-dev\n</code></pre> </li> <li> <p>Activate the environment you use for invokeai, either with <code>conda</code> or with a    virtual environment.</p> </li> <li> <p>Install pypatchmatch:</p> <pre><code>pip install pypatchmatch\n</code></pre> </li> <li> <p>Confirm that pypatchmatch is installed. At the command-line prompt enter    <code>python</code>, and then at the <code>&gt;&gt;&gt;</code> line type    <code>from patchmatch import patch_match</code>: It should look like the following:</p> <pre><code>Python 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; from patchmatch import patch_match\nCompiling and loading c extensions from \"/home/lstein/Projects/InvokeAI/.invokeai-env/src/pypatchmatch/patchmatch\".\nrm -rf build/obj libpatchmatch.so\nmkdir: created directory 'build/obj'\nmkdir: created directory 'build/obj/csrc/'\n[dep] csrc/masked_image.cpp ...\n[dep] csrc/nnf.cpp ...\n[dep] csrc/inpaint.cpp ...\n[dep] csrc/pyinterface.cpp ...\n[CC] csrc/pyinterface.cpp ...\n[CC] csrc/inpaint.cpp ...\n[CC] csrc/nnf.cpp ...\n[CC] csrc/masked_image.cpp ...\n[link] libpatchmatch.so ...\n</code></pre> </li> </ol>"},{"location":"installation/060_INSTALL_PATCHMATCH/#arch-based-distros","title":"Arch Based Distros","text":"<ol> <li> <p>Install the <code>base-devel</code> package:</p> <pre><code>sudo pacman -Syu\nsudo pacman -S --needed base-devel\n</code></pre> </li> <li> <p>Install <code>opencv</code> and <code>blas</code>:</p> <pre><code>sudo pacman -S opencv blas\n</code></pre> <p>or for CUDA support</p> <pre><code>sudo pacman -S opencv-cuda blas\n</code></pre> </li> <li> <p>Fix the naming of the <code>opencv</code> package configuration file:</p> <pre><code>cd /usr/lib/pkgconfig/\nln -sf opencv4.pc opencv.pc\n</code></pre> </li> </ol> <p>Next, Follow Steps 4-6 from the Debian Section above</p> <p>If you see no errors you're ready to go!</p>"},{"location":"installation/INSTALLATION/","title":"Installation and Updating Overview","text":"<p>Before installing, review the installation requirements to ensure your system is set up properly.</p> <p>See the FAQ for frequently-encountered installation issues.</p> <p>If you need more help, join our discord or create an issue.</p> Automatic Install &amp; Updates  <p>\u2705 The automatic install is the best way to run InvokeAI. Check out the installation guide to get started.</p> <p>\u2b06\ufe0f The same installer is also the best way to update InvokeAI - Simply rerun it for the same folder you installed to.</p> <p>The installation process simply manages installation for the core libraries &amp; application dependencies that run Invoke. Any models, images, or other assets in the Invoke root folder won't be affected by the installation process.</p> Manual Install <p>If you are familiar with python and want more control over the packages that are installed, you can install InvokeAI manually via PyPI.</p> <p>Updates are managed by reinstalling the latest version through PyPi.</p> Developer Install <p>If you want to contribute to InvokeAI, consult the developer install guide.</p> Docker Install <p>This method is recommended for those familiar with running Docker containers.</p> <p>We offer a method for creating Docker containers containing InvokeAI and its dependencies. This method is recommended for individuals with experience with Docker containers and understand the pluses and minuses of a container-based install.</p> <p>See the docker installation guide.</p> Other Installation Guides <ul> <li>PyPatchMatch</li> <li>Installing Models</li> </ul>"},{"location":"installation/INSTALL_DEVELOPMENT/","title":"Developer Install","text":"<p>Warning</p> <p>InvokeAI uses a SQLite database. By running on <code>main</code>, you accept responsibility for your database. This means making regular backups (especially before pulling) and/or fixing it yourself in the event that a PR introduces a schema change.</p> <p>If you don't need persistent backend storage, you can use an ephemeral in-memory database by setting <code>use_memory_db: true</code> in your <code>invokeai.yaml</code> file. You'll also want to set <code>scan_models_on_startup: true</code> so that your models are registered on startup.</p> <p>If this is untenable, you should run the application via the official installer or a manual install of the python package from PyPI. These releases will not break your database.</p> <p>If you have an interest in how InvokeAI works, or you would like to add features or bugfixes, you are encouraged to install the source code for InvokeAI.</p> <p>Why do I need the frontend toolchain?</p> <p>The repo doesn't contain a build of the frontend. You'll be responsible for rebuilding it (or running it in dev mode) to use the app, as described in the frontend dev toolchain docs.</p>  Installation  <ol> <li>Fork and clone the InvokeAI repo.</li> <li>Follow the manual installation docs to create a new virtual environment for the development install.</li> <li>Create a new folder outside the repo root for the installation and create the venv inside that folder.</li> <li>When installing the InvokeAI package, add <code>-e</code> to the command so you get an editable install.</li> <li>Install the frontend dev toolchain and do a production build of the UI as described.</li> <li>You can now run the app as described in the manual installation docs.</li> </ol> <p>As described in the frontend dev toolchain docs, you can run the UI using a dev server. If you do this, you won't need to continually rebuild the frontend. Instead, you run the dev server and use the app with the server URL it provides.</p>"},{"location":"installation/INSTALL_REQUIREMENTS/","title":"Requirements","text":""},{"location":"installation/INSTALL_REQUIREMENTS/#gpu","title":"GPU","text":"<p>Problematic Nvidia GPUs</p> <p>We do not recommend these GPUs. They cannot operate with half precision, but have insufficient VRAM to generate 512x512 images at full precision.</p> <ul> <li>NVIDIA 10xx series cards such as the 1080 TI</li> <li>GTX 1650 series cards</li> <li>GTX 1660 series cards</li> </ul> <p>Invoke runs best with a dedicated GPU, but will fall back to running on CPU, albeit much slower. You'll need a beefier GPU for SDXL.</p> <p>Stable Diffusion 1.5</p> NvidiaAMDMac <pre><code>Any GPU with at least 4GB VRAM.\n</code></pre> <pre><code>Any GPU with at least 4GB VRAM. Linux only.\n</code></pre> <pre><code>Any Apple Silicon Mac with at least 8GB memory.\n</code></pre> <p>Stable Diffusion XL</p> NvidiaAMDMac <pre><code>Any GPU with at least 8GB VRAM.\n</code></pre> <pre><code>Any GPU with at least 16GB VRAM. Linux only.\n</code></pre> <pre><code>Any Apple Silicon Mac with at least 16GB memory.\n</code></pre>"},{"location":"installation/INSTALL_REQUIREMENTS/#ram","title":"RAM","text":"<p>At least 12GB of RAM.</p>"},{"location":"installation/INSTALL_REQUIREMENTS/#disk","title":"Disk","text":"<p>SSDs will, of course, offer the best performance.</p> <p>The base application disk usage depends on the torch backend.</p> <p>Disk</p> Nvidia (CUDA)AMD (ROCm)Mac (MPS) <pre><code>~6.5GB\n</code></pre> <pre><code>~12GB\n</code></pre> <pre><code>~3.5GB\n</code></pre> <p>You'll need to set aside some space for images, depending on how much you generate. A couple GB is enough to get started.</p> <p>You'll need a good chunk of space for models. Even if you only install the most popular models and the usual support models (ControlNet, IP Adapter ,etc), you will quickly hit 50GB of models.</p> <p><code>tmpfs</code> on Linux</p> <p>If your temporary directory is mounted as a <code>tmpfs</code>, ensure it has sufficient space.</p>"},{"location":"installation/INSTALL_REQUIREMENTS/#python","title":"Python","text":"<p>Invoke requires python 3.10 or 3.11. If you don't already have one of these versions installed, we suggest installing 3.11, as it will be supported for longer.</p> <p>Check that your system has an up-to-date Python installed by running <code>python --version</code> in the terminal (Linux, macOS) or cmd/powershell (Windows).</p> Installing Python (Windows) <ul> <li>Install python 3.11 with an official installer.</li> <li>The installer includes an option to add python to your PATH. Be sure to enable this. If you missed it, re-run the installer, choose to modify an existing installation, and tick that checkbox.</li> <li>You may need to install Microsoft Visual C++ Redistributable.</li> </ul> Installing Python (macOS) <ul> <li>Install python 3.11 with an official installer.</li> <li>If model installs fail with a certificate error, you may need to run this command (changing the python version to match what you have installed): <code>/Applications/Python\\ 3.10/Install\\ Certificates.command</code></li> <li>If you haven't already, you will need to install the XCode CLI Tools by running <code>xcode-select --install</code> in a terminal.</li> </ul> Installing Python (Linux) <ul> <li>Follow the linux install instructions, being sure to install python 3.11.</li> <li>You'll need to install <code>libglib2.0-0</code> and <code>libgl1-mesa-glx</code> for OpenCV to work. For example, on a Debian system: <code>sudo apt update &amp;&amp; sudo apt install -y libglib2.0-0 libgl1-mesa-glx</code></li> </ul>"},{"location":"installation/INSTALL_REQUIREMENTS/#drivers","title":"Drivers","text":"<p>If you have an Nvidia or AMD GPU, you may need to manually install drivers or other support packages for things to work well or at all.</p>"},{"location":"installation/INSTALL_REQUIREMENTS/#nvidia","title":"Nvidia","text":"<p>Run <code>nvidia-smi</code> on your system's command line to verify that drivers and CUDA are installed. If this command fails, or doesn't report versions, you will need to install drivers.</p> <p>Go to the CUDA Toolkit Downloads and carefully follow the instructions for your system to get everything installed.</p> <p>Confirm that <code>nvidia-smi</code> displays driver and CUDA versions after installation.</p>"},{"location":"installation/INSTALL_REQUIREMENTS/#linux-via-nvidia-container-runtime","title":"Linux - via Nvidia Container Runtime","text":"<p>An alternative to installing CUDA locally is to use the Nvidia Container Runtime to run the application in a container.</p>"},{"location":"installation/INSTALL_REQUIREMENTS/#windows-nvidia-cudnn-dlls","title":"Windows - Nvidia cuDNN DLLs","text":"<p>An out-of-date cuDNN library can greatly hamper performance on 30-series and 40-series cards. Check with the community on discord to compare your <code>it/s</code> if you think you may need this fix.</p> <p>First, locate the destination for the DLL files and make a quick back up:</p> <ol> <li>Find your InvokeAI installation folder, e.g. <code>C:\\Users\\Username\\InvokeAI\\</code>.</li> <li>Open the <code>.venv</code> folder, e.g. <code>C:\\Users\\Username\\InvokeAI\\.venv</code> (you may need to show hidden files to see it).</li> <li>Navigate deeper to the <code>torch</code> package, e.g. <code>C:\\Users\\Username\\InvokeAI\\.venv\\Lib\\site-packages\\torch</code>.</li> <li>Copy the <code>lib</code> folder inside <code>torch</code> and back it up somewhere.</li> </ol> <p>Next, download and copy the updated cuDNN DLLs:</p> <ol> <li>Go to https://developer.nvidia.com/cudnn.</li> <li>Create an account if needed and log in.</li> <li>Choose the newest version of cuDNN that works with your GPU architecture. Consult the cuDNN support matrix to determine the correct version for your GPU.</li> <li>Download the latest version and extract it.</li> <li>Find the <code>bin</code> folder, e.g. <code>cudnn-windows-x86_64-SOME_VERSION\\bin</code>.</li> <li>Copy and paste the <code>.dll</code> files into the <code>lib</code> folder you located earlier. Replace files when prompted.</li> </ol> <p>If, after restarting the app, this doesn't improve your performance, either restore your back up or re-run the installer to reset <code>torch</code> back to its original state.</p>"},{"location":"installation/INSTALL_REQUIREMENTS/#amd","title":"AMD","text":"<p>Linux Only</p> <p>AMD GPUs are supported on Linux only, due to ROCm (the AMD equivalent of CUDA) support being Linux only.</p> <p>Bumps Ahead</p> <p>While the application does run on AMD GPUs, there are occasional bumps related to spotty torch support.</p> <p>Run <code>rocm-smi</code> on your system's command line verify that drivers and ROCm are installed. If this command fails, or doesn't report versions, you will need to install them.</p> <p>Go to the ROCm Documentation and carefully follow the instructions for your system to get everything installed.</p> <p>Confirm that <code>rocm-smi</code> displays driver and CUDA versions after installation.</p>"},{"location":"installation/INSTALL_REQUIREMENTS/#linux-via-docker-container","title":"Linux - via Docker Container","text":"<p>An alternative to installing ROCm locally is to use a ROCm docker container to run the application in a container.</p>"},{"location":"nodes/INVOCATION_API/","title":"Invocation API","text":"<p>Each invocation's <code>invoke</code> method is provided a single arg - the Invocation Context.</p> <p>This object provides access to various methods, used to interact with the application. Loading and saving images, logging messages, etc.</p> <p>This API may shift slightly until the release of v4.0.0 as we work through a few final updates to the Model Manager.</p> <pre><code>class MyInvocation(BaseInvocation):\n  ...\n  def invoke(self, context: InvocationContext) -&gt; ImageOutput:\n      image_pil = context.images.get_pil(image_name)\n      # Do something to the image\n      image_dto = context.images.save(image_pil)\n      # Log a message\n      context.logger.info(f\"Did something cool, image saved!\")\n      ...\n</code></pre> <p>The full API is documented below.</p>"},{"location":"nodes/INVOCATION_API/#invocation-mixins","title":"Invocation Mixins","text":"<p>Two important mixins are provided to facilitate working with metadata and gallery boards.</p>"},{"location":"nodes/INVOCATION_API/#withmetadata","title":"<code>WithMetadata</code>","text":"<p>Inherit from this class (in addition to <code>BaseInvocation</code>) to add a <code>metadata</code> input to your node. When you do this, you can access the metadata dict from <code>self.metadata</code> in the <code>invoke()</code> function.</p> <p>The dict will be populated via the node's input, and you can add any metadata you'd like to it. When you call <code>context.images.save()</code>, if the metadata dict has any data, it be automatically embedded in the image.</p>"},{"location":"nodes/INVOCATION_API/#withboard","title":"<code>WithBoard</code>","text":"<p>Inherit from this class (in addition to <code>BaseInvocation</code>) to add a <code>board</code> input to your node. This renders as a drop-down to select a board. The user's selection will be accessible from <code>self.board</code> in the <code>invoke()</code> function.</p> <p>When you call <code>context.images.save()</code>, if a board was selected, the image will added to that board as it is saved.</p>"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.InvocationContext","title":"InvocationContext","text":"<p>Provides access to various services and data for the current invocation.</p> <p>Attributes:</p> Name Type Description <code>images</code> <code>ImagesInterface</code> <p>Methods to save, get and update images and their metadata.</p> <code>tensors</code> <code>TensorsInterface</code> <p>Methods to save and get tensors, including image, noise, masks, and masked images.</p> <code>conditioning</code> <code>ConditioningInterface</code> <p>Methods to save and get conditioning data.</p> <code>models</code> <code>ModelsInterface</code> <p>Methods to check if a model exists, get a model, and get a model's info.</p> <code>logger</code> <code>LoggerInterface</code> <p>The app logger.</p> <code>config</code> <code>ConfigInterface</code> <p>The app config.</p> <code>util</code> <code>UtilInterface</code> <p>Utility methods, including a method to check if an invocation was canceled and step callbacks.</p> <code>boards</code> <code>BoardsInterface</code> <p>Methods to interact with boards.</p>"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.ImagesInterface","title":"ImagesInterface","text":""},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.ImagesInterface.get_dto","title":"get_dto","text":"<pre><code>get_dto(image_name: str) -&gt; ImageDTO\n</code></pre> <p>Gets an image as an ImageDTO object.</p> <p>Parameters:</p> Name Type Description Default <code>image_name</code> <code>str</code> <p>The name of the image to get.</p> required <p>Returns:</p> Type Description <code>ImageDTO</code> <p>The image as an ImageDTO object.</p>"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.ImagesInterface.get_metadata","title":"get_metadata","text":"<pre><code>get_metadata(image_name: str) -&gt; Optional[MetadataField]\n</code></pre> <p>Gets an image's metadata, if it has any.</p> <p>Parameters:</p> Name Type Description Default <code>image_name</code> <code>str</code> <p>The name of the image to get the metadata for.</p> required <p>Returns:</p> Type Description <code>Optional[MetadataField]</code> <p>The image's metadata, if it has any.</p>"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.ImagesInterface.get_path","title":"get_path","text":"<pre><code>get_path(image_name: str, thumbnail: bool = False) -&gt; Path\n</code></pre> <p>Gets the internal path to an image or thumbnail.</p> <p>Parameters:</p> Name Type Description Default <code>image_name</code> <code>str</code> <p>The name of the image to get the path of.</p> required <code>thumbnail</code> <code>bool</code> <p>Get the path of the thumbnail instead of the full image</p> <code>False</code> <p>Returns:</p> Type Description <code>Path</code> <p>The local path of the image or thumbnail.</p>"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.ImagesInterface.get_pil","title":"get_pil","text":"<pre><code>get_pil(image_name: str, mode: IMAGE_MODES | None = None) -&gt; Image\n</code></pre> <p>Gets an image as a PIL Image object.</p> <p>Parameters:</p> Name Type Description Default <code>image_name</code> <code>str</code> <p>The name of the image to get.</p> required <code>mode</code> <code>IMAGE_MODES | None</code> <p>The color mode to convert the image to. If None, the original mode is used.</p> <code>None</code> <p>Returns:</p> Type Description <code>Image</code> <p>The image as a PIL Image object.</p>"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.ImagesInterface.save","title":"save","text":"<pre><code>save(image: Image, board_id: Optional[str] = None, image_category: ImageCategory = ImageCategory.GENERAL, metadata: Optional[MetadataField] = None) -&gt; ImageDTO\n</code></pre> <p>Saves an image, returning its DTO.</p> <p>If the current queue item has a workflow or metadata, it is automatically saved with the image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The image to save, as a PIL image.</p> required <code>board_id</code> <code>Optional[str]</code> <p>The board ID to add the image to, if it should be added. It the invocation             inherits from <code>WithBoard</code>, that board will be used automatically. Use this only if             you want to override or provide a board manually!</p> <code>None</code> <code>image_category</code> <code>ImageCategory</code> <p>The category of the image. Only the GENERAL category is added             to the gallery.</p> <code>GENERAL</code> <code>metadata</code> <code>Optional[MetadataField]</code> <p>The metadata to save with the image, if it should have any. If the             invocation inherits from <code>WithMetadata</code>, that metadata will be used automatically.             Use this only if you want to override or provide metadata manually!</p> <code>None</code> <p>Returns:</p> Type Description <code>ImageDTO</code> <p>The saved image DTO.</p>"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.TensorsInterface","title":"TensorsInterface","text":""},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.TensorsInterface.load","title":"load","text":"<pre><code>load(name: str) -&gt; Tensor\n</code></pre> <p>Loads a tensor by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the tensor to load.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The loaded tensor.</p>"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.TensorsInterface.save","title":"save","text":"<pre><code>save(tensor: Tensor) -&gt; str\n</code></pre> <p>Saves a tensor, returning its name.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to save.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The name of the saved tensor.</p>"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.ConditioningInterface","title":"ConditioningInterface","text":""},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.ConditioningInterface.load","title":"load","text":"<pre><code>load(name: str) -&gt; ConditioningFieldData\n</code></pre> <p>Loads conditioning data by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the conditioning data to load.</p> required <p>Returns:</p> Type Description <code>ConditioningFieldData</code> <p>The loaded conditioning data.</p>"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.ConditioningInterface.save","title":"save","text":"<pre><code>save(conditioning_data: ConditioningFieldData) -&gt; str\n</code></pre> <p>Saves a conditioning data object, returning its name.</p> <p>Parameters:</p> Name Type Description Default <code>conditioning_data</code> <code>ConditioningFieldData</code> <p>The conditioning data to save.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The name of the saved conditioning data.</p>"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.ModelsInterface","title":"ModelsInterface","text":"<p>Common API for loading, downloading and managing models.</p>"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.ModelsInterface.download_and_cache_model","title":"download_and_cache_model","text":"<pre><code>download_and_cache_model(source: str | AnyHttpUrl) -&gt; Path\n</code></pre> <p>Download the model file located at source to the models cache and return its Path.</p> <p>This can be used to single-file install models and other resources of arbitrary types which should not get registered with the database. If the model is already installed, the cached path will be returned. Otherwise it will be downloaded.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str | AnyHttpUrl</code> <p>A URL that points to the model, or a huggingface repo_id.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path to the downloaded model</p>"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.ModelsInterface.exists","title":"exists","text":"<pre><code>exists(identifier: Union[str, ModelIdentifierField]) -&gt; bool\n</code></pre> <p>Check if a model exists.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, ModelIdentifierField]</code> <p>The key or ModelField representing the model.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the model exists, False if not.</p>"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.ModelsInterface.get_config","title":"get_config","text":"<pre><code>get_config(identifier: Union[str, ModelIdentifierField]) -&gt; AnyModelConfig\n</code></pre> <p>Get a model's config.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, ModelIdentifierField]</code> <p>The key or ModelField representing the model.</p> required <p>Returns:</p> Type Description <code>AnyModelConfig</code> <p>The model's config.</p>"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.ModelsInterface.load","title":"load","text":"<pre><code>load(identifier: Union[str, ModelIdentifierField], submodel_type: Optional[SubModelType] = None) -&gt; LoadedModel\n</code></pre> <p>Load a model.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, ModelIdentifierField]</code> <p>The key or ModelField representing the model.</p> required <code>submodel_type</code> <code>Optional[SubModelType]</code> <p>The submodel of the model to get.</p> <code>None</code> <p>Returns:</p> Type Description <code>LoadedModel</code> <p>An object representing the loaded model.</p>"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.ModelsInterface.load_by_attrs","title":"load_by_attrs","text":"<pre><code>load_by_attrs(name: str, base: BaseModelType, type: ModelType, submodel_type: Optional[SubModelType] = None) -&gt; LoadedModel\n</code></pre> <p>Load a model by its attributes.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model.</p> required <code>base</code> <code>BaseModelType</code> <p>The models' base type, e.g. <code>BaseModelType.StableDiffusion1</code>, <code>BaseModelType.StableDiffusionXL</code>, etc.</p> required <code>type</code> <code>ModelType</code> <p>Type of the model, e.g. <code>ModelType.Main</code>, <code>ModelType.Vae</code>, etc.</p> required <code>submodel_type</code> <code>Optional[SubModelType]</code> <p>The type of submodel to load, e.g. <code>SubModelType.UNet</code>, <code>SubModelType.TextEncoder</code>, etc. Only main</p> <code>None</code> <p>Returns:</p> Type Description <code>LoadedModel</code> <p>An object representing the loaded model.</p>"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.ModelsInterface.load_local_model","title":"load_local_model","text":"<pre><code>load_local_model(model_path: Path, loader: Optional[Callable[[Path], AnyModel]] = None) -&gt; LoadedModelWithoutConfig\n</code></pre> <p>Load the model file located at the indicated path</p> <p>If a loader callable is provided, it will be invoked to load the model. Otherwise, <code>safetensors.torch.load_file()</code> or <code>torch.load()</code> will be called to load the model.</p> <p>Be aware that the LoadedModelWithoutConfig object has no <code>config</code> attribute</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>A model Path</p> required <code>loader</code> <code>Optional[Callable[[Path], AnyModel]]</code> <p>A Callable that expects a Path and returns a dict[str|int, Any]</p> <code>None</code> <p>Returns:</p> Type Description <code>LoadedModelWithoutConfig</code> <p>A LoadedModelWithoutConfig object.</p>"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.ModelsInterface.load_remote_model","title":"load_remote_model","text":"<pre><code>load_remote_model(source: str | AnyHttpUrl, loader: Optional[Callable[[Path], AnyModel]] = None) -&gt; LoadedModelWithoutConfig\n</code></pre> <p>Download, cache, and load the model file located at the indicated URL or repo_id.</p> <p>If the model is already downloaded, it will be loaded from the cache.</p> <p>If the a loader callable is provided, it will be invoked to load the model. Otherwise, <code>safetensors.torch.load_file()</code> or <code>torch.load()</code> will be called to load the model.</p> <p>Be aware that the LoadedModelWithoutConfig object has no <code>config</code> attribute</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str | AnyHttpUrl</code> <p>A URL or huggingface repoid.</p> required <code>loader</code> <code>Optional[Callable[[Path], AnyModel]]</code> <p>A Callable that expects a Path and returns a dict[str|int, Any]</p> <code>None</code> <p>Returns:</p> Type Description <code>LoadedModelWithoutConfig</code> <p>A LoadedModelWithoutConfig object.</p>"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.ModelsInterface.search_by_attrs","title":"search_by_attrs","text":"<pre><code>search_by_attrs(name: Optional[str] = None, base: Optional[BaseModelType] = None, type: Optional[ModelType] = None, format: Optional[ModelFormat] = None) -&gt; list[AnyModelConfig]\n</code></pre> <p>Search for models by attributes.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>The name to search for (exact match).</p> <code>None</code> <code>base</code> <code>Optional[BaseModelType]</code> <p>The base to search for, e.g. <code>BaseModelType.StableDiffusion1</code>, <code>BaseModelType.StableDiffusionXL</code>, etc.</p> <code>None</code> <code>type</code> <code>Optional[ModelType]</code> <p>Type type of model to search for, e.g. <code>ModelType.Main</code>, <code>ModelType.Vae</code>, etc.</p> <code>None</code> <code>format</code> <code>Optional[ModelFormat]</code> <p>The format of model to search for, e.g. <code>ModelFormat.Checkpoint</code>, <code>ModelFormat.Diffusers</code>, etc.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[AnyModelConfig]</code> <p>A list of models that match the attributes.</p>"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.ModelsInterface.search_by_path","title":"search_by_path","text":"<pre><code>search_by_path(path: Path) -&gt; list[AnyModelConfig]\n</code></pre> <p>Search for models by path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path to search for.</p> required <p>Returns:</p> Type Description <code>list[AnyModelConfig]</code> <p>A list of models that match the path.</p>"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.LoggerInterface","title":"LoggerInterface","text":""},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.LoggerInterface.debug","title":"debug","text":"<pre><code>debug(message: str) -&gt; None\n</code></pre> <p>Logs a debug message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to log.</p> required"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.LoggerInterface.error","title":"error","text":"<pre><code>error(message: str) -&gt; None\n</code></pre> <p>Logs an error message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to log.</p> required"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.LoggerInterface.info","title":"info","text":"<pre><code>info(message: str) -&gt; None\n</code></pre> <p>Logs an info message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to log.</p> required"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.LoggerInterface.warning","title":"warning","text":"<pre><code>warning(message: str) -&gt; None\n</code></pre> <p>Logs a warning message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to log.</p> required"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.ConfigInterface","title":"ConfigInterface","text":""},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.ConfigInterface.get","title":"get","text":"<pre><code>get() -&gt; InvokeAIAppConfig\n</code></pre> <p>Gets the app's config.</p> <p>Returns:</p> Type Description <code>InvokeAIAppConfig</code> <p>The app's config.</p>"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.UtilInterface","title":"UtilInterface","text":""},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.UtilInterface.is_canceled","title":"is_canceled","text":"<pre><code>is_canceled() -&gt; bool\n</code></pre> <p>Checks if the current session has been canceled.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the current session has been canceled, False if not.</p>"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.UtilInterface.sd_step_callback","title":"sd_step_callback","text":"<pre><code>sd_step_callback(intermediate_state: PipelineIntermediateState, base_model: BaseModelType) -&gt; None\n</code></pre> <p>The step callback emits a progress event with the current step, the total number of steps, a preview image, and some other internal metadata.</p> <p>This should be called after each denoising step.</p> <p>Parameters:</p> Name Type Description Default <code>intermediate_state</code> <code>PipelineIntermediateState</code> <p>The intermediate state of the diffusion pipeline.</p> required <code>base_model</code> <code>BaseModelType</code> <p>The base model for the current denoising step.</p> required"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.UtilInterface.torch_device","title":"torch_device","text":"<pre><code>torch_device() -&gt; device\n</code></pre> <p>Return a torch device to use in the current invocation.</p> <p>Returns:</p> Type Description <code>device</code> <p>A torch.device not currently in use by the system.</p>"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.UtilInterface.torch_dtype","title":"torch_dtype","text":"<pre><code>torch_dtype(device: Optional[device] = None) -&gt; dtype\n</code></pre> <p>Return a precision type to use with the current invocation and torch device.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Optional[device]</code> <p>Optional device.</p> <code>None</code> <p>Returns:</p> Type Description <code>dtype</code> <p>A torch.dtype suited for the current device.</p>"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.BoardsInterface","title":"BoardsInterface","text":""},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.BoardsInterface.add_image_to_board","title":"add_image_to_board","text":"<pre><code>add_image_to_board(board_id: str, image_name: str) -&gt; None\n</code></pre> <p>Adds an image to a board.</p> <p>Parameters:</p> Name Type Description Default <code>board_id</code> <code>str</code> <p>The ID of the board to add the image to.</p> required <code>image_name</code> <code>str</code> <p>The name of the image to add to the board.</p> required"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.BoardsInterface.create","title":"create","text":"<pre><code>create(board_name: str) -&gt; BoardDTO\n</code></pre> <p>Creates a board.</p> <p>Parameters:</p> Name Type Description Default <code>board_name</code> <code>str</code> <p>The name of the board to create.</p> required <p>Returns:</p> Type Description <code>BoardDTO</code> <p>The created board DTO.</p>"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.BoardsInterface.get_all","title":"get_all","text":"<pre><code>get_all() -&gt; list[BoardDTO]\n</code></pre> <p>Gets all boards.</p> <p>Returns:</p> Type Description <code>list[BoardDTO]</code> <p>A list of all boards.</p>"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.BoardsInterface.get_all_image_names_for_board","title":"get_all_image_names_for_board","text":"<pre><code>get_all_image_names_for_board(board_id: str) -&gt; list[str]\n</code></pre> <p>Gets all image names for a board.</p> <p>Parameters:</p> Name Type Description Default <code>board_id</code> <code>str</code> <p>The ID of the board to get the image names for.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of all image names for the board.</p>"},{"location":"nodes/INVOCATION_API/#invokeai.app.services.shared.invocation_context.BoardsInterface.get_dto","title":"get_dto","text":"<pre><code>get_dto(board_id: str) -&gt; BoardDTO\n</code></pre> <p>Gets a board DTO.</p> <p>Parameters:</p> Name Type Description Default <code>board_id</code> <code>str</code> <p>The ID of the board to get.</p> required <p>Returns:</p> Type Description <code>BoardDTO</code> <p>The board DTO.</p>"},{"location":"nodes/NODES/","title":"Using the Workflow Editor","text":"<p>The workflow editor is a blank canvas allowing for the use of individual functions and image transformations to control the image generation workflow. Nodes take in inputs on the left side of the node, and return an output on the right side of the node. A node graph is composed of multiple nodes that are connected together to create a workflow. Nodes' inputs and outputs are connected by dragging connectors from node to node. Inputs and outputs are color coded for ease of use.</p> <p>If you're not familiar with Diffusion, take a look at our Diffusion Overview. Understanding how diffusion works will enable you to more easily use the Workflow Editor and build workflows to suit your needs.</p>"},{"location":"nodes/NODES/#features","title":"Features","text":""},{"location":"nodes/NODES/#workflow-library","title":"Workflow Library","text":"<p>The Workflow Library enables you to save workflows to the Invoke database, allowing you to easily creating, modify and share workflows as needed. </p> <p>A curated set of workflows are provided by default - these are designed to help explain important nodes' usage in the Workflow Editor.</p> <p></p>"},{"location":"nodes/NODES/#linear-view","title":"Linear View","text":"<p>The Workflow Editor allows you to create a UI for your workflow, to make it easier to iterate on your generations. </p> <p>To add an input to the Linear UI, right click on the input label and select \"Add to Linear View\".</p> <p>The Linear UI View will also be part of the saved workflow, allowing you share workflows and enable other to use them, regardless of complexity. </p> <p></p>"},{"location":"nodes/NODES/#renaming-fields-and-nodes","title":"Renaming Fields and Nodes","text":"<p>Any node or input field can be renamed in the workflow editor. If the input field you have renamed has been added to the Linear View, the changed name will be reflected in the Linear View and the node. </p>"},{"location":"nodes/NODES/#managing-nodes","title":"Managing Nodes","text":"<ul> <li>Ctrl+C to copy a node</li> <li>Ctrl+V to paste a node</li> <li>Backspace/Delete to delete a node</li> <li>Shift+Click to drag and select multiple nodes </li> </ul>"},{"location":"nodes/NODES/#node-caching","title":"Node Caching","text":"<p>Nodes have a \"Use Cache\" option in their footer. This allows for performance improvements by using the previously cached values during the workflow processing. </p>"},{"location":"nodes/NODES/#important-nodes-concepts","title":"Important Nodes &amp; Concepts","text":"<p>There are several node grouping concepts that can be examined with a narrow focus. These (and other) groupings can be pieced together to make up functional graph setups, and are important to understanding how groups of nodes work together as part of a whole. Note that the screenshots below aren't examples of complete functioning node graphs (see Examples).</p>"},{"location":"nodes/NODES/#noise","title":"Noise","text":"<p>An initial noise tensor is necessary for the latent diffusion process. As a result, the Denoising node requires a noise node input.  </p> <p></p>"},{"location":"nodes/NODES/#text-prompt-conditioning","title":"Text Prompt Conditioning","text":"<p>Conditioning is necessary for the latent diffusion process, whether empty or not. As a result, the Denoising node requires positive and negative conditioning inputs. Conditioning is reliant on a CLIP text encoder provided by the Model Loader node.</p> <p></p>"},{"location":"nodes/NODES/#image-to-latents-vae","title":"Image to Latents &amp; VAE","text":"<p>The ImageToLatents node takes in a pixel image and a VAE and outputs a latents. The LatentsToImage node does the opposite, taking in a latents and a VAE and outpus a pixel image. </p> <p></p>"},{"location":"nodes/NODES/#defined-random-seeds","title":"Defined &amp; Random Seeds","text":"<p>It is common to want to use both the same seed (for continuity) and random seeds (for variety). To define a seed, simply enter it into the 'Seed' field on a noise node. Conversely, the RandomInt node generates a random integer between 'Low' and 'High', and can be used as input to the 'Seed' edge point on a noise node to randomize your seed.</p> <p></p>"},{"location":"nodes/NODES/#controlnet","title":"ControlNet","text":"<p>The ControlNet node outputs a Control, which can be provided as input to a Denoise Latents node. Depending on the type of ControlNet desired, ControlNet nodes usually require an image processor node, such as a Canny Processor or Depth Processor, which prepares an input image for use with ControlNet.</p> <p></p>"},{"location":"nodes/NODES/#lora","title":"LoRA","text":"<p>The Lora Loader node lets you load a LoRA and pass it as output.A LoRA provides fine-tunes to the UNet and text encoder weights that augment the base model\u2019s image and text vocabularies.</p> <p></p>"},{"location":"nodes/NODES/#scaling","title":"Scaling","text":"<p>Use the ImageScale, ScaleLatents, and Upscale nodes to upscale images and/or latent images. Upscaling is the process of enlarging an image and adding more detail. The chosen method differs across contexts. However, be aware that latents are already noisy and compressed at their original resolution; scaling an image could produce more detailed results.</p> <p></p>"},{"location":"nodes/NODES/#iteration-multiple-images-as-input","title":"Iteration + Multiple Images as Input","text":"<p>Iteration is a common concept in any processing, and means to repeat a process with given input. In nodes, you're able to use the Iterate node to iterate through collections usually gathered by the Collect node. The Iterate node has many potential uses, from processing a collection of images one after another, to varying seeds across multiple image generations and more. This screenshot demonstrates how to collect several images and use them in an image generation workflow.</p> <p></p>"},{"location":"nodes/NODES/#batch-multiple-image-generation-random-seeds","title":"Batch / Multiple Image Generation + Random Seeds","text":"<p>Batch or multiple image generation in the workflow editor is done using the RandomRange node. In this case, the 'Size' field represents the number of images to generate, meaning this example will generate 4 images. As RandomRange produces a collection of integers, we need to add the Iterate node to iterate through the collection. This noise can then be fed to the Denoise Latents node for it to iterate through the denoising process with the different seeds provided.</p> <p></p>"},{"location":"nodes/NODES_MIGRATION_V3_V4/","title":"Invoke v4.0.0 Nodes API Migration guide","text":"<p>Invoke v4.0.0 is versioned as such due to breaking changes to the API utilized by nodes, both core and custom.</p>"},{"location":"nodes/NODES_MIGRATION_V3_V4/#motivation","title":"Motivation","text":"<p>Prior to v4.0.0, the <code>invokeai</code> python package has not be set up to be utilized as a library. That is to say, it didn't have any explicitly public API, and node authors had to work with the unstable internal application API.</p> <p>v4.0.0 introduces a stable public API for nodes.</p>"},{"location":"nodes/NODES_MIGRATION_V3_V4/#changes","title":"Changes","text":"<p>There are two node-author-facing changes:</p> <ol> <li>Import Paths</li> <li>Invocation Context API</li> </ol>"},{"location":"nodes/NODES_MIGRATION_V3_V4/#import-paths","title":"Import Paths","text":"<p>All public objects are now exported from <code>invokeai.invocation_api</code>:</p> <pre><code># Old\nfrom invokeai.app.invocations.baseinvocation import (\n    BaseInvocation,\n    InputField,\n    InvocationContext,\n    invocation,\n)\nfrom invokeai.app.invocations.primitives import ImageField\n\n# New\nfrom invokeai.invocation_api import (\n    BaseInvocation,\n    ImageField,\n    InputField,\n    InvocationContext,\n    invocation,\n)\n</code></pre> <p>It's possible that we've missed some classes you need in your node. Please let us know if that's the case.</p>"},{"location":"nodes/NODES_MIGRATION_V3_V4/#invocation-context-api","title":"Invocation Context API","text":"<p>Most nodes utilize the Invocation Context, an object that is passed to the <code>invoke</code> that provides access to data and services a node may need.</p> <p>Until now, that object and the services it exposed were internal. Exposing them to nodes means that changes to our internal implementation could break nodes. The methods on the services are also often fairly complicated and allowed nodes to footgun.</p> <p>In v4.0.0, this object has been refactored to be much simpler.</p> <p>See INVOCATION_API for full details of the API.</p> <p>This API may shift slightly until the release of v4.0.0 as we work through a few final updates to the Model Manager.</p>"},{"location":"nodes/NODES_MIGRATION_V3_V4/#improved-service-methods","title":"Improved Service Methods","text":"<p>The biggest offender was the image save method:</p> <pre><code># Old\nimage_dto = context.services.images.create(\n    image=image,\n    image_origin=ResourceOrigin.INTERNAL,\n    image_category=ImageCategory.GENERAL,\n    node_id=self.id,\n    session_id=context.graph_execution_state_id,\n    is_intermediate=self.is_intermediate,\n    metadata=self.metadata,\n    workflow=context.workflow,\n)\n\n# New\nimage_dto = context.images.save(image=image)\n</code></pre> <p>Other methods are simplified, or enhanced with additional functionality:</p> <pre><code># Old\nimage = context.services.images.get_pil_image(image_name)\n\n# New\nimage = context.images.get_pil(image_name)\nimage_cmyk = context.images.get_pil(image_name, \"CMYK\")\n</code></pre> <p>We also had some typing issues around tensors:</p> <pre><code># Old\n# `latents` typed as `torch.Tensor`, but could be `ConditioningFieldData`\nlatents = context.services.latents.get(self.latents.latents_name)\n# `data` typed as `torch.Tenssor,` but could be `ConditioningFieldData`\ncontext.services.latents.save(latents_name, data)\n\n# New - separate methods for tensors and conditioning data w/ correct typing\n# Also, the service generates the names\ntensor_name = context.tensors.save(tensor)\ntensor = context.tensors.load(tensor_name)\n# For conditioning\ncond_name = context.conditioning.save(cond_data)\ncond_data = context.conditioning.load(cond_name)\n</code></pre>"},{"location":"nodes/NODES_MIGRATION_V3_V4/#output-construction","title":"Output Construction","text":"<p>Core Outputs have builder functions right on them - no need to manually construct these objects, or use an extra utility:</p> <pre><code># Old\nimage_output = ImageOutput(\n    image=ImageField(image_name=image_dto.image_name),\n    width=image_dto.width,\n    height=image_dto.height,\n)\nlatents_output = build_latents_output(latents_name=name, latents=latents, seed=None)\nnoise_output = NoiseOutput(\n    noise=LatentsField(latents_name=latents_name, seed=seed),\n    width=latents.size()[3] * 8,\n    height=latents.size()[2] * 8,\n)\ncond_output = ConditioningOutput(\n    conditioning=ConditioningField(\n        conditioning_name=conditioning_name,\n    ),\n)\n\n# New\nimage_output = ImageOutput.build(image_dto)\nlatents_output = LatentsOutput.build(latents_name=name, latents=noise, seed=self.seed)\nnoise_output = NoiseOutput.build(latents_name=name, latents=noise, seed=self.seed)\ncond_output = ConditioningOutput.build(conditioning_name)\n</code></pre> <p>You can still create the objects using constructors if you want, but we suggest using the builder methods.</p>"},{"location":"nodes/comfyToInvoke/","title":"ComfyUI to InvokeAI","text":"<p>If you're coming to InvokeAI from ComfyUI, welcome! You'll find things are similar but different - the good news is that you already know how things should work, and it's just a matter of wiring them up! </p> <p>Some things to note: </p> <ul> <li>InvokeAI's nodes tend to be more granular than default nodes in Comfy. This means each node in Invoke will do a specific task and you might need to use multiple nodes to achieve the same result. The added granularity improves the control you have have over your workflows. </li> <li>InvokeAI's backend and ComfyUI's backend are very different which means Comfy workflows are not able to be imported into InvokeAI. However, we have created a list of popular workflows for you to get started with Nodes in InvokeAI!</li> </ul>"},{"location":"nodes/comfyToInvoke/#node-equivalents","title":"Node Equivalents:","text":"Comfy UI Category ComfyUI Node Invoke Equivalent Sampling KSampler Denoise Latents Sampling Ksampler Advanced Denoise Latents Loaders Load Checkpoint Main Model Loader or SDXL Main Model Loader Loaders Load VAE VAE Loader Loaders Load Lora LoRA Loader or SDXL Lora Loader Loaders Load ControlNet Model ControlNet Loaders Load ControlNet Model (diff) ControlNet Loaders Load Style Model Reference Only ControlNet will be coming in a future version of InvokeAI Loaders unCLIPCheckpointLoader N/A Loaders GLIGENLoader N/A Loaders Hypernetwork Loader N/A Loaders Load Upscale Model Occurs within \"Upscale (RealESRGAN)\" Conditioning CLIP Text Encode (Prompt) Compel (Prompt) or SDXL Compel (Prompt) Conditioning CLIP Set Last Layer CLIP Skip Conditioning Conditioning (Average) Use the .blend() feature of prompts Conditioning Conditioning (Combine) N/A Conditioning Conditioning (Concat) See the Prompt Tools Community Node Conditioning Conditioning (Set Area) N/A Conditioning Conditioning (Set Mask) Mask Edge Conditioning CLIP Vision Encode N/A Conditioning unCLIPConditioning N/A Conditioning Apply ControlNet ControlNet Conditioning Apply ControlNet (Advanced) ControlNet Latent VAE Decode Latents to Image Latent VAE Encode Image to Latents Latent Empty Latent Image Noise Latent Upscale Latent Resize Latents Latent Upscale Latent By Scale Latents Latent Latent Composite Blend Latents Latent LatentCompositeMasked N/A Image Save Image Image Image Preview Image Current Image Load Image Image Image Empty Image Blank Image Image Invert Image Invert Lerp Image Image Batch Images Link \"Image\" nodes into an \"Image Collection\" node Image Pad Image for Outpainting Outpainting is easily accomplished in the Unified Canvas Image ImageCompositeMasked Paste Image Image Upscale Image Resize Image Image Upscale Image By Upscale Image Image Upscale Image (using Model) Upscale Image Image ImageBlur Blur Image Image ImageQuantize N/A Image ImageSharpen N/A Image Canny Canny Processor Mask Load Image (as Mask) Image Mask Convert Mask to Image Image Mask Convert Image to Mask Image Mask SolidMask N/A Mask InvertMask Invert Lerp Image Mask CropMask Crop Image Mask MaskComposite Combine Mask Mask FeatherMask Blur Image Advanced Load CLIP Main Model Loader or SDXL Main Model Loader Advanced UNETLoader Main Model Loader or SDXL Main Model Loader Advanced DualCLIPLoader Main Model Loader or SDXL Main Model Loader Advanced Load Checkpoint Main Model Loader or SDXL Main Model Loader Advanced ConditioningZeroOut N/A Advanced ConditioningSetTimestepRange N/A Advanced CLIPTextEncodeSDXLRefiner Compel (Prompt) or SDXL Compel (Prompt) Advanced CLIPTextEncodeSDXL Compel (Prompt) or SDXL Compel (Prompt) Advanced ModelMergeSimple Model Merging is available in the Model Manager Advanced ModelMergeBlocks Model Merging is available in the Model Manager Advanced CheckpointSave Model saving is available in the Model Manager Advanced CLIPMergeSimple N/A"},{"location":"nodes/communityNodes/","title":"Community Nodes","text":"<p>These are nodes that have been developed by the community, for the community. If you're not sure what a node is, you can learn more about nodes here.</p> <p>If you'd like to submit a node for the community, please refer to the node creation overview.</p> <p>To use a node, add the node to the <code>nodes</code> folder found in your InvokeAI install location. </p> <p>The suggested method is to use <code>git clone</code> to clone the repository the node is found in. This allows for easy updates of the node in the future. </p> <p>If you'd prefer, you can also just download the whole node folder from the linked repository and add it to the <code>nodes</code> folder. </p> <p>To use a community workflow, download the the <code>.json</code> node graph file and load it into Invoke AI via the Load Workflow button in the Workflow Editor. </p> <ul> <li>Community Nodes<ul> <li>Adapters-Linked</li> <li>Autostereogram</li> <li>Average Images</li> <li>Clean Image Artifacts After Cut</li> <li>Close Color Mask </li> <li>Clothing Mask</li> <li>Contrast Limited Adaptive Histogram Equalization</li> <li>Depth Map from Wavefront OBJ</li> <li>Film Grain</li> <li>Generative Grammar-Based Prompt Nodes</li> <li>GPT2RandomPromptMaker</li> <li>Grid to Gif</li> <li>Halftone</li> <li>Hand Refiner with MeshGraphormer</li> <li>Image and Mask Composition Pack</li> <li>Image Dominant Color</li> <li>Image to Character Art Image Nodes</li> <li>Image Picker</li> <li>Image Resize Plus</li> <li>Latent Upscale</li> <li>Load Video Frame</li> <li>Make 3D</li> <li>Mask Operations</li> <li>Match Histogram</li> <li>Metadata-Linked</li> <li>Negative Image</li> <li>Nightmare Promptgen </li> <li>Oobabooga</li> <li>Prompt Tools</li> <li>Remote Image</li> <li>BriaAI Background Remove</li> <li>Remove Background </li> <li>Retroize</li> <li>Size Stepper Nodes</li> <li>Simple Skin Detection</li> <li>Text font to Image</li> <li>Thresholding</li> <li>Unsharp Mask</li> <li>XY Image to Grid and Images to Grids nodes</li> </ul> </li> <li>Example Node Template</li> <li>Disclaimer</li> <li>Help</li> </ul>"},{"location":"nodes/communityNodes/#adapters-linked-nodes","title":"Adapters Linked Nodes","text":"<p>Description: A set of nodes for linked adapters (ControlNet, IP-Adaptor &amp; T2I-Adapter). This allows multiple adapters to be chained together without using a <code>collect</code> node which means it can be used inside an <code>iterate</code> node without any collecting on every iteration issues.</p> <ul> <li><code>ControlNet-Linked</code> - Collects ControlNet info to pass to other nodes.</li> <li><code>IP-Adapter-Linked</code> - Collects IP-Adapter info to pass to other nodes.</li> <li><code>T2I-Adapter-Linked</code> - Collects T2I-Adapter info to pass to other nodes.</li> </ul> <p>Note: These are inherited from the core nodes so any update to the core nodes should be reflected in these. </p> <p>Node Link: https://github.com/skunkworxdark/adapters-linked-nodes</p>"},{"location":"nodes/communityNodes/#autostereogram-nodes","title":"Autostereogram Nodes","text":"<p>Description: Generate autostereogram images from a depth map. This is not a very practically useful node but more a 90s nostalgic indulgence as I used to love these images as a kid.</p> <p>Node Link: https://github.com/skunkworxdark/autostereogram_nodes</p> <p>Example Usage:  -&gt;  -&gt;  </p>"},{"location":"nodes/communityNodes/#average-images","title":"Average Images","text":"<p>Description: This node takes in a collection of images of the same size and averages them as output. It converts everything to RGB mode first.</p> <p>Node Link: https://github.com/JPPhoto/average-images-node</p>"},{"location":"nodes/communityNodes/#clean-image-artifacts-after-cut","title":"Clean Image Artifacts After Cut","text":"<p>Description: Removes residual artifacts after an image is separated from its background.</p> <p>Node Link: https://github.com/VeyDlin/clean-artifact-after-cut-node</p> <p>View: </p>"},{"location":"nodes/communityNodes/#close-color-mask","title":"Close Color Mask","text":"<p>Description: Generates a mask for images based on a closely matching color, useful for color-based selections.</p> <p>Node Link: https://github.com/VeyDlin/close-color-mask-node</p> <p>View: </p>"},{"location":"nodes/communityNodes/#clothing-mask","title":"Clothing Mask","text":"<p>Description: Employs a U2NET neural network trained for the segmentation of clothing items in images.</p> <p>Node Link: https://github.com/VeyDlin/clothing-mask-node</p> <p>View: </p>"},{"location":"nodes/communityNodes/#contrast-limited-adaptive-histogram-equalization","title":"Contrast Limited Adaptive Histogram Equalization","text":"<p>Description: Enhances local image contrast using adaptive histogram equalization with contrast limiting.</p> <p>Node Link: https://github.com/VeyDlin/clahe-node</p> <p>View: </p>"},{"location":"nodes/communityNodes/#depth-map-from-wavefront-obj","title":"Depth Map from Wavefront OBJ","text":"<p>Description: Render depth maps from Wavefront .obj files (triangulated) using this simple 3D renderer utilizing numpy and matplotlib to compute and color the scene. There are simple parameters to change the FOV, camera position, and model orientation.</p> <p>To be imported, an .obj must use triangulated meshes, so make sure to enable that option if exporting from a 3D modeling program. This renderer makes each triangle a solid color based on its average depth, so it will cause anomalies if your .obj has large triangles. In Blender, the Remesh modifier can be helpful to subdivide a mesh into small pieces that work well given these limitations.</p> <p>Node Link: https://github.com/dwringer/depth-from-obj-node</p> <p>Example Usage: </p>"},{"location":"nodes/communityNodes/#film-grain","title":"Film Grain","text":"<p>Description: This node adds a film grain effect to the input image based on the weights, seeds, and blur radii parameters. It works with RGB input images only.</p> <p>Node Link: https://github.com/JPPhoto/film-grain-node</p>"},{"location":"nodes/communityNodes/#generative-grammar-based-prompt-nodes","title":"Generative Grammar-Based Prompt Nodes","text":"<p>Description: This set of 3 nodes generates prompts from simple user-defined grammar rules (loaded from custom files - examples provided below). The prompts are made by recursively expanding a special template string, replacing nonterminal \"parts-of-speech\" until no nonterminal terms remain in the string.</p> <p>This includes 3 Nodes: - Lookup Table from File - loads a YAML file \"prompt\" section (or of a whole folder of YAML's) into a JSON-ified dictionary (Lookups output) - Lookups Entry from Prompt - places a single entry in a new Lookups output under the specified heading - Prompt from Lookup Table - uses a Collection of Lookups as grammar rules from which to randomly generate prompts.</p> <p>Node Link: https://github.com/dwringer/generative-grammar-prompt-nodes</p> <p>Example Usage: </p>"},{"location":"nodes/communityNodes/#gpt2randompromptmaker","title":"GPT2RandomPromptMaker","text":"<p>Description: A node for InvokeAI utilizes the GPT-2 language model to generate random prompts based on a provided seed and context.</p> <p>Node Link: https://github.com/mickr777/GPT2RandomPromptMaker</p> <p>Output Examples </p> <p>Generated Prompt: An enchanted weapon will be usable by any character regardless of their alignment.</p> <p></p>"},{"location":"nodes/communityNodes/#grid-to-gif","title":"Grid to Gif","text":"<p>Description: One node that turns a grid image into an image collection, one node that turns an image collection into a gif.</p> <p>Node Link: https://github.com/mildmisery/invokeai-GridToGifNode/blob/main/GridToGif.py</p> <p>Example Node Graph: https://github.com/mildmisery/invokeai-GridToGifNode/blob/main/Grid%20to%20Gif%20Example%20Workflow.json</p> <p>Output Examples </p> <p> </p>"},{"location":"nodes/communityNodes/#halftone","title":"Halftone","text":"<p>Description: Halftone converts the source image to grayscale and then performs halftoning. CMYK Halftone converts the image to CMYK and applies a per-channel halftoning to make the source image look like a magazine or newspaper. For both nodes, you can specify angles and halftone dot spacing.</p> <p>Node Link: https://github.com/JPPhoto/halftone-node</p> <p>Example</p> <p>Input:</p> <p></p> <p>Halftone Output:</p> <p></p> <p>CMYK Halftone Output:</p> <p></p>"},{"location":"nodes/communityNodes/#hand-refiner-with-meshgraphormer","title":"Hand Refiner with MeshGraphormer","text":"<p>Description: Hand Refiner takes in your image and automatically generates a fixed depth map for the hands along with a mask of the hands region that will conveniently allow you to use them along with ControlNet to fix the wonky hands generated by Stable Diffusion</p> <p>Node Link: https://github.com/blessedcoolant/invoke_meshgraphormer</p> <p>View </p>"},{"location":"nodes/communityNodes/#image-and-mask-composition-pack","title":"Image and Mask Composition Pack","text":"<p>Description: This is a pack of nodes for composing masks and images, including a simple text mask creator and both image and latent offset nodes. The offsets wrap around, so these can be used in conjunction with the Seamless node to progressively generate centered on different parts of the seamless tiling.</p> <p>This includes 15 Nodes:</p> <ul> <li>Adjust Image Hue Plus - Rotate the hue of an image in one of several different color spaces.</li> <li>Blend Latents/Noise (Masked) - Use a mask to blend part of one latents tensor [including Noise outputs] into another. Can be used to \"renoise\" sections during a multi-stage [masked] denoising process.</li> <li>Enhance Image - Boost or reduce color saturation, contrast, brightness, sharpness, or invert colors of any image at any stage with this simple wrapper for pillow [PIL]'s ImageEnhance module.</li> <li>Equivalent Achromatic Lightness - Calculates image lightness accounting for Helmholtz-Kohlrausch effect based on a method described by High, Green, and Nussbaum (2023).</li> <li>Text to Mask (Clipseg) - Input a prompt and an image to generate a mask representing areas of the image matched by the prompt.</li> <li>Text to Mask Advanced (Clipseg) - Output up to four prompt masks combined with logical \"and\", logical \"or\", or as separate channels of an RGBA image.</li> <li>Image Layer Blend - Perform a layered blend of two images using alpha compositing. Opacity of top layer is selectable, with optional mask and several different blend modes/color spaces.</li> <li>Image Compositor - Take a subject from an image with a flat backdrop and layer it on another image using a chroma key or flood select background removal.</li> <li>Image Dilate or Erode - Dilate or expand a mask (or any image!). This is equivalent to an expand/contract operation.</li> <li>Image Value Thresholds - Clip an image to pure black/white beyond specified thresholds.</li> <li>Offset Latents - Offset a latents tensor in the vertical and/or horizontal dimensions, wrapping it around.</li> <li>Offset Image - Offset an image in the vertical and/or horizontal dimensions, wrapping it around.</li> <li>Rotate/Flip Image - Rotate an image in degrees clockwise/counterclockwise about its center, optionally resizing the image boundaries to fit, or flipping it about the vertical and/or horizontal axes.</li> <li>Shadows/Highlights/Midtones - Extract three masks (with adjustable hard or soft thresholds) representing shadows, midtones, and highlights regions of an image.</li> <li>Text Mask (simple 2D) - create and position a white on black (or black on white) line of text using any font locally available to Invoke.</li> </ul> <p>Node Link: https://github.com/dwringer/composition-nodes</p> <p></p>"},{"location":"nodes/communityNodes/#image-dominant-color","title":"Image Dominant Color","text":"<p>Description: Identifies and extracts the dominant color from an image using k-means clustering.</p> <p>Node Link: https://github.com/VeyDlin/image-dominant-color-node</p> <p>View: </p>"},{"location":"nodes/communityNodes/#image-to-character-art-image-nodes","title":"Image to Character Art Image Nodes","text":"<p>Description: Group of nodes to convert an input image into ascii/unicode art Image</p> <p>Node Link: https://github.com/mickr777/imagetoasciiimage</p> <p>Output Examples</p> <p> </p>"},{"location":"nodes/communityNodes/#image-picker","title":"Image Picker","text":"<p>Description: This InvokeAI node takes in a collection of images and randomly chooses one. This can be useful when you have a number of poses to choose from for a ControlNet node, or a number of input images for another purpose.</p> <p>Node Link: https://github.com/JPPhoto/image-picker-node</p>"},{"location":"nodes/communityNodes/#image-resize-plus","title":"Image Resize Plus","text":"<p>Description: Provides various image resizing options such as fill, stretch, fit, center, and crop.</p> <p>Node Link: https://github.com/VeyDlin/image-resize-plus-node</p> <p>View: </p>"},{"location":"nodes/communityNodes/#latent-upscale","title":"Latent Upscale","text":"<p>Description: This node uses a small (~2.4mb) model to upscale the latents used in a Stable Diffusion 1.5 or Stable Diffusion XL image generation, rather than the typical interpolation method, avoiding the traditional downsides of the latent upscale technique.</p> <p>Node Link: https://github.com/gogurtenjoyer/latent-upscale</p>"},{"location":"nodes/communityNodes/#load-video-frame","title":"Load Video Frame","text":"<p>Description: This is a video frame image provider + indexer/video creation nodes for hooking up to iterators and ranges and ControlNets and such for invokeAI node experimentation. Think animation + ControlNet outputs.</p> <p>Node Link: https://github.com/helix4u/load_video_frame</p> <p>Output Example: </p>"},{"location":"nodes/communityNodes/#make-3d","title":"Make 3D","text":"<p>Description: Create compelling 3D stereo images from 2D originals.</p> <p>Node Link: https://gitlab.com/srcrr/shift3d/-/raw/main/make3d.py</p> <p>Example Node Graph: https://gitlab.com/srcrr/shift3d/-/raw/main/example-workflow.json?ref_type=heads&amp;inline=false</p> <p>Output Examples </p> <p> </p>"},{"location":"nodes/communityNodes/#mask-operations","title":"Mask Operations","text":"<p>Description: Offers logical operations (OR, SUB, AND) for combining and manipulating image masks.</p> <p>Node Link: https://github.com/VeyDlin/mask-operations-node</p> <p>View: </p>"},{"location":"nodes/communityNodes/#match-histogram","title":"Match Histogram","text":"<p>Description: An InvokeAI node to match a histogram from one image to another.  This is a bit like the <code>color correct</code> node in the main InvokeAI but this works in the YCbCr colourspace and can handle images of different sizes. Also does not require a mask input. - Option to only transfer luminance channel. - Option to save output as grayscale</p> <p>A good use case for this node is to normalize the colors of an image that has been through the tiled scaling workflow of my XYGrid Nodes. </p> <p>See full docs here: https://github.com/skunkworxdark/Prompt-tools-nodes/edit/main/README.md</p> <p>Node Link: https://github.com/skunkworxdark/match_histogram</p> <p>Output Examples </p> <p></p>"},{"location":"nodes/communityNodes/#metadata-linked-nodes","title":"Metadata Linked Nodes","text":"<p>Description: A set of nodes for Metadata. Collect Metadata from within an <code>iterate</code> node &amp; extract metadata from an image.</p> <ul> <li><code>Metadata Item Linked</code> - Allows collecting of metadata while within an iterate node with no need for a collect node or conversion to metadata node</li> <li><code>Metadata From Image</code> - Provides Metadata from an image</li> <li><code>Metadata To String</code> - Extracts a String value of a label from metadata</li> <li><code>Metadata To Integer</code> - Extracts an Integer value of a label from metadata</li> <li><code>Metadata To Float</code> - Extracts a Float value of a label from metadata</li> <li><code>Metadata To Scheduler</code> - Extracts a Scheduler value of a label from metadata</li> <li><code>Metadata To Bool</code> - Extracts Bool types from metadata</li> <li><code>Metadata To Model</code> - Extracts model types from metadata</li> <li><code>Metadata To SDXL Model</code> - Extracts SDXL model types from metadata</li> <li><code>Metadata To LoRAs</code> - Extracts Loras from metadata. </li> <li><code>Metadata To SDXL LoRAs</code> - Extracts SDXL Loras from metadata</li> <li><code>Metadata To ControlNets</code> - Extracts ControNets from metadata</li> <li><code>Metadata To IP-Adapters</code> - Extracts IP-Adapters from metadata</li> <li><code>Metadata To T2I-Adapters</code> - Extracts T2I-Adapters from metadata</li> <li><code>Denoise Latents + Metadata</code> - This is an inherited version of the existing <code>Denoise Latents</code> node but with a metadata input and output. </li> </ul> <p>Node Link: https://github.com/skunkworxdark/metadata-linked-nodes</p>"},{"location":"nodes/communityNodes/#negative-image","title":"Negative Image","text":"<p>Description: Creates a negative version of an image, effective for visual effects and mask inversion.</p> <p>Node Link: https://github.com/VeyDlin/negative-image-node</p> <p>View: </p>"},{"location":"nodes/communityNodes/#nightmare-promptgen","title":"Nightmare Promptgen","text":"<p>Description: Nightmare Prompt Generator - Uses a local text generation model to create unique imaginative (but usually nightmarish) prompts for InvokeAI. By default, it allows you to choose from some gpt-neo models I finetuned on over 2500 of my own InvokeAI prompts in Compel format, but you're able to add your own, as well. Offers support for replacing any troublesome words with a random choice from list you can also define.</p> <p>Node Link: https://github.com/gogurtenjoyer/nightmare-promptgen</p>"},{"location":"nodes/communityNodes/#oobabooga","title":"Oobabooga","text":"<p>Description: asks a local LLM running in Oobabooga's Text-Generation-Webui to write a prompt based on the user input.</p> <p>Link: https://github.com/sammyf/oobabooga-node</p> <p>Example:</p> <p>\"describe a new mystical  creature in its natural environment\"</p> <p>can return</p> <p>\"The mystical creature I am describing to you is called the \"Glimmerwing\". It is a majestic, iridescent being that inhabits the depths of the most enchanted forests and glimmering lakes. Its body is covered in shimmering scales that reflect every color of the rainbow, and it has delicate, translucent wings that sparkle like diamonds in the sunlight. The Glimmerwing's home is a crystal-clear lake, surrounded by towering trees with leaves that shimmer like jewels. In this serene environment, the Glimmerwing spends its days swimming gracefully through the water, chasing schools of glittering fish and playing with the gentle ripples of the lake's surface. As the sun sets, the Glimmerwing perches on a branch of one of the trees, spreading its wings to catch the last rays of light. The creature's scales glow softly, casting a rainbow of colors across the forest floor. The Glimmerwing sings a haunting melody, its voice echoing through the stillness of the night air. Its song is said to have the power to heal the sick and bring peace to troubled souls. Those who are lucky enough to hear the Glimmerwing's song are forever changed by its beauty and grace.\"</p> <p></p> <p>Requirement</p> <p>a Text-Generation-Webui instance (might work remotely too, but I never tried it) and obviously InvokeAI 3.x</p> <p>Note</p> <p>This node works best with SDXL models, especially as the style can be described independently of the LLM's output.</p>"},{"location":"nodes/communityNodes/#prompt-tools","title":"Prompt Tools","text":"<p>Description: A set of InvokeAI nodes that add general prompt (string) manipulation tools.  Designed to accompany the <code>Prompts From File</code> node and other prompt generation nodes.</p> <ol> <li><code>Prompt To File</code> - saves a prompt or collection of prompts to a file. one per line. There is an append/overwrite option.</li> <li><code>PTFields Collect</code> - Converts image generation fields into a Json format string that can be passed to Prompt to file. </li> <li><code>PTFields Expand</code> - Takes Json string and converts it to individual generation parameters. This can be fed from the Prompts to file node.</li> <li><code>Prompt Strength</code> - Formats prompt with strength like the weighted format of compel </li> <li><code>Prompt Strength Combine</code> - Combines weighted prompts for .and()/.blend()</li> <li><code>CSV To Index String</code> - Gets a string from a CSV by index. Includes a Random index option</li> </ol> <p>The following Nodes are now included in v3.2 of Invoke and are nolonger in this set of tools. - <code>Prompt Join</code> -&gt; <code>String Join</code> - <code>Prompt Join Three</code> -&gt; <code>String Join Three</code> - <code>Prompt Replace</code> -&gt; <code>String Replace</code> - <code>Prompt Split Neg</code> -&gt; <code>String Split Neg</code></p> <p>See full docs here: https://github.com/skunkworxdark/Prompt-tools-nodes/edit/main/README.md</p> <p>Node Link: https://github.com/skunkworxdark/Prompt-tools-nodes</p> <p>Workflow Examples </p> <p></p>"},{"location":"nodes/communityNodes/#remote-image","title":"Remote Image","text":"<p>Description: This is a pack of nodes to interoperate with other services, be they public websites or bespoke local servers. The pack consists of these nodes:</p> <ul> <li>Load Remote Image - Lets you load remote images such as a realtime webcam image, an image of the day, or dynamically created images.</li> <li>Post Image to Remote Server - Lets you upload an image to a remote server using an HTTP POST request, eg for storage, display or further processing.</li> </ul> <p>Node Link: https://github.com/fieldOfView/InvokeAI-remote_image</p>"},{"location":"nodes/communityNodes/#briaai-remove-background","title":"BriaAI Remove Background","text":"<p>Description: Implements one click background removal with BriaAI's new version 1.4 model which seems to be be producing better results than any other previous background removal tool.</p> <p>Node Link: https://github.com/blessedcoolant/invoke_bria_rmbg</p> <p>View </p>"},{"location":"nodes/communityNodes/#remove-background","title":"Remove Background","text":"<p>Description: An integration of the rembg package to remove backgrounds from images using multiple U2NET models.</p> <p>Node Link: https://github.com/VeyDlin/remove-background-node</p> <p>View: </p>"},{"location":"nodes/communityNodes/#retroize","title":"Retroize","text":"<p>Description: Retroize is a collection of nodes for InvokeAI to \"Retroize\" images. Any image can be given a fresh coat of retro paint with these nodes, either from your gallery or from within the graph itself. It includes nodes to pixelize, quantize, palettize, and ditherize images; as well as to retrieve palettes from existing images.</p> <p>Node Link: https://github.com/Ar7ific1al/invokeai-retroizeinode/</p> <p>Retroize Output Examples</p> <p></p>"},{"location":"nodes/communityNodes/#simple-skin-detection","title":"Simple Skin Detection","text":"<p>Description: Detects skin in images based on predefined color thresholds.</p> <p>Node Link: https://github.com/VeyDlin/simple-skin-detection-node</p> <p>View: </p>"},{"location":"nodes/communityNodes/#size-stepper-nodes","title":"Size Stepper Nodes","text":"<p>Description: This is a set of nodes for calculating the necessary size increments for doing upscaling workflows. Use the Final Size &amp; Orientation node to enter your full size dimensions and orientation (portrait/landscape/random), then plug that and your initial generation dimensions into the Ideal Size Stepper and get 1, 2, or 3 intermediate pairs of dimensions for upscaling. Note this does not output the initial size or full size dimensions: the 1, 2, or 3 outputs of this node are only the intermediate sizes.</p> <p>A third node is included, Random Switch (Integers), which is just a generic version of Final Size with no orientation selection.</p> <p>Node Link: https://github.com/dwringer/size-stepper-nodes</p> <p>Example Usage: </p>"},{"location":"nodes/communityNodes/#text-font-to-image","title":"Text font to Image","text":"<p>Description: text font to text image node for InvokeAI, download a font to use (or if in font cache uses it from there), the text is always resized to the image size, but can control that with padding, optional 2<sup>nd</sup> line</p> <p>Node Link: https://github.com/mickr777/textfontimage</p> <p>Output Examples</p> <p></p> <p>Results after using the depth controlnet</p> <p> </p>"},{"location":"nodes/communityNodes/#thresholding","title":"Thresholding","text":"<p>Description: This node generates masks for highlights, midtones, and shadows given an input image. You can optionally specify a blur for the lookup table used in making those masks from the source image.</p> <p>Node Link: https://github.com/JPPhoto/thresholding-node</p> <p>Examples</p> <p>Input:</p> <p></p> <p>Highlights/Midtones/Shadows:</p> <p> </p> <p>Highlights/Midtones/Shadows (with LUT blur enabled):</p> <p> </p>"},{"location":"nodes/communityNodes/#unsharp-mask","title":"Unsharp Mask","text":"<p>Description: Applies an unsharp mask filter to an image, preserving its alpha channel in the process.</p> <p>Node Link: https://github.com/JPPhoto/unsharp-mask-node</p>"},{"location":"nodes/communityNodes/#xy-image-to-grid-and-images-to-grids-nodes","title":"XY Image to Grid and Images to Grids nodes","text":"<p>Description: These nodes add the following to InvokeAI: - Generate grids of images from multiple input images - Create XY grid images with labels from parameters - Split images into overlapping tiles for processing (for super-resolution workflows) - Recombine image tiles into a single output image blending the seams </p> <p>The nodes include: 1. <code>Images To Grids</code> - Combine multiple images into a grid of images 2. <code>XYImage To Grid</code> - Take X &amp; Y params and creates a labeled image grid. 3. <code>XYImage Tiles</code> - Super-resolution (embiggen) style tiled resizing 4. <code>Image Tot XYImages</code> - Takes an image and cuts it up into a number of columns and rows. 5. Multiple supporting nodes - Helper nodes for data wrangling and building <code>XYImage</code> collections</p> <p>See full docs here: https://github.com/skunkworxdark/XYGrid_nodes/edit/main/README.md</p> <p>Node Link: https://github.com/skunkworxdark/XYGrid_nodes</p> <p>Output Examples </p> <p></p>"},{"location":"nodes/communityNodes/#example-node-template","title":"Example Node Template","text":"<p>Description: This node allows you to do super cool things with InvokeAI.</p> <p>Node Link: https://github.com/invoke-ai/InvokeAI/blob/main/invokeai/app/invocations/prompt.py</p> <p>Example Workflow: https://github.com/invoke-ai/InvokeAI/blob/docs/main/docs/workflows/Prompt_from_File.json</p> <p>Output Examples </p> <p></p>"},{"location":"nodes/communityNodes/#disclaimer","title":"Disclaimer","text":"<p>The nodes linked have been developed and contributed by members of the Invoke AI community. While we strive to ensure the quality and safety of these contributions, we do not guarantee the reliability or security of the nodes. If you have issues or concerns with any of the nodes below, please raise it on GitHub or in the Discord.</p>"},{"location":"nodes/communityNodes/#help","title":"Help","text":"<p>If you run into any issues with a node, please post in the InvokeAI Discord. </p>"},{"location":"nodes/contributingNodes/","title":"Contributing Nodes","text":"<p>To learn about the specifics of creating a new node, please visit our Node creation documentation. </p> <p>Once you\u2019ve created a node and confirmed that it behaves as expected locally, follow these steps: </p> <ul> <li>Make sure the node is contained in a new Python (.py) file. Preferably, the node is in a repo with a README detailing the nodes usage &amp; examples to help others more easily use your node. Including the tag \"invokeai-node\" in your repository's README can also help other users find it more easily. </li> <li>Submit a pull request with a link to your node(s) repo in GitHub against the <code>main</code> branch to add the node to the Community Nodes list<ul> <li>Make sure you are following the template below and have provided all relevant details about the node and what it does. Example output images and workflows are very helpful for other users looking to use your node.</li> </ul> </li> <li>A maintainer will review the pull request and node. If the node is aligned with the direction of the project, you may be asked for permission to include it in the core project.</li> </ul>"},{"location":"nodes/contributingNodes/#community-node-template","title":"Community Node Template","text":"<pre><code>--------------------------------\n### Super Cool Node Template\n\n**Description:** This node allows you to do super cool things with InvokeAI.\n\n**Node Link:** https://github.com/invoke-ai/InvokeAI/fake_node.py\n\n**Example Node Graph:**  https://github.com/invoke-ai/InvokeAI/fake_node_graph.json\n\n**Output Examples** \n\n![InvokeAI](https://invoke-ai.github.io/InvokeAI/assets/invoke_ai_banner.png)\n</code></pre>"},{"location":"nodes/defaultNodes/","title":"List of Default Nodes","text":"<p>The table below contains a list of the default nodes shipped with InvokeAI and their descriptions.</p> Node  Function Add Integers Adds two numbers Boolean Primitive Collection A collection of boolean primitive values Boolean Primitive A boolean primitive value Canny Processor Canny edge detection for ControlNet CenterPadCrop Pad or crop an image's sides from the center by specified pixels. Positive values are outside of the image. CLIP Skip Skip layers in clip text_encoder model. Collect Collects values into a collection Color Correct Shifts the colors of a target image to match the reference image, optionally using a mask to only color-correct certain regions of the target image. Color Primitive A color primitive value Compel Prompt Parse prompt using compel package to conditioning. Conditioning Primitive Collection A collection of conditioning tensor primitive values Conditioning Primitive A conditioning tensor primitive value Content Shuffle Processor Applies content shuffle processing to image ControlNet Collects ControlNet info to pass to other nodes Create Denoise Mask Converts a greyscale or transparency image into a mask for denoising. Create Gradient Mask Creates a mask for Gradient (\"soft\", \"differential\") inpainting that gradually expands during denoising. Improves edge coherence. Denoise Latents Denoises noisy latents to decodable images Divide Integers Divides two numbers Dynamic Prompt Parses a prompt using adieyal/dynamicprompts' random or combinatorial generator FaceMask Generates masks for faces in an image to use with Inpainting FaceIdentifier Identifies and labels faces in an image FaceOff Creates a new image that is a scaled bounding box with a mask on the face for Inpainting Float Math Perform basic math operations on two floats Float Primitive Collection A collection of float primitive values Float Primitive A float primitive value Float Range Creates a range HED (softedge) Processor Applies HED edge detection to image Blur Image Blurs an image Extract Image Channel Gets a channel from an image. Image Primitive Collection A collection of image primitive values Integer Math Perform basic math operations on two integers Convert Image Mode Converts an image to a different mode. Crop Image Crops an image to a specified box. The box can be outside of the image. Ideal Size Calculates an ideal image size for latents for a first pass of a multi-pass upscaling to avoid duplication and other artifacts Image Hue Adjustment Adjusts the Hue of an image. Inverse Lerp Image Inverse linear interpolation of all pixels of an image Image Primitive An image primitive value Lerp Image Linear interpolation of all pixels of an image Offset Image Channel Add to or subtract from an image color channel by a uniform value. Multiply Image Channel Multiply or Invert an image color channel by a scalar value. Multiply Images Multiplies two images together using <code>PIL.ImageChops.multiply()</code>. Blur NSFW Image Add blur to NSFW-flagged images Paste Image Pastes an image into another image. ImageProcessor Base class for invocations that preprocess images for ControlNet Resize Image Resizes an image to specific dimensions Round Float Rounds a float to a specified number of decimal places Float to Integer Converts a float to an integer. Optionally rounds to an even multiple of a input number. Scale Image Scales an image by a factor Image to Latents Encodes an image into latents. Add Invisible Watermark Add an invisible watermark to an image Solid Color Infill Infills transparent areas of an image with a solid color PatchMatch Infill Infills transparent areas of an image using the PatchMatch algorithm Tile Infill Infills transparent areas of an image with tiles of the image Integer Primitive Collection A collection of integer primitive values Integer Primitive An integer primitive value Iterate Iterates over a list of items Latents Primitive Collection A collection of latents tensor primitive values Latents Primitive A latents tensor primitive value Latents to Image Generates an image from latents. Leres (Depth) Processor Applies leres processing to image Lineart Anime Processor Applies line art anime processing to image Lineart Processor Applies line art processing to image LoRA Loader Apply selected lora to unet and text_encoder. Main Model Loader Loads a main model, outputting its submodels. Combine Mask Combine two masks together by multiplying them using <code>PIL.ImageChops.multiply()</code>. Mask Edge Applies an edge mask to an image Mask from Alpha Extracts the alpha channel of an image as a mask. Mediapipe Face Processor Applies mediapipe face processing to image Midas (Depth) Processor Applies Midas depth processing to image MLSD Processor Applies MLSD processing to image Multiply Integers Multiplies two numbers Noise Generates latent noise. Normal BAE Processor Applies NormalBae processing to image ONNX Latents to Image Generates an image from latents. ONNX Prompt (Raw) A node to process inputs and produce outputs. May use dependency injection in init to receive providers. ONNX Text to Latents Generates latents from conditionings. ONNX Model Loader Loads a main model, outputting its submodels. OpenCV Inpaint Simple inpaint using opencv. DW Openpose Processor Applies Openpose processing to image PIDI Processor Applies PIDI processing to image Prompts from File Loads prompts from a text file Random Integer Outputs a single random integer. Random Range Creates a collection of random numbers Integer Range Creates a range of numbers from start to stop with step Integer Range of Size Creates a range from start to start + size with step Resize Latents Resizes latents to explicit width/height (in pixels). Provided dimensions are floor-divided by 8. SDXL Compel Prompt Parse prompt using compel package to conditioning. SDXL LoRA Loader Apply selected lora to unet and text_encoder. SDXL Main Model Loader Loads an sdxl base model, outputting its submodels. SDXL Refiner Compel Prompt Parse prompt using compel package to conditioning. SDXL Refiner Model Loader Loads an sdxl refiner model, outputting its submodels. Scale Latents Scales latents by a given factor. Segment Anything Processor Applies segment anything processing to image Show Image Displays a provided image, and passes it forward in the pipeline. Step Param Easing Experimental per-step parameter easing for denoising steps String Primitive Collection A collection of string primitive values String Primitive A string primitive value Subtract Integers Subtracts two numbers Tile Resample Processor Tile resampler processor Upscale (RealESRGAN) Upscales an image using RealESRGAN. VAE Loader Loads a VAE model, outputting a VaeLoaderOutput Zoe (Depth) Processor Applies Zoe depth processing to image"},{"location":"nodes/exampleWorkflows/","title":"Example Workflows","text":"<p>We've curated some example workflows for you to get started with Workflows in InvokeAI! These can also be found in the Workflow Library, located in the Workflow Editor of Invoke.</p> <p>To use them, right click on your desired workflow, follow the link to GitHub and click the \"\u2b07\" button to download the raw file. You can then use the \"Load Workflow\" functionality in InvokeAI to load the workflow and start generating images!</p> <p>If you're interested in finding more workflows, checkout the #share-your-workflows channel in the InvokeAI Discord.</p> <ul> <li>SD1.5 / SD2 Text to Image</li> <li>SDXL Text to Image</li> <li>SDXL Text to Image with Refiner</li> <li>Multi ControlNet (Canny &amp; Depth)</li> <li>Tiled Upscaling with ControlNet</li> <li>Prompt From File</li> <li>Face Detailer with IP-Adapter &amp; ControlNet</li> <li>FaceMask</li> <li>FaceOff with 2x Face Scaling</li> <li>QR Code Monster</li> </ul>"},{"location":"nodes/overview/","title":"Nodes","text":""},{"location":"nodes/overview/#what-are-nodes","title":"What are Nodes?","text":"<p>An Node is simply a single operation that takes in inputs and returns out outputs. Multiple nodes can be linked together to create more complex functionality. All InvokeAI features are added through nodes.</p>"},{"location":"nodes/overview/#anatomy-of-a-node","title":"Anatomy of a Node","text":"<p>Individual nodes are made up of the following:</p> <ul> <li>Inputs: Edge points on the left side of the node window where you connect outputs from other nodes.</li> <li>Outputs: Edge points on the right side of the node window where you connect to inputs on other nodes.</li> <li>Options: Various options which are either manually configured, or overridden by connecting an output from another node to the input.</li> </ul> <p>With nodes, you can can easily extend the image generation capabilities of InvokeAI, and allow you build workflows that suit your needs. </p> <p>You can read more about nodes and the node editor here. </p> <p>To get started with nodes, take a look at some of our examples for common workflows</p>"},{"location":"nodes/overview/#downloading-new-nodes","title":"Downloading New Nodes","text":"<p>To download a new node, visit our list of Community Nodes. These are nodes that have been created by the community, for the community. </p>"},{"location":"nodes/detailedNodes/faceTools/","title":"Face Nodes","text":""},{"location":"nodes/detailedNodes/faceTools/#faceoff","title":"FaceOff","text":"<p>FaceOff mimics a user finding a face in an image and resizing the bounding box around the head in Canvas.</p> <p>Enter a face ID (found with FaceIdentifier) to choose which face to mask.</p> <p>Just as you would add more context inside the bounding box by making it larger in Canvas, the node gives you a padding input (in pixels) which will simultaneously add more context, and increase the resolution of the bounding box so the face remains the same size inside it.</p> <p>The \"Minimum Confidence\" input defaults to 0.5 (50%), and represents a pass/fail threshold a detected face must reach for it to be processed. Lowering this value may help if detection is failing. If the detected masks are imperfect and stray too far outside/inside of faces, the node gives you X &amp; Y offsets to shrink/grow the masks by a multiplier.</p> <p>FaceOff will output the face in a bounded image, taking the face off of the original image for input into any node that accepts image inputs. The node also outputs a face mask with the dimensions of the bounded image. The X &amp; Y outputs are for connecting to the X &amp; Y inputs of the Paste Image node, which will place the bounded image back on the original image using these coordinates.</p>"},{"location":"nodes/detailedNodes/faceTools/#inputsoutputs","title":"Inputs/Outputs","text":"Input Description Image Image for face detection Face ID The face ID to process, numbered from 0. Multiple faces not supported. Find a face's ID with FaceIdentifier node. Minimum Confidence Minimum confidence for face detection (lower if detection is failing) X Offset X-axis offset of the mask Y Offset Y-axis offset of the mask Padding All-axis padding around the mask in pixels Chunk Chunk (or divide) the image into sections to greatly improve face detection success. Defaults to off, but will activate if no faces are detected normally. Activate to chunk by default. Output Description Bounded Image Original image bound, cropped, and resized Width The width of the bounded image in pixels Height The height of the bounded image in pixels Mask The output mask X The x coordinate of the bounding box's left side Y The y coordinate of the bounding box's top side"},{"location":"nodes/detailedNodes/faceTools/#facemask","title":"FaceMask","text":"<p>FaceMask mimics a user drawing masks on faces in an image in Canvas.</p> <p>The \"Face IDs\" input allows the user to select specific faces to be masked. Leave empty to detect and mask all faces, or a comma-separated list for a specific combination of faces (ex: <code>1,2,4</code>). A single integer will detect and mask that specific face. Find face IDs with the FaceIdentifier node.</p> <p>The \"Minimum Confidence\" input defaults to 0.5 (50%), and represents a pass/fail threshold a detected face must reach for it to be processed. Lowering this value may help if detection is failing.</p> <p>If the detected masks are imperfect and stray too far outside/inside of faces, the node gives you X &amp; Y offsets to shrink/grow the masks by a multiplier. All masks shrink/grow together by the X &amp; Y offset values.</p> <p>By default, masks are created to change faces. When masks are inverted, they change surrounding areas, protecting faces.</p>"},{"location":"nodes/detailedNodes/faceTools/#inputsoutputs_1","title":"Inputs/Outputs","text":"Input Description Image Image for face detection Face IDs Comma-separated list of face ids to mask eg '0,2,7'. Numbered from 0. Leave empty to mask all. Find face IDs with FaceIdentifier node. Minimum Confidence Minimum confidence for face detection (lower if detection is failing) X Offset X-axis offset of the mask Y Offset Y-axis offset of the mask Chunk Chunk (or divide) the image into sections to greatly improve face detection success. Defaults to off, but will activate if no faces are detected normally. Activate to chunk by default. Invert Mask Toggle to invert the face mask Output Description Image The original image Width The width of the image in pixels Height The height of the image in pixels Mask The output face mask"},{"location":"nodes/detailedNodes/faceTools/#faceidentifier","title":"FaceIdentifier","text":"<p>FaceIdentifier outputs an image with detected face IDs printed in white numbers onto each face.</p> <p>Face IDs can then be used in FaceMask and FaceOff to selectively mask all, a specific combination, or single faces.</p> <p>The FaceIdentifier output image is generated for user reference, and isn't meant to be passed on to other image-processing nodes.</p> <p>The \"Minimum Confidence\" input defaults to 0.5 (50%), and represents a pass/fail threshold a detected face must reach for it to be processed. Lowering this value may help if detection is failing. If an image is changed in the slightest, run it through FaceIdentifier again to get updated FaceIDs.</p>"},{"location":"nodes/detailedNodes/faceTools/#inputsoutputs_2","title":"Inputs/Outputs","text":"Input Description Image Image for face detection Minimum Confidence Minimum confidence for face detection (lower if detection is failing) Chunk Chunk (or divide) the image into sections to greatly improve face detection success. Defaults to off, but will activate if no faces are detected normally. Activate to chunk by default. Output Description Image The original image with small face ID numbers printed in white onto each face for user reference Width The width of the original image in pixels Height The height of the original image in pixels"},{"location":"nodes/detailedNodes/faceTools/#tips","title":"Tips","text":"<ul> <li>If not all target faces are being detected, activate Chunk to bypass full   image face detection and greatly improve detection success.</li> <li>Final results will vary between full-image detection and chunking for faces   that are detectable by both due to the nature of the process. Try either to   your taste.</li> <li>Be sure Minimum Confidence is set the same when using FaceIdentifier with   FaceOff/FaceMask.</li> <li>For FaceOff, use the color correction node before faceplace to correct edges   being noticeable in the final image (see example screenshot).</li> <li>Non-inpainting models may struggle to paint/generate correctly around faces.</li> <li>If your face won't change the way you want it to no matter what you change,   consider that the change you're trying to make is too much at that resolution.   For example, if an image is only 512x768 total, the face might only be 128x128   or 256x256, much smaller than the 512x512 your SD1.5 model was probably   trained on. Try increasing the resolution of the image by upscaling or   resizing, add padding to increase the bounding box's resolution, or use an   image where the face takes up more pixels.</li> <li>If the resulting face seems out of place pasted back on the original image   (ie. too large, not proportional), add more padding on the FaceOff node to   give inpainting more context. Context and good prompting are important to   keeping things proportional.</li> <li>If you find the mask is too big/small and going too far outside/inside the   area you want to affect, adjust the x &amp; y offsets to shrink/grow the mask area</li> <li>Use a higher denoise start value to resemble aspects of the original face or   surroundings. Denoise start = 0 &amp; denoise end = 1 will make something new,   while denoise start = 0.50 &amp; denoise end = 1 will be 50% old and 50% new.</li> <li>mediapipe isn't good at detecting faces with lots of face paint, hair covering   the face, etc. Anything that obstructs the face will likely result in no faces   being detected.</li> <li>If you find your face isn't being detected, try lowering the minimum   confidence value from 0.5. This could result in false positives, however   (random areas being detected as faces and masked).</li> <li>After altering an image and wanting to process a different face in the newly   altered image, run the altered image through FaceIdentifier again to see the   new Face IDs. MediaPipe will most likely detect faces in a different order   after an image has been changed in the slightest.</li> </ul>"},{"location":"other/CONTRIBUTORS/","title":"Contributors","text":"<p>The list of all the amazing people who have contributed to the various features that you get to experience in this fork.</p> <p>We thank them for all of their time and hard work.</p>"},{"location":"other/CONTRIBUTORS/#original-author","title":"Original Author","text":"<ul> <li>Lincoln D. Stein</li> </ul>"},{"location":"other/CONTRIBUTORS/#current-core-team","title":"Current Core Team","text":"<ul> <li>@lstein (Lincoln Stein) - Co-maintainer</li> <li>@blessedcoolant - Co-maintainer</li> <li>@hipsterusername (Kent Keirsey) - Co-maintainer, CEO, Positive Vibes</li> <li>@psychedelicious (Spencer Mabrito) - Web Team Leader</li> <li>@chainchompa (Jennifer Player) - Web Development &amp; Chain-Chomping</li> <li>@josh is toast (Josh Corbett) - Web Development</li> <li>@cheerio (Mary Rogers) - Lead Engineer &amp; Web App Development</li> <li>@ebr (Eugene Brodsky) - Cloud/DevOps/Sofware engineer; your friendly neighbourhood cluster-autoscaler</li> <li>@sunija - Standalone version</li> <li>@genomancer (Gregg Helt) - Controlnet support</li> <li>@brandon (Brandon Rising) - Platform, Infrastructure, Backend Systems</li> <li>@ryanjdick (Ryan Dick) - Machine Learning &amp; Training</li> <li>@JPPhoto - Core image generation nodes</li> <li>@dunkeroni - Image generation backend</li> <li>@SkunkWorxDark - Image generation backend</li> <li>@keturn (Kevin Turner) - Diffusers</li> <li>@millu (Millun Atluri) - Community Wizard, Documentation, Node-wrangler, </li> <li>@glimmerleaf (Devon Hopkins) - Community Wizard</li> <li>@gogurt enjoyer - Discord moderator and end user support</li> <li>@whosawhatsis - Discord moderator and end user support</li> <li>@dwinrger - Discord moderator and end user support</li> <li>@526christian - Discord moderator and end user support</li> <li>@harvester62 -  Discord moderator and end user support</li> </ul>"},{"location":"other/CONTRIBUTORS/#honored-team-alumni","title":"Honored Team Alumni","text":"<ul> <li>@StAlKeR7779 (Sergey Borisov) - Torch stack, ONNX, model management, optimization</li> <li>@damian0815 - Attention Systems and Compel Maintainer</li> <li>@netsvetaev (Artur) - Localization support</li> <li>@Kyle0654 (Kyle Schouviller) - Node Architect and General Backend Wizard</li> <li>@tildebyte - Installation and configuration</li> <li>@mauwii (Matthias Wilde) - Installation, release, continuous integration</li> </ul>"},{"location":"other/CONTRIBUTORS/#full-list-of-contributors-by-commit-name","title":"Full List of Contributors by Commit Name","text":"<ul> <li>\uc774\uc2b9\uc11d</li> <li>AbdBarho</li> <li>ablattmann</li> <li>AdamOStark</li> <li>Adam Rice</li> <li>Airton Silva</li> <li>Aldo Hoeben</li> <li>Alexander Eichhorn</li> <li>Alexandre D. Roberge</li> <li>Alexandre Macabies</li> <li>Alfie John</li> <li>Andreas Rozek</li> <li>Andre LaBranche</li> <li>Andy Bearman</li> <li>Andy Luhrs</li> <li>Andy Pilate</li> <li>Anonymous</li> <li>Anthony Monthe</li> <li>Any-Winter-4079</li> <li>apolinario</li> <li>Ar7ific1al</li> <li>ArDiouscuros</li> <li>Armando C. Santisbon</li> <li>Arnold Cordewiner</li> <li>Arthur Holstvoogd</li> <li>artmen1516</li> <li>Artur</li> <li>Arturo Mendivil</li> <li>Ben Alkov</li> <li>Benjamin Warner</li> <li>Bernard Maltais</li> <li>blessedcoolant</li> <li>blhook</li> <li>BlueAmulet</li> <li>Bouncyknighter</li> <li>Brandon</li> <li>Brandon Rising</li> <li>Brent Ozar</li> <li>Brian Racer</li> <li>bsilvereagle</li> <li>c67e708d</li> <li>camenduru</li> <li>CapableWeb</li> <li>Carson Katri</li> <li>chainchompa</li> <li>Chloe</li> <li>Chris Dawson</li> <li>Chris Hayes</li> <li>Chris Jones</li> <li>chromaticist</li> <li>Claus F. Strasburger</li> <li>cmdr2</li> <li>cody</li> <li>Conor Reid</li> <li>Cora Johnson-Roberson</li> <li>coreco</li> <li>cosmii02</li> <li>cpacker</li> <li>Cragin Godley</li> <li>creachec</li> <li>CrypticWit</li> <li>d8ahazard</li> <li>damian</li> <li>damian0815</li> <li>Damian at mba</li> <li>Damian Stewart</li> <li>Daniel Manzke</li> <li>Danny Beer</li> <li>Dan Sully</li> <li>Darren Ringer</li> <li>David Burnett</li> <li>David Ford</li> <li>David Regla</li> <li>David Sisco</li> <li>David Wager</li> <li>Daya Adianto</li> <li>db3000</li> <li>DekitaRPG</li> <li>Denis Olshin</li> <li>Dennis</li> <li>dependabot[bot]</li> <li>Dmitry Parnas</li> <li>Dobrynia100</li> <li>Dominic Letz</li> <li>DrGunnarMallon</li> <li>Drun555</li> <li>dunkeroni</li> <li>Edward Johan</li> <li>elliotsayes</li> <li>Elrik</li> <li>ElrikUnderlake</li> <li>Eric Khun</li> <li>Eric Wolf</li> <li>Eugene</li> <li>Eugene Brodsky</li> <li>ExperimentalCyborg</li> <li>Fabian Bahl</li> <li>Fabio 'MrWHO' Torchetti</li> <li>Fattire</li> <li>fattire</li> <li>Felipe Nogueira</li> <li>F\u00e9lix Sanz</li> <li>figgefigge</li> <li>Gabriel Mackievicz Telles</li> <li>gabrielrotbart</li> <li>gallegonovato</li> <li>G\u00e9rald LONLAS</li> <li>Gille</li> <li>GitHub Actions Bot</li> <li>glibesyck</li> <li>gogurtenjoyer</li> <li>Gohsuke Shimada</li> <li>greatwolf</li> <li>greentext2</li> <li>Gregg Helt</li> <li>H4rk</li> <li>H\u00e5vard Gulldahl</li> <li>henry</li> <li>Henry van Megen</li> <li>hipsterusername</li> <li>hj</li> <li>Hosted Weblate</li> <li>Iman Karim</li> <li>ismail ihsan b\u00fclb\u00fcl</li> <li>ItzAttila</li> <li>Ivan Efimov</li> <li>jakehl</li> <li>Jakub Kolc\u030ca\u0301r\u030c</li> <li>JamDon2</li> <li>James Reynolds</li> <li>Jan Skurovec</li> <li>Jari Vetoniemi</li> <li>Jason Toffaletti</li> <li>Jaulustus</li> <li>Jeff Mahoney</li> <li>Jennifer Player</li> <li>jeremy</li> <li>Jeremy Clark</li> <li>JigenD</li> <li>Jim Hays</li> <li>Johan Roxendal</li> <li>Johnathon Selstad</li> <li>Jonathan</li> <li>Jordan Hewitt</li> <li>Joseph Dries III</li> <li>Josh Corbett</li> <li>JPPhoto</li> <li>jspraul</li> <li>junzi</li> <li>Justin Wong</li> <li>Juuso V</li> <li>Kaspar Emanuel</li> <li>Katsuyuki-Karasawa</li> <li>Keerigan45</li> <li>Kent Keirsey</li> <li>Kevin Brack</li> <li>Kevin Coakley</li> <li>Kevin Gibbons</li> <li>Kevin Schaul</li> <li>Kevin Turner</li> <li>Kieran Klaassen</li> <li>krummrey</li> <li>Kyle</li> <li>Kyle Lacy</li> <li>Kyle Schouviller</li> <li>Lawrence Norton</li> <li>LemonDouble</li> <li>Leo Pasanen</li> <li>Lincoln Stein</li> <li>LoganPederson</li> <li>Lynne Whitehorn</li> <li>majick</li> <li>Marco Labarile</li> <li>Marta Nahorniuk</li> <li>Martin Kristiansen</li> <li>Mary Hipp</li> <li>maryhipp</li> <li>Mary Hipp Rogers</li> <li>mastercaster</li> <li>mastercaster9000</li> <li>Matthias Wild</li> <li>mauwii</li> <li>michaelk71</li> <li>mickr777</li> <li>Mihai</li> <li>Mihail Dumitrescu</li> <li>Mikhail Tishin</li> <li>Millun Atluri</li> <li>Minjune Song</li> <li>Mitchell Allain</li> <li>mitien</li> <li>mofuzz</li> <li>Muhammad Usama</li> <li>Name</li> <li>_nderscore</li> <li>Neil Wang</li> <li>nekowaiz</li> <li>nemuruibai</li> <li>Netzer R</li> <li>Nicholas Koh</li> <li>Nicholas K\u00f6rfer</li> <li>nicolai256</li> <li>Niek van der Maas</li> <li>noodlebox</li> <li>Nuno Cora\u00e7\u00e3o</li> <li>ofirkris</li> <li>Olivier Louvignes</li> <li>owenvincent</li> <li>pand4z31</li> <li>Patrick Esser</li> <li>Patrick Tien</li> <li>Patrick von Platen</li> <li>Paul Curry</li> <li>Paul Sajna</li> <li>pejotr</li> <li>Peter Baylies</li> <li>Peter Lin</li> <li>plucked</li> <li>prixt</li> <li>psychedelicious</li> <li>psychedelicious@windows</li> <li>Rainer Bernhardt</li> <li>Riccardo Giovanetti</li> <li>Rich Jones</li> <li>rmagur1203</li> <li>Rob Baines</li> <li>Robert Bolender</li> <li>Robin Rombach</li> <li>Rohan Barar</li> <li>Rohinish</li> <li>rpagliuca</li> <li>rromb</li> <li>Rupesh Sreeraman</li> <li>Ryan</li> <li>Ryan Cao</li> <li>Ryan Dick</li> <li>Saifeddine</li> <li>Saifeddine ALOUI</li> <li>Sam</li> <li>SammCheese</li> <li>Sam McLeod</li> <li>Sammy</li> <li>sammyf</li> <li>Samuel Husso</li> <li>Saurav Maheshkar</li> <li>Scott Lahteine</li> <li>Sean McLellan</li> <li>Sebastian Aigner</li> <li>Sergey Borisov</li> <li>Sergey Krashevich</li> <li>Shapor Naghibzadeh</li> <li>Shawn Zhong</li> <li>Simona Liliac</li> <li>Simon Vans-Colina</li> <li>skunkworxdark</li> <li>slashtechno</li> <li>SoheilRezaei</li> <li>Song, Pengcheng</li> <li>spezialspezial</li> <li>ssantos</li> <li>StAlKeR7779</li> <li>Stefan Tobler</li> <li>Stephan Koglin-Fischer</li> <li>SteveCaruso</li> <li>Steve Martinelli</li> <li>Steven Frank</li> <li>Surisen</li> <li>System X - Files</li> <li>Taylor Kems</li> <li>techicode</li> <li>techybrain-dev</li> <li>tesseractcat</li> <li>thealanle</li> <li>Thomas</li> <li>tildebyte</li> <li>Tim Cabbage</li> <li>Tom</li> <li>Tom Elovi Spruce</li> <li>Tom Gouville</li> <li>tomosuto</li> <li>Travco</li> <li>Travis Palmer</li> <li>tyler</li> <li>unknown</li> <li>user1</li> <li>vedant-3010</li> <li>Vedant Madane</li> <li>veprogames</li> <li>wa.code</li> <li>wfng92</li> <li>whjms</li> <li>whosawhatsis</li> <li>Will</li> <li>William Becher</li> <li>William Chong</li> <li>Wilson E. Alvarez</li> <li>woweenie</li> <li>Wubbbi</li> <li>xra</li> <li>Yeung Yiu Hung</li> <li>ymgenesis</li> <li>Yorzaren</li> <li>Yosuke Shinya</li> <li>yun saki</li> <li>ZachNagengast</li> <li>Zadagu</li> <li>zeptofine</li> <li>Zerdoumi</li> <li>\u0412\u0430\u0441\u044f\u043d\u0430\u0442\u043e\u0440</li> <li>\u51af\u4e0d\u6e38</li> <li>\u5510\u6fa4 \u514b\u5e78</li> </ul>"},{"location":"other/CONTRIBUTORS/#original-compvis-stable-diffusion-authors","title":"Original CompVis (Stable Diffusion) Authors","text":"<ul> <li>Robin Rombach</li> <li>Patrick von Platen</li> <li>ablattmann</li> <li>Patrick Esser</li> <li>owenvincent</li> <li>apolinario</li> <li>Charles Packer</li> </ul> <p>If you have contributed and don't see your name on the list of contributors, please let one of the collaborators know about the omission, or feel free to make a pull request.</p>"},{"location":"other/README-CompViz/","title":"README from CompViz/stable-diffusion","text":"<p>Stable Diffusion was made possible thanks to a collaboration with Stability AI and Runway and builds upon our previous work:</p> <p>High-Resolution Image Synthesis with Latent Diffusion Models Robin Rombach*, Andreas Blattmann*, Dominik Lorenz\\, Patrick Esser, Bj\u00f6rn Ommer</p>"},{"location":"other/README-CompViz/#cvpr-22-oral","title":"CVPR '22 Oral","text":"<p>which is available on GitHub. PDF at arXiv. Please also visit our Project page.</p> <p> Stable Diffusion is a latent text-to-image diffusion model. Thanks to a generous compute donation from Stability AI and support from LAION, we were able to train a Latent Diffusion Model on 512x512 images from a subset of the LAION-5B database. Similar to Google's Imagen, this model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and runs on a GPU with at least 10GB VRAM. See this section below and the model card.</p>"},{"location":"other/README-CompViz/#requirements","title":"Requirements","text":"<p>A suitable conda environment named <code>ldm</code> can be created and activated with:</p> <pre><code>conda env create\nconda activate ldm\n</code></pre> <p>Note that the first line may be abbreviated <code>conda env create</code>, since conda will look for <code>environment.yml</code> by default.</p> <p>You can also update an existing latent diffusion environment by running</p> <pre><code>conda install pytorch torchvision -c pytorch\npip install transformers==4.19.2\npip install -e .\n</code></pre>"},{"location":"other/README-CompViz/#stable-diffusion-v1","title":"Stable Diffusion v1","text":"<p>Stable Diffusion v1 refers to a specific configuration of the model architecture that uses a downsampling-factor 8 autoencoder with an 860M UNet and CLIP ViT-L/14 text encoder for the diffusion model. The model was pretrained on 256x256 images and then finetuned on 512x512 images.</p> <p>*Note: Stable Diffusion v1 is a general text-to-image diffusion model and therefore mirrors biases and (mis-)conceptions that are present in its training data. Details on the training procedure and data, as well as the intended use of the model can be found in the corresponding model card. Research into the safe deployment of general text-to-image models is an ongoing effort. To prevent misuse and harm, we currently provide access to the checkpoints only for academic research purposes upon request. This is an experiment in safe and community-driven publication of a capable and general text-to-image model. We are working on a public release with a more permissive license that also incorporates ethical considerations.*</p> <p>Request access to Stable Diffusion v1 checkpoints for academic research</p>"},{"location":"other/README-CompViz/#weights","title":"Weights","text":"<p>We currently provide three checkpoints, <code>sd-v1-1.ckpt</code>, <code>sd-v1-2.ckpt</code> and <code>sd-v1-3.ckpt</code>, which were trained as follows,</p> <ul> <li><code>sd-v1-1.ckpt</code>: 237k steps at resolution <code>256x256</code> on   laion2B-en. 194k steps at   resolution <code>512x512</code> on   laion-high-resolution   (170M examples from LAION-5B with resolution <code>&gt;= 1024x1024</code>).</li> <li><code>sd-v1-2.ckpt</code>: Resumed from <code>sd-v1-1.ckpt</code>. 515k steps at resolution   <code>512x512</code> on \"laion-improved-aesthetics\" (a subset of laion2B-en, filtered to   images with an original size <code>&gt;= 512x512</code>, estimated aesthetics score <code>&gt; 5.0</code>,   and an estimated watermark probability <code>&lt; 0.5</code>. The watermark estimate is from   the LAION-5B metadata, the aesthetics score is estimated using an   improved aesthetics estimator).</li> <li><code>sd-v1-3.ckpt</code>: Resumed from <code>sd-v1-2.ckpt</code>. 195k steps at resolution   <code>512x512</code> on \"laion-improved-aesthetics\" and 10\\% dropping of the   text-conditioning to improve   classifier-free guidance sampling.</li> </ul> <p>Evaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0) and 50 PLMS sampling steps show the relative improvements of the checkpoints: </p>"},{"location":"other/README-CompViz/#text-to-image-with-stable-diffusion","title":"Text-to-Image with Stable Diffusion","text":"<p>Stable Diffusion is a latent diffusion model conditioned on the (non-pooled) text embeddings of a CLIP ViT-L/14 text encoder.</p>"},{"location":"other/README-CompViz/#sampling-script","title":"Sampling Script","text":"<p>After obtaining the weights, link them</p> <pre><code>mkdir -p models/ldm/stable-diffusion-v1/\nln -s &lt;path/to/model.ckpt&gt; models/ldm/stable-diffusion-v1/model.ckpt\n</code></pre> <p>and sample with</p> <pre><code>python scripts/txt2img.py --prompt \"a photograph of an astronaut riding a horse\" --plms\n</code></pre> <p>By default, this uses a guidance scale of <code>--scale 7.5</code>, Katherine Crowson's implementation of the PLMS sampler, and renders images of size 512x512 (which it was trained on) in 50 steps. All supported arguments are listed below (type <code>python scripts/txt2img.py --help</code>).</p> <pre><code>usage: txt2img.py [-h] [--prompt [PROMPT]] [--outdir [OUTDIR]] [--skip_grid] [--skip_save] [--ddim_steps DDIM_STEPS] [--plms] [--laion400m] [--fixed_code] [--ddim_eta DDIM_ETA] [--n_iter N_ITER] [--H H] [--W W] [--C C] [--f F] [--n_samples N_SAMPLES] [--n_rows N_ROWS]\n                  [--scale SCALE] [--from-file FROM_FILE] [--config CONFIG] [--ckpt CKPT] [--seed SEED] [--precision {full,autocast}]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --prompt [PROMPT]     the prompt to render\n  --outdir [OUTDIR]     dir to write results to\n  --skip_grid           do not save a grid, only individual samples. Helpful when evaluating lots of samples\n  --skip_save           do not save individual samples. For speed measurements.\n  --ddim_steps DDIM_STEPS\n                        number of ddim sampling steps\n  --plms                use plms sampling\n  --laion400m           uses the LAION400M model\n  --fixed_code          if enabled, uses the same starting code across samples\n  --ddim_eta DDIM_ETA   ddim eta (eta=0.0 corresponds to deterministic sampling\n  --n_iter N_ITER       sample this often\n  --H H                 image height, in pixel space\n  --W W                 image width, in pixel space\n  --C C                 latent channels\n  --f F                 downsampling factor\n  --n_samples N_SAMPLES\n                        how many samples to produce for each given prompt. A.k.a. batch size\n                        (note that the seeds for each image in the batch will be unavailable)\n  --n_rows N_ROWS       rows in the grid (default: n_samples)\n  --scale SCALE         unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))\n  --from-file FROM_FILE\n                        if specified, load prompts from this file\n  --config CONFIG       path to config which constructs model\n  --ckpt CKPT           path to checkpoint of model\n  --seed SEED           the seed (for reproducible sampling)\n  --precision {full,autocast}\n                        evaluate at this precision\n</code></pre> <p>Note: The inference config for all v1 versions is designed to be used with EMA-only checkpoints. For this reason <code>use_ema=False</code> is set in the configuration, otherwise the code will try to switch from non-EMA to EMA weights. If you want to examine the effect of EMA vs no EMA, we provide \"full\" checkpoints which contain both types of weights. For these, <code>use_ema=False</code> will load and use the non-EMA weights.</p>"},{"location":"other/README-CompViz/#diffusers-integration","title":"Diffusers Integration","text":"<p>Another way to download and sample Stable Diffusion is by using the diffusers library</p> <pre><code># make sure you're logged in with `huggingface-cli login`\nfrom torch import autocast\nfrom diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n\npipe = StableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-3-diffusers\",\n    use_auth_token=True\n)\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nwith autocast(\"cuda\"):\n    image = pipe(prompt)[\"sample\"][0]\n\nimage.save(\"astronaut_rides_horse.png\")\n</code></pre>"},{"location":"other/README-CompViz/#image-modification-with-stable-diffusion","title":"Image Modification with Stable Diffusion","text":"<p>By using a diffusion-denoising mechanism as first proposed by SDEdit, the model can be used for different tasks such as text-guided image-to-image translation and upscaling. Similar to the txt2img sampling script, we provide a script to perform image modification with Stable Diffusion.</p> <p>The following describes an example where a rough sketch made in Pinta is converted into a detailed artwork.</p> <pre><code>python scripts/img2img.py --prompt \"A fantasy landscape, trending on artstation\" --init-img &lt;path-to-img.jpg&gt; --strength 0.8\n</code></pre> <p>Here, strength is a value between 0.0 and 1.0, that controls the amount of noise that is added to the input image. Values that approach 1.0 allow for lots of variations but will also produce images that are not semantically consistent with the input. See the following example.</p> <p>Input</p> <p></p> <p>Outputs</p> <p> </p> <p>This procedure can, for example, also be used to upscale samples from the base model.</p>"},{"location":"other/README-CompViz/#comments","title":"Comments","text":"<ul> <li> <p>Our codebase for the diffusion models builds heavily on   OpenAI's ADM codebase and   https://github.com/lucidrains/denoising-diffusion-pytorch.   Thanks for open-sourcing!</p> </li> <li> <p>The implementation of the transformer encoder is from   x-transformers by   lucidrains.</p> </li> </ul>"},{"location":"other/README-CompViz/#bibtex","title":"BibTeX","text":"<pre><code>@misc{rombach2021highresolution,\n      title={High-Resolution Image Synthesis with Latent Diffusion Models},\n      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Bj\u00f6rn Ommer},\n      year={2021},\n      eprint={2112.10752},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"}]}